{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 6160,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.3246753246753247,
      "grad_norm": 4.474750518798828,
      "learning_rate": 3.96e-06,
      "loss": 2.4971,
      "step": 100
    },
    {
      "epoch": 0.6493506493506493,
      "grad_norm": 5.213691234588623,
      "learning_rate": 7.960000000000002e-06,
      "loss": 2.3754,
      "step": 200
    },
    {
      "epoch": 0.974025974025974,
      "grad_norm": 6.50809383392334,
      "learning_rate": 1.196e-05,
      "loss": 2.2848,
      "step": 300
    },
    {
      "epoch": 1.2987012987012987,
      "grad_norm": 8.662930488586426,
      "learning_rate": 1.5960000000000003e-05,
      "loss": 2.1735,
      "step": 400
    },
    {
      "epoch": 1.6233766233766234,
      "grad_norm": 12.545072555541992,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 2.1512,
      "step": 500
    },
    {
      "epoch": 1.948051948051948,
      "grad_norm": 5.162384033203125,
      "learning_rate": 1.9650176678445233e-05,
      "loss": 2.0886,
      "step": 600
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 5.820300102233887,
      "learning_rate": 1.9296819787985867e-05,
      "loss": 2.0108,
      "step": 700
    },
    {
      "epoch": 2.5974025974025974,
      "grad_norm": 6.1853718757629395,
      "learning_rate": 1.8943462897526504e-05,
      "loss": 1.9818,
      "step": 800
    },
    {
      "epoch": 2.9220779220779223,
      "grad_norm": 6.902851581573486,
      "learning_rate": 1.8590106007067138e-05,
      "loss": 1.9757,
      "step": 900
    },
    {
      "epoch": 3.2467532467532467,
      "grad_norm": 6.698544979095459,
      "learning_rate": 1.8236749116607775e-05,
      "loss": 1.8687,
      "step": 1000
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 13.578015327453613,
      "learning_rate": 1.7883392226148412e-05,
      "loss": 1.8257,
      "step": 1100
    },
    {
      "epoch": 3.896103896103896,
      "grad_norm": 12.687973022460938,
      "learning_rate": 1.753003533568905e-05,
      "loss": 1.8118,
      "step": 1200
    },
    {
      "epoch": 4.220779220779221,
      "grad_norm": 11.286373138427734,
      "learning_rate": 1.7176678445229683e-05,
      "loss": 1.721,
      "step": 1300
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 12.938920974731445,
      "learning_rate": 1.682332155477032e-05,
      "loss": 1.6714,
      "step": 1400
    },
    {
      "epoch": 4.87012987012987,
      "grad_norm": 15.552838325500488,
      "learning_rate": 1.6469964664310957e-05,
      "loss": 1.6707,
      "step": 1500
    },
    {
      "epoch": 5.194805194805195,
      "grad_norm": 12.009137153625488,
      "learning_rate": 1.611660777385159e-05,
      "loss": 1.6035,
      "step": 1600
    },
    {
      "epoch": 5.51948051948052,
      "grad_norm": 13.935859680175781,
      "learning_rate": 1.5763250883392228e-05,
      "loss": 1.5148,
      "step": 1700
    },
    {
      "epoch": 5.8441558441558445,
      "grad_norm": 18.116209030151367,
      "learning_rate": 1.5409893992932862e-05,
      "loss": 1.509,
      "step": 1800
    },
    {
      "epoch": 6.1688311688311686,
      "grad_norm": 11.473575592041016,
      "learning_rate": 1.5056537102473499e-05,
      "loss": 1.4473,
      "step": 1900
    },
    {
      "epoch": 6.4935064935064934,
      "grad_norm": 12.821670532226562,
      "learning_rate": 1.4703180212014136e-05,
      "loss": 1.3748,
      "step": 2000
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 15.414849281311035,
      "learning_rate": 1.4349823321554771e-05,
      "loss": 1.3866,
      "step": 2100
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 12.226683616638184,
      "learning_rate": 1.3996466431095409e-05,
      "loss": 1.2963,
      "step": 2200
    },
    {
      "epoch": 7.467532467532467,
      "grad_norm": 24.43671417236328,
      "learning_rate": 1.3643109540636044e-05,
      "loss": 1.2143,
      "step": 2300
    },
    {
      "epoch": 7.792207792207792,
      "grad_norm": 14.791390419006348,
      "learning_rate": 1.328975265017668e-05,
      "loss": 1.2624,
      "step": 2400
    },
    {
      "epoch": 8.116883116883116,
      "grad_norm": 18.21048355102539,
      "learning_rate": 1.2936395759717315e-05,
      "loss": 1.1875,
      "step": 2500
    },
    {
      "epoch": 8.441558441558442,
      "grad_norm": 24.61248207092285,
      "learning_rate": 1.258303886925795e-05,
      "loss": 1.1415,
      "step": 2600
    },
    {
      "epoch": 8.766233766233766,
      "grad_norm": 14.485072135925293,
      "learning_rate": 1.2229681978798587e-05,
      "loss": 1.1127,
      "step": 2700
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 13.744318962097168,
      "learning_rate": 1.1876325088339224e-05,
      "loss": 1.0731,
      "step": 2800
    },
    {
      "epoch": 9.415584415584416,
      "grad_norm": 15.722199440002441,
      "learning_rate": 1.152296819787986e-05,
      "loss": 0.9942,
      "step": 2900
    },
    {
      "epoch": 9.74025974025974,
      "grad_norm": 17.683664321899414,
      "learning_rate": 1.1169611307420495e-05,
      "loss": 1.012,
      "step": 3000
    },
    {
      "epoch": 10.064935064935066,
      "grad_norm": 33.35253143310547,
      "learning_rate": 1.0816254416961132e-05,
      "loss": 0.9887,
      "step": 3100
    },
    {
      "epoch": 10.38961038961039,
      "grad_norm": 27.216754913330078,
      "learning_rate": 1.0462897526501768e-05,
      "loss": 0.9006,
      "step": 3200
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 26.369552612304688,
      "learning_rate": 1.0109540636042403e-05,
      "loss": 0.9212,
      "step": 3300
    },
    {
      "epoch": 11.03896103896104,
      "grad_norm": 19.799354553222656,
      "learning_rate": 9.75618374558304e-06,
      "loss": 0.9102,
      "step": 3400
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 18.399063110351562,
      "learning_rate": 9.402826855123676e-06,
      "loss": 0.814,
      "step": 3500
    },
    {
      "epoch": 11.688311688311689,
      "grad_norm": 18.140296936035156,
      "learning_rate": 9.049469964664311e-06,
      "loss": 0.8234,
      "step": 3600
    },
    {
      "epoch": 12.012987012987013,
      "grad_norm": 25.74734115600586,
      "learning_rate": 8.696113074204948e-06,
      "loss": 0.8216,
      "step": 3700
    },
    {
      "epoch": 12.337662337662337,
      "grad_norm": 17.196582794189453,
      "learning_rate": 8.342756183745584e-06,
      "loss": 0.73,
      "step": 3800
    },
    {
      "epoch": 12.662337662337663,
      "grad_norm": 26.300989151000977,
      "learning_rate": 7.98939929328622e-06,
      "loss": 0.7325,
      "step": 3900
    },
    {
      "epoch": 12.987012987012987,
      "grad_norm": 22.713788986206055,
      "learning_rate": 7.636042402826855e-06,
      "loss": 0.7374,
      "step": 4000
    },
    {
      "epoch": 13.311688311688311,
      "grad_norm": 13.917069435119629,
      "learning_rate": 7.2826855123674926e-06,
      "loss": 0.6542,
      "step": 4100
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 20.578384399414062,
      "learning_rate": 6.929328621908128e-06,
      "loss": 0.6555,
      "step": 4200
    },
    {
      "epoch": 13.96103896103896,
      "grad_norm": 19.47864532470703,
      "learning_rate": 6.575971731448763e-06,
      "loss": 0.6429,
      "step": 4300
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 27.013811111450195,
      "learning_rate": 6.2226148409894e-06,
      "loss": 0.5841,
      "step": 4400
    },
    {
      "epoch": 14.61038961038961,
      "grad_norm": 16.978084564208984,
      "learning_rate": 5.869257950530036e-06,
      "loss": 0.5726,
      "step": 4500
    },
    {
      "epoch": 14.935064935064934,
      "grad_norm": 18.5717716217041,
      "learning_rate": 5.515901060070672e-06,
      "loss": 0.5913,
      "step": 4600
    },
    {
      "epoch": 15.25974025974026,
      "grad_norm": 16.905994415283203,
      "learning_rate": 5.162544169611308e-06,
      "loss": 0.5527,
      "step": 4700
    },
    {
      "epoch": 15.584415584415584,
      "grad_norm": 36.459495544433594,
      "learning_rate": 4.809187279151944e-06,
      "loss": 0.5345,
      "step": 4800
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 24.231216430664062,
      "learning_rate": 4.455830388692579e-06,
      "loss": 0.521,
      "step": 4900
    },
    {
      "epoch": 16.233766233766232,
      "grad_norm": 22.581119537353516,
      "learning_rate": 4.102473498233216e-06,
      "loss": 0.5006,
      "step": 5000
    },
    {
      "epoch": 16.558441558441558,
      "grad_norm": 25.76401138305664,
      "learning_rate": 3.749116607773852e-06,
      "loss": 0.4868,
      "step": 5100
    },
    {
      "epoch": 16.883116883116884,
      "grad_norm": 16.621850967407227,
      "learning_rate": 3.395759717314488e-06,
      "loss": 0.4836,
      "step": 5200
    },
    {
      "epoch": 17.207792207792206,
      "grad_norm": 25.592924118041992,
      "learning_rate": 3.0424028268551236e-06,
      "loss": 0.455,
      "step": 5300
    },
    {
      "epoch": 17.532467532467532,
      "grad_norm": 25.904285430908203,
      "learning_rate": 2.68904593639576e-06,
      "loss": 0.4473,
      "step": 5400
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 18.384000778198242,
      "learning_rate": 2.335689045936396e-06,
      "loss": 0.4433,
      "step": 5500
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 13.221158981323242,
      "learning_rate": 1.982332155477032e-06,
      "loss": 0.4151,
      "step": 5600
    },
    {
      "epoch": 18.506493506493506,
      "grad_norm": 14.062454223632812,
      "learning_rate": 1.628975265017668e-06,
      "loss": 0.4133,
      "step": 5700
    },
    {
      "epoch": 18.83116883116883,
      "grad_norm": 19.01766586303711,
      "learning_rate": 1.275618374558304e-06,
      "loss": 0.3948,
      "step": 5800
    },
    {
      "epoch": 19.155844155844157,
      "grad_norm": 6.242285251617432,
      "learning_rate": 9.2226148409894e-07,
      "loss": 0.3966,
      "step": 5900
    },
    {
      "epoch": 19.48051948051948,
      "grad_norm": 23.44762420654297,
      "learning_rate": 5.689045936395761e-07,
      "loss": 0.3884,
      "step": 6000
    },
    {
      "epoch": 19.805194805194805,
      "grad_norm": 18.87531280517578,
      "learning_rate": 2.1554770318021203e-07,
      "loss": 0.3864,
      "step": 6100
    }
  ],
  "logging_steps": 100,
  "max_steps": 6160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.05922880171168e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
