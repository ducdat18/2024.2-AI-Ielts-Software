{
  "best_global_step": 23940,
  "best_metric": 1.7650781869888306,
  "best_model_checkpoint": "./gpt_ielts/checkpoint-23940",
  "epoch": 7.0,
  "eval_steps": 500,
  "global_step": 23940,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0029239766081871343,
      "grad_norm": 2.4124019145965576,
      "learning_rate": 0.0001999248120300752,
      "loss": 1.5634,
      "step": 10
    },
    {
      "epoch": 0.005847953216374269,
      "grad_norm": 1.623066782951355,
      "learning_rate": 0.00019984126984126985,
      "loss": 1.4054,
      "step": 20
    },
    {
      "epoch": 0.008771929824561403,
      "grad_norm": 1.4450666904449463,
      "learning_rate": 0.0001997577276524645,
      "loss": 1.2937,
      "step": 30
    },
    {
      "epoch": 0.011695906432748537,
      "grad_norm": 1.315651774406433,
      "learning_rate": 0.00019967418546365916,
      "loss": 1.3759,
      "step": 40
    },
    {
      "epoch": 0.014619883040935672,
      "grad_norm": 1.3450779914855957,
      "learning_rate": 0.00019959064327485383,
      "loss": 1.4043,
      "step": 50
    },
    {
      "epoch": 0.017543859649122806,
      "grad_norm": 1.466721773147583,
      "learning_rate": 0.00019950710108604847,
      "loss": 1.2218,
      "step": 60
    },
    {
      "epoch": 0.02046783625730994,
      "grad_norm": 1.1575404405593872,
      "learning_rate": 0.0001994235588972431,
      "loss": 1.3195,
      "step": 70
    },
    {
      "epoch": 0.023391812865497075,
      "grad_norm": 1.267769694328308,
      "learning_rate": 0.00019934001670843777,
      "loss": 1.2768,
      "step": 80
    },
    {
      "epoch": 0.02631578947368421,
      "grad_norm": 1.264460802078247,
      "learning_rate": 0.00019925647451963244,
      "loss": 1.3071,
      "step": 90
    },
    {
      "epoch": 0.029239766081871343,
      "grad_norm": 1.3044551610946655,
      "learning_rate": 0.00019917293233082708,
      "loss": 1.2831,
      "step": 100
    },
    {
      "epoch": 0.03216374269005848,
      "grad_norm": 1.2447595596313477,
      "learning_rate": 0.00019908939014202172,
      "loss": 1.2178,
      "step": 110
    },
    {
      "epoch": 0.03508771929824561,
      "grad_norm": 1.4428608417510986,
      "learning_rate": 0.00019900584795321636,
      "loss": 1.2256,
      "step": 120
    },
    {
      "epoch": 0.038011695906432746,
      "grad_norm": 1.8102868795394897,
      "learning_rate": 0.00019892230576441105,
      "loss": 1.3121,
      "step": 130
    },
    {
      "epoch": 0.04093567251461988,
      "grad_norm": 1.1726683378219604,
      "learning_rate": 0.0001988387635756057,
      "loss": 1.1907,
      "step": 140
    },
    {
      "epoch": 0.043859649122807015,
      "grad_norm": 1.2156747579574585,
      "learning_rate": 0.00019875522138680033,
      "loss": 1.2177,
      "step": 150
    },
    {
      "epoch": 0.04678362573099415,
      "grad_norm": 1.2191554307937622,
      "learning_rate": 0.00019867167919799497,
      "loss": 1.2858,
      "step": 160
    },
    {
      "epoch": 0.049707602339181284,
      "grad_norm": 1.4536787271499634,
      "learning_rate": 0.00019858813700918967,
      "loss": 1.353,
      "step": 170
    },
    {
      "epoch": 0.05263157894736842,
      "grad_norm": 0.922555148601532,
      "learning_rate": 0.0001985045948203843,
      "loss": 1.3087,
      "step": 180
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 1.2155903577804565,
      "learning_rate": 0.00019842105263157895,
      "loss": 1.1446,
      "step": 190
    },
    {
      "epoch": 0.05847953216374269,
      "grad_norm": 1.2239643335342407,
      "learning_rate": 0.0001983375104427736,
      "loss": 1.3511,
      "step": 200
    },
    {
      "epoch": 0.06140350877192982,
      "grad_norm": 1.2974363565444946,
      "learning_rate": 0.00019825396825396828,
      "loss": 1.2402,
      "step": 210
    },
    {
      "epoch": 0.06432748538011696,
      "grad_norm": 1.226890206336975,
      "learning_rate": 0.00019817042606516292,
      "loss": 1.2395,
      "step": 220
    },
    {
      "epoch": 0.06725146198830409,
      "grad_norm": 1.2691954374313354,
      "learning_rate": 0.00019808688387635756,
      "loss": 1.2091,
      "step": 230
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 1.0409901142120361,
      "learning_rate": 0.0001980033416875522,
      "loss": 1.2646,
      "step": 240
    },
    {
      "epoch": 0.07309941520467836,
      "grad_norm": 1.095580816268921,
      "learning_rate": 0.00019791979949874687,
      "loss": 1.1849,
      "step": 250
    },
    {
      "epoch": 0.07602339181286549,
      "grad_norm": 1.5723263025283813,
      "learning_rate": 0.00019783625730994154,
      "loss": 1.3427,
      "step": 260
    },
    {
      "epoch": 0.07894736842105263,
      "grad_norm": 1.105979323387146,
      "learning_rate": 0.00019775271512113618,
      "loss": 1.2221,
      "step": 270
    },
    {
      "epoch": 0.08187134502923976,
      "grad_norm": 1.2110220193862915,
      "learning_rate": 0.00019766917293233082,
      "loss": 1.3193,
      "step": 280
    },
    {
      "epoch": 0.0847953216374269,
      "grad_norm": 1.4400768280029297,
      "learning_rate": 0.00019758563074352548,
      "loss": 1.2881,
      "step": 290
    },
    {
      "epoch": 0.08771929824561403,
      "grad_norm": 1.713606595993042,
      "learning_rate": 0.00019750208855472015,
      "loss": 1.2468,
      "step": 300
    },
    {
      "epoch": 0.09064327485380116,
      "grad_norm": 1.046634316444397,
      "learning_rate": 0.0001974185463659148,
      "loss": 1.3191,
      "step": 310
    },
    {
      "epoch": 0.0935672514619883,
      "grad_norm": 1.1981046199798584,
      "learning_rate": 0.00019733500417710943,
      "loss": 1.16,
      "step": 320
    },
    {
      "epoch": 0.09649122807017543,
      "grad_norm": 1.2704472541809082,
      "learning_rate": 0.0001972514619883041,
      "loss": 1.2347,
      "step": 330
    },
    {
      "epoch": 0.09941520467836257,
      "grad_norm": 1.1738897562026978,
      "learning_rate": 0.00019716791979949876,
      "loss": 1.1521,
      "step": 340
    },
    {
      "epoch": 0.1023391812865497,
      "grad_norm": 1.2330589294433594,
      "learning_rate": 0.0001970843776106934,
      "loss": 1.1718,
      "step": 350
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 1.1000934839248657,
      "learning_rate": 0.00019700083542188804,
      "loss": 1.2628,
      "step": 360
    },
    {
      "epoch": 0.10818713450292397,
      "grad_norm": 1.0805038213729858,
      "learning_rate": 0.0001969172932330827,
      "loss": 1.0886,
      "step": 370
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.1208701133728027,
      "learning_rate": 0.00019683375104427738,
      "loss": 1.2047,
      "step": 380
    },
    {
      "epoch": 0.11403508771929824,
      "grad_norm": 1.1701985597610474,
      "learning_rate": 0.00019675020885547202,
      "loss": 1.1642,
      "step": 390
    },
    {
      "epoch": 0.11695906432748537,
      "grad_norm": 1.1529518365859985,
      "learning_rate": 0.00019666666666666666,
      "loss": 1.1595,
      "step": 400
    },
    {
      "epoch": 0.11988304093567251,
      "grad_norm": 1.2965373992919922,
      "learning_rate": 0.00019658312447786133,
      "loss": 1.1422,
      "step": 410
    },
    {
      "epoch": 0.12280701754385964,
      "grad_norm": 1.622321367263794,
      "learning_rate": 0.000196499582289056,
      "loss": 1.2936,
      "step": 420
    },
    {
      "epoch": 0.12573099415204678,
      "grad_norm": 1.1322349309921265,
      "learning_rate": 0.00019641604010025063,
      "loss": 1.1355,
      "step": 430
    },
    {
      "epoch": 0.1286549707602339,
      "grad_norm": 1.3726507425308228,
      "learning_rate": 0.00019633249791144527,
      "loss": 1.1319,
      "step": 440
    },
    {
      "epoch": 0.13157894736842105,
      "grad_norm": 1.1889506578445435,
      "learning_rate": 0.00019624895572263994,
      "loss": 1.119,
      "step": 450
    },
    {
      "epoch": 0.13450292397660818,
      "grad_norm": 1.1766294240951538,
      "learning_rate": 0.0001961654135338346,
      "loss": 1.2481,
      "step": 460
    },
    {
      "epoch": 0.13742690058479531,
      "grad_norm": 1.130707025527954,
      "learning_rate": 0.00019608187134502925,
      "loss": 1.1356,
      "step": 470
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 1.199259638786316,
      "learning_rate": 0.0001959983291562239,
      "loss": 1.1876,
      "step": 480
    },
    {
      "epoch": 0.14327485380116958,
      "grad_norm": 1.793304681777954,
      "learning_rate": 0.00019591478696741855,
      "loss": 1.2953,
      "step": 490
    },
    {
      "epoch": 0.14619883040935672,
      "grad_norm": 0.9470129609107971,
      "learning_rate": 0.00019583124477861322,
      "loss": 1.1569,
      "step": 500
    },
    {
      "epoch": 0.14912280701754385,
      "grad_norm": 1.1203190088272095,
      "learning_rate": 0.00019574770258980786,
      "loss": 1.2465,
      "step": 510
    },
    {
      "epoch": 0.15204678362573099,
      "grad_norm": 0.8487420678138733,
      "learning_rate": 0.0001956641604010025,
      "loss": 1.1538,
      "step": 520
    },
    {
      "epoch": 0.15497076023391812,
      "grad_norm": 1.2957020998001099,
      "learning_rate": 0.00019558061821219717,
      "loss": 1.1735,
      "step": 530
    },
    {
      "epoch": 0.15789473684210525,
      "grad_norm": 1.2683199644088745,
      "learning_rate": 0.00019549707602339183,
      "loss": 1.1643,
      "step": 540
    },
    {
      "epoch": 0.1608187134502924,
      "grad_norm": 1.0244141817092896,
      "learning_rate": 0.00019541353383458647,
      "loss": 1.0923,
      "step": 550
    },
    {
      "epoch": 0.16374269005847952,
      "grad_norm": 1.5477880239486694,
      "learning_rate": 0.00019532999164578111,
      "loss": 1.271,
      "step": 560
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.4305070638656616,
      "learning_rate": 0.00019524644945697578,
      "loss": 1.1536,
      "step": 570
    },
    {
      "epoch": 0.1695906432748538,
      "grad_norm": 1.1810905933380127,
      "learning_rate": 0.00019516290726817045,
      "loss": 1.2948,
      "step": 580
    },
    {
      "epoch": 0.17251461988304093,
      "grad_norm": 1.0777710676193237,
      "learning_rate": 0.0001950793650793651,
      "loss": 1.1197,
      "step": 590
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 1.0130174160003662,
      "learning_rate": 0.00019499582289055973,
      "loss": 1.1846,
      "step": 600
    },
    {
      "epoch": 0.1783625730994152,
      "grad_norm": 1.0850352048873901,
      "learning_rate": 0.0001949122807017544,
      "loss": 1.1175,
      "step": 610
    },
    {
      "epoch": 0.18128654970760233,
      "grad_norm": 0.9342467188835144,
      "learning_rate": 0.00019482873851294906,
      "loss": 1.1504,
      "step": 620
    },
    {
      "epoch": 0.18421052631578946,
      "grad_norm": 1.0876224040985107,
      "learning_rate": 0.0001947451963241437,
      "loss": 1.2058,
      "step": 630
    },
    {
      "epoch": 0.1871345029239766,
      "grad_norm": 1.0056732892990112,
      "learning_rate": 0.00019466165413533834,
      "loss": 1.2452,
      "step": 640
    },
    {
      "epoch": 0.19005847953216373,
      "grad_norm": 1.178617000579834,
      "learning_rate": 0.000194578111946533,
      "loss": 1.1754,
      "step": 650
    },
    {
      "epoch": 0.19298245614035087,
      "grad_norm": 0.8719242811203003,
      "learning_rate": 0.00019449456975772765,
      "loss": 1.2328,
      "step": 660
    },
    {
      "epoch": 0.195906432748538,
      "grad_norm": 0.923870325088501,
      "learning_rate": 0.00019441102756892232,
      "loss": 1.2032,
      "step": 670
    },
    {
      "epoch": 0.19883040935672514,
      "grad_norm": 1.338371992111206,
      "learning_rate": 0.00019432748538011696,
      "loss": 1.2298,
      "step": 680
    },
    {
      "epoch": 0.20175438596491227,
      "grad_norm": 1.3498848676681519,
      "learning_rate": 0.00019424394319131162,
      "loss": 1.1674,
      "step": 690
    },
    {
      "epoch": 0.2046783625730994,
      "grad_norm": 1.2696597576141357,
      "learning_rate": 0.00019416040100250626,
      "loss": 1.1321,
      "step": 700
    },
    {
      "epoch": 0.20760233918128654,
      "grad_norm": 1.174076795578003,
      "learning_rate": 0.00019407685881370093,
      "loss": 1.1828,
      "step": 710
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 0.9683161973953247,
      "learning_rate": 0.00019399331662489557,
      "loss": 1.2828,
      "step": 720
    },
    {
      "epoch": 0.2134502923976608,
      "grad_norm": 1.3617429733276367,
      "learning_rate": 0.00019390977443609024,
      "loss": 1.1629,
      "step": 730
    },
    {
      "epoch": 0.21637426900584794,
      "grad_norm": 0.9820993542671204,
      "learning_rate": 0.00019382623224728488,
      "loss": 1.22,
      "step": 740
    },
    {
      "epoch": 0.21929824561403508,
      "grad_norm": 1.2660741806030273,
      "learning_rate": 0.00019374269005847954,
      "loss": 1.1641,
      "step": 750
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.2282246351242065,
      "learning_rate": 0.00019365914786967418,
      "loss": 1.0828,
      "step": 760
    },
    {
      "epoch": 0.22514619883040934,
      "grad_norm": 1.1647650003433228,
      "learning_rate": 0.00019357560568086885,
      "loss": 1.1936,
      "step": 770
    },
    {
      "epoch": 0.22807017543859648,
      "grad_norm": 0.9128993153572083,
      "learning_rate": 0.0001934920634920635,
      "loss": 1.1715,
      "step": 780
    },
    {
      "epoch": 0.2309941520467836,
      "grad_norm": 1.0369324684143066,
      "learning_rate": 0.00019340852130325816,
      "loss": 1.197,
      "step": 790
    },
    {
      "epoch": 0.23391812865497075,
      "grad_norm": 0.9550421237945557,
      "learning_rate": 0.0001933249791144528,
      "loss": 1.1128,
      "step": 800
    },
    {
      "epoch": 0.23684210526315788,
      "grad_norm": 1.2738028764724731,
      "learning_rate": 0.00019324143692564747,
      "loss": 1.1895,
      "step": 810
    },
    {
      "epoch": 0.23976608187134502,
      "grad_norm": 0.9601523876190186,
      "learning_rate": 0.0001931578947368421,
      "loss": 1.1032,
      "step": 820
    },
    {
      "epoch": 0.24269005847953215,
      "grad_norm": 1.1314390897750854,
      "learning_rate": 0.00019307435254803677,
      "loss": 1.1864,
      "step": 830
    },
    {
      "epoch": 0.24561403508771928,
      "grad_norm": 0.9078497886657715,
      "learning_rate": 0.0001929908103592314,
      "loss": 1.1889,
      "step": 840
    },
    {
      "epoch": 0.24853801169590642,
      "grad_norm": 1.2200324535369873,
      "learning_rate": 0.00019290726817042608,
      "loss": 1.1875,
      "step": 850
    },
    {
      "epoch": 0.25146198830409355,
      "grad_norm": 1.3155333995819092,
      "learning_rate": 0.00019282372598162072,
      "loss": 1.2892,
      "step": 860
    },
    {
      "epoch": 0.2543859649122807,
      "grad_norm": 1.0508770942687988,
      "learning_rate": 0.0001927401837928154,
      "loss": 1.1187,
      "step": 870
    },
    {
      "epoch": 0.2573099415204678,
      "grad_norm": 0.989165186882019,
      "learning_rate": 0.00019265664160401003,
      "loss": 1.179,
      "step": 880
    },
    {
      "epoch": 0.260233918128655,
      "grad_norm": 0.8981691002845764,
      "learning_rate": 0.0001925730994152047,
      "loss": 1.292,
      "step": 890
    },
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 1.1759570837020874,
      "learning_rate": 0.00019248955722639933,
      "loss": 1.0688,
      "step": 900
    },
    {
      "epoch": 0.26608187134502925,
      "grad_norm": 1.1683745384216309,
      "learning_rate": 0.000192406015037594,
      "loss": 1.2142,
      "step": 910
    },
    {
      "epoch": 0.26900584795321636,
      "grad_norm": 1.135593295097351,
      "learning_rate": 0.00019232247284878864,
      "loss": 1.1521,
      "step": 920
    },
    {
      "epoch": 0.2719298245614035,
      "grad_norm": 0.9996246695518494,
      "learning_rate": 0.0001922389306599833,
      "loss": 1.1278,
      "step": 930
    },
    {
      "epoch": 0.27485380116959063,
      "grad_norm": 0.8188198208808899,
      "learning_rate": 0.00019215538847117795,
      "loss": 1.165,
      "step": 940
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.9175700545310974,
      "learning_rate": 0.00019207184628237261,
      "loss": 1.1272,
      "step": 950
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 0.9423975348472595,
      "learning_rate": 0.00019198830409356725,
      "loss": 1.2317,
      "step": 960
    },
    {
      "epoch": 0.28362573099415206,
      "grad_norm": 1.1538349390029907,
      "learning_rate": 0.00019190476190476192,
      "loss": 1.1067,
      "step": 970
    },
    {
      "epoch": 0.28654970760233917,
      "grad_norm": 0.9053179025650024,
      "learning_rate": 0.00019182121971595656,
      "loss": 1.1834,
      "step": 980
    },
    {
      "epoch": 0.2894736842105263,
      "grad_norm": 0.9886608123779297,
      "learning_rate": 0.00019173767752715123,
      "loss": 1.0921,
      "step": 990
    },
    {
      "epoch": 0.29239766081871343,
      "grad_norm": 1.1578291654586792,
      "learning_rate": 0.00019165413533834587,
      "loss": 1.1238,
      "step": 1000
    },
    {
      "epoch": 0.2953216374269006,
      "grad_norm": 1.0819436311721802,
      "learning_rate": 0.00019157059314954054,
      "loss": 1.0877,
      "step": 1010
    },
    {
      "epoch": 0.2982456140350877,
      "grad_norm": 0.9077773690223694,
      "learning_rate": 0.00019148705096073518,
      "loss": 1.0758,
      "step": 1020
    },
    {
      "epoch": 0.30116959064327486,
      "grad_norm": 0.9822357892990112,
      "learning_rate": 0.00019140350877192984,
      "loss": 1.1918,
      "step": 1030
    },
    {
      "epoch": 0.30409356725146197,
      "grad_norm": 0.9701476693153381,
      "learning_rate": 0.00019131996658312448,
      "loss": 1.0686,
      "step": 1040
    },
    {
      "epoch": 0.30701754385964913,
      "grad_norm": 1.022387146949768,
      "learning_rate": 0.00019123642439431915,
      "loss": 1.1531,
      "step": 1050
    },
    {
      "epoch": 0.30994152046783624,
      "grad_norm": 0.9592752456665039,
      "learning_rate": 0.0001911528822055138,
      "loss": 1.2276,
      "step": 1060
    },
    {
      "epoch": 0.3128654970760234,
      "grad_norm": 1.2947897911071777,
      "learning_rate": 0.00019106934001670843,
      "loss": 1.1527,
      "step": 1070
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 1.087003231048584,
      "learning_rate": 0.0001909857978279031,
      "loss": 1.1574,
      "step": 1080
    },
    {
      "epoch": 0.31871345029239767,
      "grad_norm": 0.8688156008720398,
      "learning_rate": 0.00019090225563909776,
      "loss": 1.1971,
      "step": 1090
    },
    {
      "epoch": 0.3216374269005848,
      "grad_norm": 1.3863972425460815,
      "learning_rate": 0.0001908187134502924,
      "loss": 1.1996,
      "step": 1100
    },
    {
      "epoch": 0.32456140350877194,
      "grad_norm": 1.049919843673706,
      "learning_rate": 0.00019073517126148704,
      "loss": 1.1717,
      "step": 1110
    },
    {
      "epoch": 0.32748538011695905,
      "grad_norm": 0.8062308430671692,
      "learning_rate": 0.0001906516290726817,
      "loss": 1.0846,
      "step": 1120
    },
    {
      "epoch": 0.3304093567251462,
      "grad_norm": 1.175329566001892,
      "learning_rate": 0.00019056808688387638,
      "loss": 1.1012,
      "step": 1130
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.9346116781234741,
      "learning_rate": 0.00019048454469507102,
      "loss": 1.2644,
      "step": 1140
    },
    {
      "epoch": 0.3362573099415205,
      "grad_norm": 1.2653295993804932,
      "learning_rate": 0.00019040100250626566,
      "loss": 1.1385,
      "step": 1150
    },
    {
      "epoch": 0.3391812865497076,
      "grad_norm": 1.072365403175354,
      "learning_rate": 0.00019031746031746032,
      "loss": 1.2117,
      "step": 1160
    },
    {
      "epoch": 0.34210526315789475,
      "grad_norm": 1.0119365453720093,
      "learning_rate": 0.000190233918128655,
      "loss": 1.1977,
      "step": 1170
    },
    {
      "epoch": 0.34502923976608185,
      "grad_norm": 1.0752331018447876,
      "learning_rate": 0.00019015037593984963,
      "loss": 1.1771,
      "step": 1180
    },
    {
      "epoch": 0.347953216374269,
      "grad_norm": 0.8588363528251648,
      "learning_rate": 0.00019006683375104427,
      "loss": 1.141,
      "step": 1190
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 0.9864112734794617,
      "learning_rate": 0.00018998329156223894,
      "loss": 1.2164,
      "step": 1200
    },
    {
      "epoch": 0.3538011695906433,
      "grad_norm": 1.1767258644104004,
      "learning_rate": 0.0001898997493734336,
      "loss": 1.0896,
      "step": 1210
    },
    {
      "epoch": 0.3567251461988304,
      "grad_norm": 1.0213000774383545,
      "learning_rate": 0.00018981620718462825,
      "loss": 1.1273,
      "step": 1220
    },
    {
      "epoch": 0.35964912280701755,
      "grad_norm": 0.8578765392303467,
      "learning_rate": 0.00018973266499582289,
      "loss": 1.0417,
      "step": 1230
    },
    {
      "epoch": 0.36257309941520466,
      "grad_norm": 0.9656695127487183,
      "learning_rate": 0.00018964912280701755,
      "loss": 1.0846,
      "step": 1240
    },
    {
      "epoch": 0.3654970760233918,
      "grad_norm": 0.9031704068183899,
      "learning_rate": 0.00018956558061821222,
      "loss": 1.1184,
      "step": 1250
    },
    {
      "epoch": 0.3684210526315789,
      "grad_norm": 0.8159964680671692,
      "learning_rate": 0.00018948203842940686,
      "loss": 1.1354,
      "step": 1260
    },
    {
      "epoch": 0.3713450292397661,
      "grad_norm": 0.8266323804855347,
      "learning_rate": 0.0001893984962406015,
      "loss": 1.0545,
      "step": 1270
    },
    {
      "epoch": 0.3742690058479532,
      "grad_norm": 1.1157891750335693,
      "learning_rate": 0.00018931495405179617,
      "loss": 1.1134,
      "step": 1280
    },
    {
      "epoch": 0.37719298245614036,
      "grad_norm": 0.8307954668998718,
      "learning_rate": 0.00018923141186299083,
      "loss": 1.0831,
      "step": 1290
    },
    {
      "epoch": 0.38011695906432746,
      "grad_norm": 0.8873001933097839,
      "learning_rate": 0.00018914786967418547,
      "loss": 1.0282,
      "step": 1300
    },
    {
      "epoch": 0.3830409356725146,
      "grad_norm": 1.038109540939331,
      "learning_rate": 0.0001890643274853801,
      "loss": 1.0962,
      "step": 1310
    },
    {
      "epoch": 0.38596491228070173,
      "grad_norm": 0.8821406364440918,
      "learning_rate": 0.00018898078529657478,
      "loss": 1.1777,
      "step": 1320
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.9931619763374329,
      "learning_rate": 0.00018889724310776945,
      "loss": 1.149,
      "step": 1330
    },
    {
      "epoch": 0.391812865497076,
      "grad_norm": 1.000845193862915,
      "learning_rate": 0.0001888137009189641,
      "loss": 1.1526,
      "step": 1340
    },
    {
      "epoch": 0.39473684210526316,
      "grad_norm": 0.977139413356781,
      "learning_rate": 0.00018873015873015873,
      "loss": 1.2123,
      "step": 1350
    },
    {
      "epoch": 0.39766081871345027,
      "grad_norm": 0.9501912593841553,
      "learning_rate": 0.0001886466165413534,
      "loss": 1.1396,
      "step": 1360
    },
    {
      "epoch": 0.40058479532163743,
      "grad_norm": 0.8779495358467102,
      "learning_rate": 0.00018856307435254806,
      "loss": 1.1235,
      "step": 1370
    },
    {
      "epoch": 0.40350877192982454,
      "grad_norm": 1.1493422985076904,
      "learning_rate": 0.0001884795321637427,
      "loss": 1.1818,
      "step": 1380
    },
    {
      "epoch": 0.4064327485380117,
      "grad_norm": 1.1042094230651855,
      "learning_rate": 0.00018839598997493734,
      "loss": 1.0956,
      "step": 1390
    },
    {
      "epoch": 0.4093567251461988,
      "grad_norm": 0.7945120334625244,
      "learning_rate": 0.000188312447786132,
      "loss": 1.2171,
      "step": 1400
    },
    {
      "epoch": 0.41228070175438597,
      "grad_norm": 0.9490628838539124,
      "learning_rate": 0.00018822890559732668,
      "loss": 1.121,
      "step": 1410
    },
    {
      "epoch": 0.4152046783625731,
      "grad_norm": 1.0298078060150146,
      "learning_rate": 0.00018814536340852132,
      "loss": 1.1167,
      "step": 1420
    },
    {
      "epoch": 0.41812865497076024,
      "grad_norm": 0.8528461456298828,
      "learning_rate": 0.00018806182121971596,
      "loss": 1.1705,
      "step": 1430
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 1.0244526863098145,
      "learning_rate": 0.00018797827903091062,
      "loss": 1.1234,
      "step": 1440
    },
    {
      "epoch": 0.4239766081871345,
      "grad_norm": 1.0059417486190796,
      "learning_rate": 0.0001878947368421053,
      "loss": 1.2226,
      "step": 1450
    },
    {
      "epoch": 0.4269005847953216,
      "grad_norm": 0.8065606951713562,
      "learning_rate": 0.00018781119465329993,
      "loss": 1.114,
      "step": 1460
    },
    {
      "epoch": 0.4298245614035088,
      "grad_norm": 1.0179815292358398,
      "learning_rate": 0.00018772765246449457,
      "loss": 1.1006,
      "step": 1470
    },
    {
      "epoch": 0.4327485380116959,
      "grad_norm": 0.9624338150024414,
      "learning_rate": 0.0001876441102756892,
      "loss": 1.1867,
      "step": 1480
    },
    {
      "epoch": 0.43567251461988304,
      "grad_norm": 1.0493419170379639,
      "learning_rate": 0.0001875605680868839,
      "loss": 1.128,
      "step": 1490
    },
    {
      "epoch": 0.43859649122807015,
      "grad_norm": 0.7332073450088501,
      "learning_rate": 0.00018747702589807854,
      "loss": 1.1153,
      "step": 1500
    },
    {
      "epoch": 0.4415204678362573,
      "grad_norm": 0.9218752980232239,
      "learning_rate": 0.00018739348370927318,
      "loss": 1.0869,
      "step": 1510
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.0191962718963623,
      "learning_rate": 0.00018730994152046782,
      "loss": 1.1777,
      "step": 1520
    },
    {
      "epoch": 0.4473684210526316,
      "grad_norm": 0.9886483550071716,
      "learning_rate": 0.00018722639933166252,
      "loss": 1.0995,
      "step": 1530
    },
    {
      "epoch": 0.4502923976608187,
      "grad_norm": 0.9370087385177612,
      "learning_rate": 0.00018714285714285716,
      "loss": 1.1143,
      "step": 1540
    },
    {
      "epoch": 0.45321637426900585,
      "grad_norm": 0.8812897801399231,
      "learning_rate": 0.0001870593149540518,
      "loss": 1.1948,
      "step": 1550
    },
    {
      "epoch": 0.45614035087719296,
      "grad_norm": 0.8351472616195679,
      "learning_rate": 0.00018697577276524644,
      "loss": 1.0233,
      "step": 1560
    },
    {
      "epoch": 0.4590643274853801,
      "grad_norm": 0.9672799110412598,
      "learning_rate": 0.00018689223057644113,
      "loss": 1.0732,
      "step": 1570
    },
    {
      "epoch": 0.4619883040935672,
      "grad_norm": 1.081644892692566,
      "learning_rate": 0.00018680868838763577,
      "loss": 1.2008,
      "step": 1580
    },
    {
      "epoch": 0.4649122807017544,
      "grad_norm": 0.9108291268348694,
      "learning_rate": 0.0001867251461988304,
      "loss": 1.2365,
      "step": 1590
    },
    {
      "epoch": 0.4678362573099415,
      "grad_norm": 0.9947437047958374,
      "learning_rate": 0.00018664160401002505,
      "loss": 1.1165,
      "step": 1600
    },
    {
      "epoch": 0.47076023391812866,
      "grad_norm": 1.0423905849456787,
      "learning_rate": 0.00018655806182121972,
      "loss": 1.2074,
      "step": 1610
    },
    {
      "epoch": 0.47368421052631576,
      "grad_norm": 1.0600279569625854,
      "learning_rate": 0.00018647451963241439,
      "loss": 1.1555,
      "step": 1620
    },
    {
      "epoch": 0.4766081871345029,
      "grad_norm": 1.2464076280593872,
      "learning_rate": 0.00018639097744360903,
      "loss": 1.1137,
      "step": 1630
    },
    {
      "epoch": 0.47953216374269003,
      "grad_norm": 0.8290591835975647,
      "learning_rate": 0.00018630743525480367,
      "loss": 1.1815,
      "step": 1640
    },
    {
      "epoch": 0.4824561403508772,
      "grad_norm": 0.8503272533416748,
      "learning_rate": 0.00018622389306599833,
      "loss": 1.1183,
      "step": 1650
    },
    {
      "epoch": 0.4853801169590643,
      "grad_norm": 1.07460355758667,
      "learning_rate": 0.000186140350877193,
      "loss": 1.1071,
      "step": 1660
    },
    {
      "epoch": 0.48830409356725146,
      "grad_norm": 1.1698830127716064,
      "learning_rate": 0.00018605680868838764,
      "loss": 1.1307,
      "step": 1670
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 0.9066745638847351,
      "learning_rate": 0.00018597326649958228,
      "loss": 1.1065,
      "step": 1680
    },
    {
      "epoch": 0.49415204678362573,
      "grad_norm": 1.0201914310455322,
      "learning_rate": 0.00018588972431077695,
      "loss": 1.1735,
      "step": 1690
    },
    {
      "epoch": 0.49707602339181284,
      "grad_norm": 0.8127151131629944,
      "learning_rate": 0.0001858061821219716,
      "loss": 1.1492,
      "step": 1700
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.2191109657287598,
      "learning_rate": 0.00018572263993316625,
      "loss": 1.1362,
      "step": 1710
    },
    {
      "epoch": 0.5029239766081871,
      "grad_norm": 0.8854021430015564,
      "learning_rate": 0.0001856390977443609,
      "loss": 1.1413,
      "step": 1720
    },
    {
      "epoch": 0.5058479532163743,
      "grad_norm": 0.851799726486206,
      "learning_rate": 0.00018555555555555556,
      "loss": 1.164,
      "step": 1730
    },
    {
      "epoch": 0.5087719298245614,
      "grad_norm": 1.124094843864441,
      "learning_rate": 0.00018547201336675023,
      "loss": 1.2223,
      "step": 1740
    },
    {
      "epoch": 0.5116959064327485,
      "grad_norm": 1.0798547267913818,
      "learning_rate": 0.00018538847117794487,
      "loss": 1.1538,
      "step": 1750
    },
    {
      "epoch": 0.5146198830409356,
      "grad_norm": 0.7804651260375977,
      "learning_rate": 0.0001853049289891395,
      "loss": 1.1506,
      "step": 1760
    },
    {
      "epoch": 0.5175438596491229,
      "grad_norm": 0.8263794183731079,
      "learning_rate": 0.00018522138680033417,
      "loss": 1.1144,
      "step": 1770
    },
    {
      "epoch": 0.52046783625731,
      "grad_norm": 1.023290991783142,
      "learning_rate": 0.00018513784461152884,
      "loss": 1.1333,
      "step": 1780
    },
    {
      "epoch": 0.5233918128654971,
      "grad_norm": 1.0277137756347656,
      "learning_rate": 0.00018505430242272348,
      "loss": 1.1762,
      "step": 1790
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 0.8248470425605774,
      "learning_rate": 0.00018497076023391812,
      "loss": 1.0764,
      "step": 1800
    },
    {
      "epoch": 0.5292397660818714,
      "grad_norm": 0.9237201809883118,
      "learning_rate": 0.0001848872180451128,
      "loss": 1.1272,
      "step": 1810
    },
    {
      "epoch": 0.5321637426900585,
      "grad_norm": 0.7865254282951355,
      "learning_rate": 0.00018480367585630746,
      "loss": 1.1197,
      "step": 1820
    },
    {
      "epoch": 0.5350877192982456,
      "grad_norm": 0.9061840176582336,
      "learning_rate": 0.0001847201336675021,
      "loss": 1.07,
      "step": 1830
    },
    {
      "epoch": 0.5380116959064327,
      "grad_norm": 0.9895060658454895,
      "learning_rate": 0.00018463659147869674,
      "loss": 1.1963,
      "step": 1840
    },
    {
      "epoch": 0.5409356725146199,
      "grad_norm": 0.6712530255317688,
      "learning_rate": 0.0001845530492898914,
      "loss": 1.1192,
      "step": 1850
    },
    {
      "epoch": 0.543859649122807,
      "grad_norm": 0.8928467035293579,
      "learning_rate": 0.00018446950710108607,
      "loss": 1.1789,
      "step": 1860
    },
    {
      "epoch": 0.5467836257309941,
      "grad_norm": 0.8384220004081726,
      "learning_rate": 0.0001843859649122807,
      "loss": 1.1101,
      "step": 1870
    },
    {
      "epoch": 0.5497076023391813,
      "grad_norm": 0.6848403811454773,
      "learning_rate": 0.00018430242272347535,
      "loss": 1.0046,
      "step": 1880
    },
    {
      "epoch": 0.5526315789473685,
      "grad_norm": 0.8462963104248047,
      "learning_rate": 0.00018421888053467002,
      "loss": 1.0328,
      "step": 1890
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.0406112670898438,
      "learning_rate": 0.00018413533834586468,
      "loss": 1.2153,
      "step": 1900
    },
    {
      "epoch": 0.5584795321637427,
      "grad_norm": 1.0375579595565796,
      "learning_rate": 0.00018405179615705932,
      "loss": 1.1109,
      "step": 1910
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 1.246148705482483,
      "learning_rate": 0.00018396825396825396,
      "loss": 1.1491,
      "step": 1920
    },
    {
      "epoch": 0.564327485380117,
      "grad_norm": 1.046627163887024,
      "learning_rate": 0.00018388471177944863,
      "loss": 1.1925,
      "step": 1930
    },
    {
      "epoch": 0.5672514619883041,
      "grad_norm": 1.0142565965652466,
      "learning_rate": 0.0001838011695906433,
      "loss": 1.1532,
      "step": 1940
    },
    {
      "epoch": 0.5701754385964912,
      "grad_norm": 0.9693454504013062,
      "learning_rate": 0.00018371762740183794,
      "loss": 1.1932,
      "step": 1950
    },
    {
      "epoch": 0.5730994152046783,
      "grad_norm": 1.054888367652893,
      "learning_rate": 0.00018363408521303258,
      "loss": 1.1777,
      "step": 1960
    },
    {
      "epoch": 0.5760233918128655,
      "grad_norm": 0.908535361289978,
      "learning_rate": 0.00018355054302422724,
      "loss": 1.1279,
      "step": 1970
    },
    {
      "epoch": 0.5789473684210527,
      "grad_norm": 0.8593538999557495,
      "learning_rate": 0.0001834670008354219,
      "loss": 1.1125,
      "step": 1980
    },
    {
      "epoch": 0.5818713450292398,
      "grad_norm": 0.8350898623466492,
      "learning_rate": 0.00018338345864661655,
      "loss": 1.1094,
      "step": 1990
    },
    {
      "epoch": 0.5847953216374269,
      "grad_norm": 1.0109652280807495,
      "learning_rate": 0.0001832999164578112,
      "loss": 1.12,
      "step": 2000
    },
    {
      "epoch": 0.5877192982456141,
      "grad_norm": 1.0049610137939453,
      "learning_rate": 0.00018321637426900586,
      "loss": 1.0708,
      "step": 2010
    },
    {
      "epoch": 0.5906432748538012,
      "grad_norm": 0.8329327702522278,
      "learning_rate": 0.0001831328320802005,
      "loss": 1.0576,
      "step": 2020
    },
    {
      "epoch": 0.5935672514619883,
      "grad_norm": 1.0239663124084473,
      "learning_rate": 0.00018304928989139517,
      "loss": 1.0427,
      "step": 2030
    },
    {
      "epoch": 0.5964912280701754,
      "grad_norm": 0.7969609498977661,
      "learning_rate": 0.0001829657477025898,
      "loss": 1.046,
      "step": 2040
    },
    {
      "epoch": 0.5994152046783626,
      "grad_norm": 0.8774564266204834,
      "learning_rate": 0.00018288220551378447,
      "loss": 1.0751,
      "step": 2050
    },
    {
      "epoch": 0.6023391812865497,
      "grad_norm": 0.9628018736839294,
      "learning_rate": 0.0001827986633249791,
      "loss": 1.0465,
      "step": 2060
    },
    {
      "epoch": 0.6052631578947368,
      "grad_norm": 0.7977367639541626,
      "learning_rate": 0.00018271512113617378,
      "loss": 1.1385,
      "step": 2070
    },
    {
      "epoch": 0.6081871345029239,
      "grad_norm": 0.9023635387420654,
      "learning_rate": 0.00018263157894736842,
      "loss": 1.0609,
      "step": 2080
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 0.9256485104560852,
      "learning_rate": 0.00018254803675856309,
      "loss": 1.1185,
      "step": 2090
    },
    {
      "epoch": 0.6140350877192983,
      "grad_norm": 1.2260334491729736,
      "learning_rate": 0.00018246449456975773,
      "loss": 1.1806,
      "step": 2100
    },
    {
      "epoch": 0.6169590643274854,
      "grad_norm": 0.9987436532974243,
      "learning_rate": 0.0001823809523809524,
      "loss": 1.2613,
      "step": 2110
    },
    {
      "epoch": 0.6198830409356725,
      "grad_norm": 1.215234398841858,
      "learning_rate": 0.00018229741019214703,
      "loss": 1.1562,
      "step": 2120
    },
    {
      "epoch": 0.6228070175438597,
      "grad_norm": 0.7789454460144043,
      "learning_rate": 0.0001822138680033417,
      "loss": 1.058,
      "step": 2130
    },
    {
      "epoch": 0.6257309941520468,
      "grad_norm": 0.8379114866256714,
      "learning_rate": 0.00018213032581453634,
      "loss": 1.0693,
      "step": 2140
    },
    {
      "epoch": 0.6286549707602339,
      "grad_norm": 1.0519347190856934,
      "learning_rate": 0.000182046783625731,
      "loss": 1.2349,
      "step": 2150
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 1.1658622026443481,
      "learning_rate": 0.00018196324143692565,
      "loss": 1.5638,
      "step": 2160
    },
    {
      "epoch": 0.6345029239766082,
      "grad_norm": 0.9269095659255981,
      "learning_rate": 0.00018187969924812031,
      "loss": 1.5042,
      "step": 2170
    },
    {
      "epoch": 0.6374269005847953,
      "grad_norm": 0.8052358031272888,
      "learning_rate": 0.00018179615705931495,
      "loss": 1.0546,
      "step": 2180
    },
    {
      "epoch": 0.6403508771929824,
      "grad_norm": 0.8200955390930176,
      "learning_rate": 0.00018171261487050962,
      "loss": 1.075,
      "step": 2190
    },
    {
      "epoch": 0.6432748538011696,
      "grad_norm": 0.7697106003761292,
      "learning_rate": 0.00018162907268170426,
      "loss": 1.1351,
      "step": 2200
    },
    {
      "epoch": 0.6461988304093568,
      "grad_norm": 8745.8359375,
      "learning_rate": 0.00018154553049289893,
      "loss": 1.3314,
      "step": 2210
    },
    {
      "epoch": 0.6491228070175439,
      "grad_norm": 0.954134464263916,
      "learning_rate": 0.00018146198830409357,
      "loss": 1.3329,
      "step": 2220
    },
    {
      "epoch": 0.652046783625731,
      "grad_norm": 0.8199519515037537,
      "learning_rate": 0.00018137844611528824,
      "loss": 1.2668,
      "step": 2230
    },
    {
      "epoch": 0.6549707602339181,
      "grad_norm": 0.9622941017150879,
      "learning_rate": 0.00018129490392648288,
      "loss": 1.0814,
      "step": 2240
    },
    {
      "epoch": 0.6578947368421053,
      "grad_norm": 0.9495453238487244,
      "learning_rate": 0.00018121136173767754,
      "loss": 1.1039,
      "step": 2250
    },
    {
      "epoch": 0.6608187134502924,
      "grad_norm": 0.844725489616394,
      "learning_rate": 0.00018112781954887218,
      "loss": 1.1747,
      "step": 2260
    },
    {
      "epoch": 0.6637426900584795,
      "grad_norm": 7730.90234375,
      "learning_rate": 0.00018104427736006685,
      "loss": 1.3738,
      "step": 2270
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3605.6982421875,
      "learning_rate": 0.0001809607351712615,
      "loss": 1.3729,
      "step": 2280
    },
    {
      "epoch": 0.6695906432748538,
      "grad_norm": 0.8641389012336731,
      "learning_rate": 0.00018087719298245616,
      "loss": 1.1469,
      "step": 2290
    },
    {
      "epoch": 0.672514619883041,
      "grad_norm": 1.1081582307815552,
      "learning_rate": 0.0001807936507936508,
      "loss": 1.3129,
      "step": 2300
    },
    {
      "epoch": 0.6754385964912281,
      "grad_norm": 0.918106198310852,
      "learning_rate": 0.00018071010860484546,
      "loss": 1.1429,
      "step": 2310
    },
    {
      "epoch": 0.6783625730994152,
      "grad_norm": 1.0303874015808105,
      "learning_rate": 0.0001806265664160401,
      "loss": 1.1827,
      "step": 2320
    },
    {
      "epoch": 0.6812865497076024,
      "grad_norm": 0.8491673469543457,
      "learning_rate": 0.00018054302422723477,
      "loss": 1.0731,
      "step": 2330
    },
    {
      "epoch": 0.6842105263157895,
      "grad_norm": 0.8926559686660767,
      "learning_rate": 0.0001804594820384294,
      "loss": 1.1669,
      "step": 2340
    },
    {
      "epoch": 0.6871345029239766,
      "grad_norm": 0.8698117136955261,
      "learning_rate": 0.00018037593984962408,
      "loss": 1.1168,
      "step": 2350
    },
    {
      "epoch": 0.6900584795321637,
      "grad_norm": 1.1443090438842773,
      "learning_rate": 0.00018029239766081872,
      "loss": 1.095,
      "step": 2360
    },
    {
      "epoch": 0.6929824561403509,
      "grad_norm": 0.9757795333862305,
      "learning_rate": 0.00018020885547201338,
      "loss": 1.0947,
      "step": 2370
    },
    {
      "epoch": 0.695906432748538,
      "grad_norm": 0.957348644733429,
      "learning_rate": 0.00018012531328320802,
      "loss": 1.0606,
      "step": 2380
    },
    {
      "epoch": 0.6988304093567251,
      "grad_norm": 0.8182503581047058,
      "learning_rate": 0.0001800417710944027,
      "loss": 1.0925,
      "step": 2390
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 0.8263635039329529,
      "learning_rate": 0.00017995822890559733,
      "loss": 1.0832,
      "step": 2400
    },
    {
      "epoch": 0.7046783625730995,
      "grad_norm": 1.0016945600509644,
      "learning_rate": 0.000179874686716792,
      "loss": 1.1071,
      "step": 2410
    },
    {
      "epoch": 0.7076023391812866,
      "grad_norm": 0.9769853353500366,
      "learning_rate": 0.00017979114452798664,
      "loss": 1.2368,
      "step": 2420
    },
    {
      "epoch": 0.7105263157894737,
      "grad_norm": 1.45025634765625,
      "learning_rate": 0.00017970760233918128,
      "loss": 1.1487,
      "step": 2430
    },
    {
      "epoch": 0.7134502923976608,
      "grad_norm": 0.9546375870704651,
      "learning_rate": 0.00017962406015037595,
      "loss": 1.2271,
      "step": 2440
    },
    {
      "epoch": 0.716374269005848,
      "grad_norm": 0.9210566282272339,
      "learning_rate": 0.0001795405179615706,
      "loss": 1.1522,
      "step": 2450
    },
    {
      "epoch": 0.7192982456140351,
      "grad_norm": 0.8689937591552734,
      "learning_rate": 0.00017945697577276525,
      "loss": 1.0831,
      "step": 2460
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.6933755278587341,
      "learning_rate": 0.0001793734335839599,
      "loss": 1.0637,
      "step": 2470
    },
    {
      "epoch": 0.7251461988304093,
      "grad_norm": 0.9806301593780518,
      "learning_rate": 0.00017928989139515456,
      "loss": 1.0417,
      "step": 2480
    },
    {
      "epoch": 0.7280701754385965,
      "grad_norm": 0.9516319036483765,
      "learning_rate": 0.00017920634920634923,
      "loss": 1.1551,
      "step": 2490
    },
    {
      "epoch": 0.7309941520467836,
      "grad_norm": 0.8186897039413452,
      "learning_rate": 0.00017912280701754387,
      "loss": 1.0497,
      "step": 2500
    },
    {
      "epoch": 0.7339181286549707,
      "grad_norm": 0.9713053107261658,
      "learning_rate": 0.0001790392648287385,
      "loss": 1.1078,
      "step": 2510
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 0.8958616256713867,
      "learning_rate": 0.00017895572263993317,
      "loss": 1.0478,
      "step": 2520
    },
    {
      "epoch": 0.7397660818713451,
      "grad_norm": 0.9957031011581421,
      "learning_rate": 0.00017887218045112784,
      "loss": 1.113,
      "step": 2530
    },
    {
      "epoch": 0.7426900584795322,
      "grad_norm": 1.5397838354110718,
      "learning_rate": 0.00017878863826232248,
      "loss": 1.1867,
      "step": 2540
    },
    {
      "epoch": 0.7456140350877193,
      "grad_norm": 0.9021263122558594,
      "learning_rate": 0.00017870509607351712,
      "loss": 1.1114,
      "step": 2550
    },
    {
      "epoch": 0.7485380116959064,
      "grad_norm": 0.9078192710876465,
      "learning_rate": 0.0001786215538847118,
      "loss": 1.1915,
      "step": 2560
    },
    {
      "epoch": 0.7514619883040936,
      "grad_norm": 1.0004767179489136,
      "learning_rate": 0.00017853801169590645,
      "loss": 1.1165,
      "step": 2570
    },
    {
      "epoch": 0.7543859649122807,
      "grad_norm": 0.827596127986908,
      "learning_rate": 0.0001784544695071011,
      "loss": 1.1542,
      "step": 2580
    },
    {
      "epoch": 0.7573099415204678,
      "grad_norm": 0.9124414920806885,
      "learning_rate": 0.00017837092731829573,
      "loss": 1.1868,
      "step": 2590
    },
    {
      "epoch": 0.7602339181286549,
      "grad_norm": 0.889372706413269,
      "learning_rate": 0.0001782873851294904,
      "loss": 1.0633,
      "step": 2600
    },
    {
      "epoch": 0.7631578947368421,
      "grad_norm": 0.8362826108932495,
      "learning_rate": 0.00017820384294068507,
      "loss": 1.085,
      "step": 2610
    },
    {
      "epoch": 0.7660818713450293,
      "grad_norm": 0.8905161619186401,
      "learning_rate": 0.0001781203007518797,
      "loss": 1.012,
      "step": 2620
    },
    {
      "epoch": 0.7690058479532164,
      "grad_norm": 0.7526170611381531,
      "learning_rate": 0.00017803675856307435,
      "loss": 1.0942,
      "step": 2630
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 0.9705289006233215,
      "learning_rate": 0.00017795321637426902,
      "loss": 1.1232,
      "step": 2640
    },
    {
      "epoch": 0.7748538011695907,
      "grad_norm": 0.9866281747817993,
      "learning_rate": 0.00017786967418546368,
      "loss": 1.0858,
      "step": 2650
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.7876876592636108,
      "learning_rate": 0.00017778613199665832,
      "loss": 1.0627,
      "step": 2660
    },
    {
      "epoch": 0.7807017543859649,
      "grad_norm": 0.8494402170181274,
      "learning_rate": 0.00017770258980785296,
      "loss": 1.0495,
      "step": 2670
    },
    {
      "epoch": 0.783625730994152,
      "grad_norm": 0.8814900517463684,
      "learning_rate": 0.00017761904761904763,
      "loss": 1.097,
      "step": 2680
    },
    {
      "epoch": 0.7865497076023392,
      "grad_norm": 0.844111979007721,
      "learning_rate": 0.0001775355054302423,
      "loss": 1.1784,
      "step": 2690
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 0.7920035719871521,
      "learning_rate": 0.00017745196324143694,
      "loss": 1.1571,
      "step": 2700
    },
    {
      "epoch": 0.7923976608187134,
      "grad_norm": 0.9765743017196655,
      "learning_rate": 0.00017736842105263158,
      "loss": 1.0957,
      "step": 2710
    },
    {
      "epoch": 0.7953216374269005,
      "grad_norm": 0.7414309978485107,
      "learning_rate": 0.00017728487886382624,
      "loss": 1.0524,
      "step": 2720
    },
    {
      "epoch": 0.7982456140350878,
      "grad_norm": 0.6484593152999878,
      "learning_rate": 0.0001772013366750209,
      "loss": 1.1376,
      "step": 2730
    },
    {
      "epoch": 0.8011695906432749,
      "grad_norm": 1.3010772466659546,
      "learning_rate": 0.00017711779448621555,
      "loss": 1.096,
      "step": 2740
    },
    {
      "epoch": 0.804093567251462,
      "grad_norm": 0.8591032028198242,
      "learning_rate": 0.0001770342522974102,
      "loss": 1.0547,
      "step": 2750
    },
    {
      "epoch": 0.8070175438596491,
      "grad_norm": 0.9882166385650635,
      "learning_rate": 0.00017695071010860486,
      "loss": 1.0257,
      "step": 2760
    },
    {
      "epoch": 0.8099415204678363,
      "grad_norm": 1.047082781791687,
      "learning_rate": 0.00017686716791979952,
      "loss": 1.1093,
      "step": 2770
    },
    {
      "epoch": 0.8128654970760234,
      "grad_norm": 0.9633401036262512,
      "learning_rate": 0.00017678362573099416,
      "loss": 1.1393,
      "step": 2780
    },
    {
      "epoch": 0.8157894736842105,
      "grad_norm": 0.9879512786865234,
      "learning_rate": 0.0001767000835421888,
      "loss": 1.0407,
      "step": 2790
    },
    {
      "epoch": 0.8187134502923976,
      "grad_norm": 0.9814889430999756,
      "learning_rate": 0.00017661654135338347,
      "loss": 1.1188,
      "step": 2800
    },
    {
      "epoch": 0.8216374269005848,
      "grad_norm": 0.6720944046974182,
      "learning_rate": 0.00017653299916457814,
      "loss": 1.0565,
      "step": 2810
    },
    {
      "epoch": 0.8245614035087719,
      "grad_norm": 0.9213815927505493,
      "learning_rate": 0.00017644945697577278,
      "loss": 1.1072,
      "step": 2820
    },
    {
      "epoch": 0.827485380116959,
      "grad_norm": 0.9928708076477051,
      "learning_rate": 0.00017636591478696742,
      "loss": 1.03,
      "step": 2830
    },
    {
      "epoch": 0.8304093567251462,
      "grad_norm": 0.831228494644165,
      "learning_rate": 0.00017628237259816206,
      "loss": 1.0833,
      "step": 2840
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.9433948993682861,
      "learning_rate": 0.00017619883040935675,
      "loss": 1.0782,
      "step": 2850
    },
    {
      "epoch": 0.8362573099415205,
      "grad_norm": 0.697948694229126,
      "learning_rate": 0.0001761152882205514,
      "loss": 1.0371,
      "step": 2860
    },
    {
      "epoch": 0.8391812865497076,
      "grad_norm": 1.2577370405197144,
      "learning_rate": 0.00017603174603174603,
      "loss": 1.1313,
      "step": 2870
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 0.886056661605835,
      "learning_rate": 0.00017594820384294067,
      "loss": 1.0526,
      "step": 2880
    },
    {
      "epoch": 0.8450292397660819,
      "grad_norm": 0.8373905420303345,
      "learning_rate": 0.00017586466165413537,
      "loss": 1.1942,
      "step": 2890
    },
    {
      "epoch": 0.847953216374269,
      "grad_norm": 0.8510035872459412,
      "learning_rate": 0.00017578111946533,
      "loss": 1.026,
      "step": 2900
    },
    {
      "epoch": 0.8508771929824561,
      "grad_norm": 0.8685462474822998,
      "learning_rate": 0.00017569757727652465,
      "loss": 1.051,
      "step": 2910
    },
    {
      "epoch": 0.8538011695906432,
      "grad_norm": 0.974135160446167,
      "learning_rate": 0.00017561403508771929,
      "loss": 1.1238,
      "step": 2920
    },
    {
      "epoch": 0.8567251461988304,
      "grad_norm": 0.7720366716384888,
      "learning_rate": 0.00017553049289891398,
      "loss": 1.0989,
      "step": 2930
    },
    {
      "epoch": 0.8596491228070176,
      "grad_norm": 0.8919097781181335,
      "learning_rate": 0.00017544695071010862,
      "loss": 1.1849,
      "step": 2940
    },
    {
      "epoch": 0.8625730994152047,
      "grad_norm": 1.1132488250732422,
      "learning_rate": 0.00017536340852130326,
      "loss": 1.0683,
      "step": 2950
    },
    {
      "epoch": 0.8654970760233918,
      "grad_norm": 0.8050568699836731,
      "learning_rate": 0.0001752798663324979,
      "loss": 1.1176,
      "step": 2960
    },
    {
      "epoch": 0.868421052631579,
      "grad_norm": 1.209118127822876,
      "learning_rate": 0.00017519632414369257,
      "loss": 1.1775,
      "step": 2970
    },
    {
      "epoch": 0.8713450292397661,
      "grad_norm": 0.9077125191688538,
      "learning_rate": 0.00017511278195488723,
      "loss": 1.1045,
      "step": 2980
    },
    {
      "epoch": 0.8742690058479532,
      "grad_norm": 0.720100462436676,
      "learning_rate": 0.00017502923976608187,
      "loss": 1.0431,
      "step": 2990
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 0.8860930800437927,
      "learning_rate": 0.00017494569757727651,
      "loss": 1.0013,
      "step": 3000
    },
    {
      "epoch": 0.8801169590643275,
      "grad_norm": 0.7640936970710754,
      "learning_rate": 0.00017486215538847118,
      "loss": 1.0028,
      "step": 3010
    },
    {
      "epoch": 0.8830409356725146,
      "grad_norm": 0.9175049066543579,
      "learning_rate": 0.00017477861319966585,
      "loss": 1.1821,
      "step": 3020
    },
    {
      "epoch": 0.8859649122807017,
      "grad_norm": 0.9965119361877441,
      "learning_rate": 0.0001746950710108605,
      "loss": 1.1405,
      "step": 3030
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.8234232068061829,
      "learning_rate": 0.00017461152882205513,
      "loss": 1.0677,
      "step": 3040
    },
    {
      "epoch": 0.8918128654970761,
      "grad_norm": 0.8172674179077148,
      "learning_rate": 0.0001745279866332498,
      "loss": 1.161,
      "step": 3050
    },
    {
      "epoch": 0.8947368421052632,
      "grad_norm": 0.9791858792304993,
      "learning_rate": 0.00017444444444444446,
      "loss": 1.1157,
      "step": 3060
    },
    {
      "epoch": 0.8976608187134503,
      "grad_norm": 0.7376898527145386,
      "learning_rate": 0.0001743609022556391,
      "loss": 1.1386,
      "step": 3070
    },
    {
      "epoch": 0.9005847953216374,
      "grad_norm": 0.8130390644073486,
      "learning_rate": 0.00017427736006683374,
      "loss": 1.0647,
      "step": 3080
    },
    {
      "epoch": 0.9035087719298246,
      "grad_norm": 0.7421343326568604,
      "learning_rate": 0.0001741938178780284,
      "loss": 1.0638,
      "step": 3090
    },
    {
      "epoch": 0.9064327485380117,
      "grad_norm": 0.8935278654098511,
      "learning_rate": 0.00017411027568922308,
      "loss": 1.0059,
      "step": 3100
    },
    {
      "epoch": 0.9093567251461988,
      "grad_norm": 0.7259190678596497,
      "learning_rate": 0.00017402673350041772,
      "loss": 1.0376,
      "step": 3110
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 0.7598577737808228,
      "learning_rate": 0.00017394319131161236,
      "loss": 1.0302,
      "step": 3120
    },
    {
      "epoch": 0.9152046783625731,
      "grad_norm": 0.8800897002220154,
      "learning_rate": 0.00017385964912280702,
      "loss": 1.0405,
      "step": 3130
    },
    {
      "epoch": 0.9181286549707602,
      "grad_norm": 0.8561326265335083,
      "learning_rate": 0.0001737761069340017,
      "loss": 1.0838,
      "step": 3140
    },
    {
      "epoch": 0.9210526315789473,
      "grad_norm": 0.6592310070991516,
      "learning_rate": 0.00017369256474519633,
      "loss": 1.1518,
      "step": 3150
    },
    {
      "epoch": 0.9239766081871345,
      "grad_norm": 0.8872536420822144,
      "learning_rate": 0.00017360902255639097,
      "loss": 1.0867,
      "step": 3160
    },
    {
      "epoch": 0.9269005847953217,
      "grad_norm": 0.7063078880310059,
      "learning_rate": 0.00017352548036758564,
      "loss": 1.0218,
      "step": 3170
    },
    {
      "epoch": 0.9298245614035088,
      "grad_norm": 0.9230292439460754,
      "learning_rate": 0.0001734419381787803,
      "loss": 1.0209,
      "step": 3180
    },
    {
      "epoch": 0.9327485380116959,
      "grad_norm": 0.7236250042915344,
      "learning_rate": 0.00017335839598997494,
      "loss": 1.0953,
      "step": 3190
    },
    {
      "epoch": 0.935672514619883,
      "grad_norm": 0.9351952075958252,
      "learning_rate": 0.00017327485380116958,
      "loss": 1.0693,
      "step": 3200
    },
    {
      "epoch": 0.9385964912280702,
      "grad_norm": 0.9345112442970276,
      "learning_rate": 0.00017319131161236425,
      "loss": 1.0743,
      "step": 3210
    },
    {
      "epoch": 0.9415204678362573,
      "grad_norm": 0.9278674721717834,
      "learning_rate": 0.00017310776942355892,
      "loss": 1.0695,
      "step": 3220
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.7766509652137756,
      "learning_rate": 0.00017302422723475356,
      "loss": 1.0926,
      "step": 3230
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 0.7468211054801941,
      "learning_rate": 0.0001729406850459482,
      "loss": 1.0927,
      "step": 3240
    },
    {
      "epoch": 0.9502923976608187,
      "grad_norm": 0.8987843990325928,
      "learning_rate": 0.00017285714285714287,
      "loss": 1.0489,
      "step": 3250
    },
    {
      "epoch": 0.9532163742690059,
      "grad_norm": 0.8875238299369812,
      "learning_rate": 0.00017277360066833753,
      "loss": 1.0955,
      "step": 3260
    },
    {
      "epoch": 0.956140350877193,
      "grad_norm": 0.7056061029434204,
      "learning_rate": 0.00017269005847953217,
      "loss": 1.0774,
      "step": 3270
    },
    {
      "epoch": 0.9590643274853801,
      "grad_norm": 1.39278244972229,
      "learning_rate": 0.0001726065162907268,
      "loss": 1.233,
      "step": 3280
    },
    {
      "epoch": 0.9619883040935673,
      "grad_norm": 0.8074460029602051,
      "learning_rate": 0.00017252297410192148,
      "loss": 1.0677,
      "step": 3290
    },
    {
      "epoch": 0.9649122807017544,
      "grad_norm": 0.8768604397773743,
      "learning_rate": 0.00017243943191311615,
      "loss": 1.1278,
      "step": 3300
    },
    {
      "epoch": 0.9678362573099415,
      "grad_norm": 1.2095117568969727,
      "learning_rate": 0.00017235588972431079,
      "loss": 1.084,
      "step": 3310
    },
    {
      "epoch": 0.9707602339181286,
      "grad_norm": 0.9521526098251343,
      "learning_rate": 0.00017227234753550543,
      "loss": 1.047,
      "step": 3320
    },
    {
      "epoch": 0.9736842105263158,
      "grad_norm": 0.7435516119003296,
      "learning_rate": 0.0001721888053467001,
      "loss": 1.1088,
      "step": 3330
    },
    {
      "epoch": 0.9766081871345029,
      "grad_norm": 0.8615549802780151,
      "learning_rate": 0.00017210526315789476,
      "loss": 1.0835,
      "step": 3340
    },
    {
      "epoch": 0.97953216374269,
      "grad_norm": 1.039450764656067,
      "learning_rate": 0.0001720217209690894,
      "loss": 1.1372,
      "step": 3350
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 0.7786360383033752,
      "learning_rate": 0.00017193817878028404,
      "loss": 1.0761,
      "step": 3360
    },
    {
      "epoch": 0.9853801169590644,
      "grad_norm": 0.7369186878204346,
      "learning_rate": 0.0001718546365914787,
      "loss": 1.0346,
      "step": 3370
    },
    {
      "epoch": 0.9883040935672515,
      "grad_norm": 0.72834712266922,
      "learning_rate": 0.00017177109440267335,
      "loss": 1.1518,
      "step": 3380
    },
    {
      "epoch": 0.9912280701754386,
      "grad_norm": 0.7792141437530518,
      "learning_rate": 0.00017168755221386801,
      "loss": 1.152,
      "step": 3390
    },
    {
      "epoch": 0.9941520467836257,
      "grad_norm": 0.8308858275413513,
      "learning_rate": 0.00017160401002506265,
      "loss": 1.1786,
      "step": 3400
    },
    {
      "epoch": 0.9970760233918129,
      "grad_norm": 0.8694866299629211,
      "learning_rate": 0.00017152046783625732,
      "loss": 1.1648,
      "step": 3410
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.1654472351074219,
      "learning_rate": 0.00017143692564745196,
      "loss": 1.103,
      "step": 3420
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.0407040119171143,
      "eval_runtime": 76.1735,
      "eval_samples_per_second": 11.671,
      "eval_steps_per_second": 1.47,
      "step": 3420
    },
    {
      "epoch": 1.0029239766081872,
      "grad_norm": 0.7534571290016174,
      "learning_rate": 0.00017135338345864663,
      "loss": 0.9951,
      "step": 3430
    },
    {
      "epoch": 1.0058479532163742,
      "grad_norm": 0.9725307822227478,
      "learning_rate": 0.00017126984126984127,
      "loss": 0.9882,
      "step": 3440
    },
    {
      "epoch": 1.0087719298245614,
      "grad_norm": 0.7010172009468079,
      "learning_rate": 0.00017118629908103594,
      "loss": 0.9961,
      "step": 3450
    },
    {
      "epoch": 1.0116959064327484,
      "grad_norm": 0.8354718089103699,
      "learning_rate": 0.00017110275689223058,
      "loss": 1.0811,
      "step": 3460
    },
    {
      "epoch": 1.0146198830409356,
      "grad_norm": 0.708854615688324,
      "learning_rate": 0.00017101921470342524,
      "loss": 1.0559,
      "step": 3470
    },
    {
      "epoch": 1.0175438596491229,
      "grad_norm": 0.7244396805763245,
      "learning_rate": 0.00017093567251461988,
      "loss": 1.0106,
      "step": 3480
    },
    {
      "epoch": 1.0204678362573099,
      "grad_norm": 0.8345986604690552,
      "learning_rate": 0.00017085213032581455,
      "loss": 1.1255,
      "step": 3490
    },
    {
      "epoch": 1.023391812865497,
      "grad_norm": 0.8193466067314148,
      "learning_rate": 0.0001707685881370092,
      "loss": 0.9881,
      "step": 3500
    },
    {
      "epoch": 1.0263157894736843,
      "grad_norm": 0.9618562459945679,
      "learning_rate": 0.00017068504594820386,
      "loss": 1.106,
      "step": 3510
    },
    {
      "epoch": 1.0292397660818713,
      "grad_norm": 0.9041397571563721,
      "learning_rate": 0.0001706015037593985,
      "loss": 1.005,
      "step": 3520
    },
    {
      "epoch": 1.0321637426900585,
      "grad_norm": 1.0911697149276733,
      "learning_rate": 0.00017051796157059316,
      "loss": 1.1054,
      "step": 3530
    },
    {
      "epoch": 1.0350877192982457,
      "grad_norm": 1.1844065189361572,
      "learning_rate": 0.0001704344193817878,
      "loss": 1.074,
      "step": 3540
    },
    {
      "epoch": 1.0380116959064327,
      "grad_norm": 0.8022108674049377,
      "learning_rate": 0.00017035087719298247,
      "loss": 1.0773,
      "step": 3550
    },
    {
      "epoch": 1.04093567251462,
      "grad_norm": 0.7523813843727112,
      "learning_rate": 0.0001702673350041771,
      "loss": 1.055,
      "step": 3560
    },
    {
      "epoch": 1.043859649122807,
      "grad_norm": 1.084007740020752,
      "learning_rate": 0.00017018379281537178,
      "loss": 1.0548,
      "step": 3570
    },
    {
      "epoch": 1.0467836257309941,
      "grad_norm": 0.9543949961662292,
      "learning_rate": 0.00017010025062656642,
      "loss": 0.9605,
      "step": 3580
    },
    {
      "epoch": 1.0497076023391814,
      "grad_norm": 0.7240610718727112,
      "learning_rate": 0.00017001670843776108,
      "loss": 1.0692,
      "step": 3590
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 0.8080556988716125,
      "learning_rate": 0.00016993316624895572,
      "loss": 1.0614,
      "step": 3600
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.685502290725708,
      "learning_rate": 0.0001698496240601504,
      "loss": 1.0001,
      "step": 3610
    },
    {
      "epoch": 1.0584795321637426,
      "grad_norm": 0.7907028794288635,
      "learning_rate": 0.00016976608187134503,
      "loss": 1.0797,
      "step": 3620
    },
    {
      "epoch": 1.0614035087719298,
      "grad_norm": 0.8253843188285828,
      "learning_rate": 0.0001696825396825397,
      "loss": 1.0452,
      "step": 3630
    },
    {
      "epoch": 1.064327485380117,
      "grad_norm": 0.7496328353881836,
      "learning_rate": 0.00016959899749373434,
      "loss": 1.0091,
      "step": 3640
    },
    {
      "epoch": 1.067251461988304,
      "grad_norm": 0.9333305954933167,
      "learning_rate": 0.000169515455304929,
      "loss": 1.0597,
      "step": 3650
    },
    {
      "epoch": 1.0701754385964912,
      "grad_norm": 0.7602313160896301,
      "learning_rate": 0.00016943191311612365,
      "loss": 1.0629,
      "step": 3660
    },
    {
      "epoch": 1.0730994152046784,
      "grad_norm": 0.7980261445045471,
      "learning_rate": 0.0001693483709273183,
      "loss": 1.0161,
      "step": 3670
    },
    {
      "epoch": 1.0760233918128654,
      "grad_norm": 0.8856683373451233,
      "learning_rate": 0.00016926482873851295,
      "loss": 1.1239,
      "step": 3680
    },
    {
      "epoch": 1.0789473684210527,
      "grad_norm": 1.1038621664047241,
      "learning_rate": 0.00016918128654970762,
      "loss": 1.1233,
      "step": 3690
    },
    {
      "epoch": 1.0818713450292399,
      "grad_norm": 0.8966583013534546,
      "learning_rate": 0.00016909774436090226,
      "loss": 1.0709,
      "step": 3700
    },
    {
      "epoch": 1.0847953216374269,
      "grad_norm": 0.7407881617546082,
      "learning_rate": 0.00016901420217209693,
      "loss": 1.0197,
      "step": 3710
    },
    {
      "epoch": 1.087719298245614,
      "grad_norm": 0.9601074457168579,
      "learning_rate": 0.00016893065998329157,
      "loss": 0.9872,
      "step": 3720
    },
    {
      "epoch": 1.090643274853801,
      "grad_norm": 1.0446343421936035,
      "learning_rate": 0.00016884711779448623,
      "loss": 0.9763,
      "step": 3730
    },
    {
      "epoch": 1.0935672514619883,
      "grad_norm": 1.026969075202942,
      "learning_rate": 0.00016876357560568087,
      "loss": 1.0785,
      "step": 3740
    },
    {
      "epoch": 1.0964912280701755,
      "grad_norm": 0.6716090440750122,
      "learning_rate": 0.00016868003341687554,
      "loss": 0.9732,
      "step": 3750
    },
    {
      "epoch": 1.0994152046783625,
      "grad_norm": 1.1563537120819092,
      "learning_rate": 0.00016859649122807018,
      "loss": 1.1445,
      "step": 3760
    },
    {
      "epoch": 1.1023391812865497,
      "grad_norm": 0.7918924689292908,
      "learning_rate": 0.00016851294903926485,
      "loss": 1.087,
      "step": 3770
    },
    {
      "epoch": 1.1052631578947367,
      "grad_norm": 1.1140908002853394,
      "learning_rate": 0.0001684294068504595,
      "loss": 1.1093,
      "step": 3780
    },
    {
      "epoch": 1.108187134502924,
      "grad_norm": 0.987931489944458,
      "learning_rate": 0.00016834586466165413,
      "loss": 1.0663,
      "step": 3790
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.8935747742652893,
      "learning_rate": 0.0001682623224728488,
      "loss": 0.9806,
      "step": 3800
    },
    {
      "epoch": 1.1140350877192982,
      "grad_norm": 1.224637508392334,
      "learning_rate": 0.00016817878028404346,
      "loss": 0.9492,
      "step": 3810
    },
    {
      "epoch": 1.1169590643274854,
      "grad_norm": 0.7862735390663147,
      "learning_rate": 0.0001680952380952381,
      "loss": 1.0232,
      "step": 3820
    },
    {
      "epoch": 1.1198830409356726,
      "grad_norm": 0.645286500453949,
      "learning_rate": 0.00016801169590643274,
      "loss": 1.0067,
      "step": 3830
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 1.0292876958847046,
      "learning_rate": 0.0001679281537176274,
      "loss": 1.0454,
      "step": 3840
    },
    {
      "epoch": 1.1257309941520468,
      "grad_norm": 0.8127819299697876,
      "learning_rate": 0.00016784461152882208,
      "loss": 0.9688,
      "step": 3850
    },
    {
      "epoch": 1.128654970760234,
      "grad_norm": 0.6650227308273315,
      "learning_rate": 0.00016776106934001672,
      "loss": 0.9729,
      "step": 3860
    },
    {
      "epoch": 1.131578947368421,
      "grad_norm": 0.8690394163131714,
      "learning_rate": 0.00016767752715121136,
      "loss": 1.0875,
      "step": 3870
    },
    {
      "epoch": 1.1345029239766082,
      "grad_norm": 0.8092660903930664,
      "learning_rate": 0.00016759398496240602,
      "loss": 1.0288,
      "step": 3880
    },
    {
      "epoch": 1.1374269005847952,
      "grad_norm": 0.7140331864356995,
      "learning_rate": 0.0001675104427736007,
      "loss": 0.9788,
      "step": 3890
    },
    {
      "epoch": 1.1403508771929824,
      "grad_norm": 0.7455633282661438,
      "learning_rate": 0.00016742690058479533,
      "loss": 1.0067,
      "step": 3900
    },
    {
      "epoch": 1.1432748538011697,
      "grad_norm": 0.7532961964607239,
      "learning_rate": 0.00016734335839598997,
      "loss": 1.0257,
      "step": 3910
    },
    {
      "epoch": 1.1461988304093567,
      "grad_norm": 0.7643506526947021,
      "learning_rate": 0.00016725981620718464,
      "loss": 1.0684,
      "step": 3920
    },
    {
      "epoch": 1.1491228070175439,
      "grad_norm": 1.132635474205017,
      "learning_rate": 0.0001671762740183793,
      "loss": 1.0769,
      "step": 3930
    },
    {
      "epoch": 1.1520467836257309,
      "grad_norm": 0.8923588395118713,
      "learning_rate": 0.00016709273182957394,
      "loss": 1.0101,
      "step": 3940
    },
    {
      "epoch": 1.154970760233918,
      "grad_norm": 0.8001316785812378,
      "learning_rate": 0.00016700918964076858,
      "loss": 0.9733,
      "step": 3950
    },
    {
      "epoch": 1.1578947368421053,
      "grad_norm": 1.1134610176086426,
      "learning_rate": 0.00016692564745196325,
      "loss": 1.1447,
      "step": 3960
    },
    {
      "epoch": 1.1608187134502923,
      "grad_norm": 0.9475582242012024,
      "learning_rate": 0.00016684210526315792,
      "loss": 1.1122,
      "step": 3970
    },
    {
      "epoch": 1.1637426900584795,
      "grad_norm": 0.8198599219322205,
      "learning_rate": 0.00016675856307435256,
      "loss": 0.9719,
      "step": 3980
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 1.0011094808578491,
      "learning_rate": 0.0001666750208855472,
      "loss": 1.0196,
      "step": 3990
    },
    {
      "epoch": 1.1695906432748537,
      "grad_norm": 0.936615526676178,
      "learning_rate": 0.00016659147869674186,
      "loss": 0.9807,
      "step": 4000
    },
    {
      "epoch": 1.172514619883041,
      "grad_norm": 0.9799204468727112,
      "learning_rate": 0.00016650793650793653,
      "loss": 0.9761,
      "step": 4010
    },
    {
      "epoch": 1.1754385964912282,
      "grad_norm": 1.0015153884887695,
      "learning_rate": 0.00016642439431913117,
      "loss": 0.994,
      "step": 4020
    },
    {
      "epoch": 1.1783625730994152,
      "grad_norm": 0.809845507144928,
      "learning_rate": 0.0001663408521303258,
      "loss": 1.0684,
      "step": 4030
    },
    {
      "epoch": 1.1812865497076024,
      "grad_norm": 0.9137018322944641,
      "learning_rate": 0.00016625730994152048,
      "loss": 0.988,
      "step": 4040
    },
    {
      "epoch": 1.1842105263157894,
      "grad_norm": 0.8403558135032654,
      "learning_rate": 0.00016617376775271515,
      "loss": 1.1422,
      "step": 4050
    },
    {
      "epoch": 1.1871345029239766,
      "grad_norm": 0.7576554417610168,
      "learning_rate": 0.00016609022556390979,
      "loss": 1.0225,
      "step": 4060
    },
    {
      "epoch": 1.1900584795321638,
      "grad_norm": 0.7629342079162598,
      "learning_rate": 0.00016600668337510443,
      "loss": 1.05,
      "step": 4070
    },
    {
      "epoch": 1.1929824561403508,
      "grad_norm": 1.0054742097854614,
      "learning_rate": 0.0001659231411862991,
      "loss": 1.0413,
      "step": 4080
    },
    {
      "epoch": 1.195906432748538,
      "grad_norm": 0.9305086135864258,
      "learning_rate": 0.00016583959899749376,
      "loss": 1.0817,
      "step": 4090
    },
    {
      "epoch": 1.198830409356725,
      "grad_norm": 1.1591107845306396,
      "learning_rate": 0.0001657560568086884,
      "loss": 0.9085,
      "step": 4100
    },
    {
      "epoch": 1.2017543859649122,
      "grad_norm": 0.7387591004371643,
      "learning_rate": 0.00016567251461988304,
      "loss": 1.0923,
      "step": 4110
    },
    {
      "epoch": 1.2046783625730995,
      "grad_norm": 0.8540749549865723,
      "learning_rate": 0.0001655889724310777,
      "loss": 1.001,
      "step": 4120
    },
    {
      "epoch": 1.2076023391812865,
      "grad_norm": 0.9599475860595703,
      "learning_rate": 0.00016550543024227237,
      "loss": 0.9824,
      "step": 4130
    },
    {
      "epoch": 1.2105263157894737,
      "grad_norm": 0.7544087767601013,
      "learning_rate": 0.000165421888053467,
      "loss": 0.9835,
      "step": 4140
    },
    {
      "epoch": 1.213450292397661,
      "grad_norm": 1.1689337491989136,
      "learning_rate": 0.00016533834586466165,
      "loss": 1.0558,
      "step": 4150
    },
    {
      "epoch": 1.2163742690058479,
      "grad_norm": 0.6956526041030884,
      "learning_rate": 0.00016525480367585632,
      "loss": 0.9986,
      "step": 4160
    },
    {
      "epoch": 1.219298245614035,
      "grad_norm": 0.8202791213989258,
      "learning_rate": 0.000165171261487051,
      "loss": 0.9687,
      "step": 4170
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.9986303448677063,
      "learning_rate": 0.00016508771929824563,
      "loss": 0.9202,
      "step": 4180
    },
    {
      "epoch": 1.2251461988304093,
      "grad_norm": 0.781658947467804,
      "learning_rate": 0.00016500417710944027,
      "loss": 1.0216,
      "step": 4190
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 0.9545192718505859,
      "learning_rate": 0.0001649206349206349,
      "loss": 1.0654,
      "step": 4200
    },
    {
      "epoch": 1.2309941520467835,
      "grad_norm": 0.8937556147575378,
      "learning_rate": 0.0001648370927318296,
      "loss": 1.025,
      "step": 4210
    },
    {
      "epoch": 1.2339181286549707,
      "grad_norm": 0.8763492703437805,
      "learning_rate": 0.00016475355054302424,
      "loss": 1.1123,
      "step": 4220
    },
    {
      "epoch": 1.236842105263158,
      "grad_norm": 0.9123777747154236,
      "learning_rate": 0.00016467000835421888,
      "loss": 1.0346,
      "step": 4230
    },
    {
      "epoch": 1.239766081871345,
      "grad_norm": 0.8213644623756409,
      "learning_rate": 0.00016458646616541352,
      "loss": 1.0092,
      "step": 4240
    },
    {
      "epoch": 1.2426900584795322,
      "grad_norm": 0.8204146027565002,
      "learning_rate": 0.00016450292397660822,
      "loss": 1.0984,
      "step": 4250
    },
    {
      "epoch": 1.2456140350877192,
      "grad_norm": 0.9359686374664307,
      "learning_rate": 0.00016441938178780286,
      "loss": 1.0095,
      "step": 4260
    },
    {
      "epoch": 1.2485380116959064,
      "grad_norm": 0.8398817777633667,
      "learning_rate": 0.0001643358395989975,
      "loss": 1.0546,
      "step": 4270
    },
    {
      "epoch": 1.2514619883040936,
      "grad_norm": 0.8538243770599365,
      "learning_rate": 0.00016425229741019214,
      "loss": 0.9976,
      "step": 4280
    },
    {
      "epoch": 1.2543859649122808,
      "grad_norm": 0.8758577108383179,
      "learning_rate": 0.00016416875522138683,
      "loss": 1.0266,
      "step": 4290
    },
    {
      "epoch": 1.2573099415204678,
      "grad_norm": 0.7522488236427307,
      "learning_rate": 0.00016408521303258147,
      "loss": 0.9178,
      "step": 4300
    },
    {
      "epoch": 1.260233918128655,
      "grad_norm": 1.084803581237793,
      "learning_rate": 0.0001640016708437761,
      "loss": 1.0828,
      "step": 4310
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 0.7960859537124634,
      "learning_rate": 0.00016391812865497075,
      "loss": 1.0902,
      "step": 4320
    },
    {
      "epoch": 1.2660818713450293,
      "grad_norm": 0.7163617014884949,
      "learning_rate": 0.00016383458646616542,
      "loss": 1.0701,
      "step": 4330
    },
    {
      "epoch": 1.2690058479532165,
      "grad_norm": 0.9322823882102966,
      "learning_rate": 0.00016375104427736008,
      "loss": 1.0281,
      "step": 4340
    },
    {
      "epoch": 1.2719298245614035,
      "grad_norm": 0.6701377034187317,
      "learning_rate": 0.00016366750208855472,
      "loss": 1.1185,
      "step": 4350
    },
    {
      "epoch": 1.2748538011695907,
      "grad_norm": 0.9274327754974365,
      "learning_rate": 0.00016358395989974936,
      "loss": 0.966,
      "step": 4360
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.8017268776893616,
      "learning_rate": 0.00016350041771094403,
      "loss": 1.0763,
      "step": 4370
    },
    {
      "epoch": 1.280701754385965,
      "grad_norm": 0.8926110863685608,
      "learning_rate": 0.0001634168755221387,
      "loss": 1.0695,
      "step": 4380
    },
    {
      "epoch": 1.2836257309941521,
      "grad_norm": 0.6030037999153137,
      "learning_rate": 0.00016333333333333334,
      "loss": 1.1059,
      "step": 4390
    },
    {
      "epoch": 1.286549707602339,
      "grad_norm": 0.7277844548225403,
      "learning_rate": 0.00016324979114452798,
      "loss": 1.034,
      "step": 4400
    },
    {
      "epoch": 1.2894736842105263,
      "grad_norm": 0.7171879410743713,
      "learning_rate": 0.00016316624895572264,
      "loss": 1.0406,
      "step": 4410
    },
    {
      "epoch": 1.2923976608187133,
      "grad_norm": 0.7786285281181335,
      "learning_rate": 0.0001630827067669173,
      "loss": 1.0719,
      "step": 4420
    },
    {
      "epoch": 1.2953216374269005,
      "grad_norm": 0.782067596912384,
      "learning_rate": 0.00016299916457811195,
      "loss": 0.9019,
      "step": 4430
    },
    {
      "epoch": 1.2982456140350878,
      "grad_norm": 0.7210870981216431,
      "learning_rate": 0.0001629156223893066,
      "loss": 1.0516,
      "step": 4440
    },
    {
      "epoch": 1.301169590643275,
      "grad_norm": 0.6811277866363525,
      "learning_rate": 0.00016283208020050126,
      "loss": 1.0463,
      "step": 4450
    },
    {
      "epoch": 1.304093567251462,
      "grad_norm": 0.8044763207435608,
      "learning_rate": 0.00016274853801169593,
      "loss": 1.0332,
      "step": 4460
    },
    {
      "epoch": 1.3070175438596492,
      "grad_norm": 0.7436519265174866,
      "learning_rate": 0.00016266499582289057,
      "loss": 1.0093,
      "step": 4470
    },
    {
      "epoch": 1.3099415204678362,
      "grad_norm": 0.8314284086227417,
      "learning_rate": 0.0001625814536340852,
      "loss": 0.9906,
      "step": 4480
    },
    {
      "epoch": 1.3128654970760234,
      "grad_norm": 0.8071933388710022,
      "learning_rate": 0.00016249791144527987,
      "loss": 1.0303,
      "step": 4490
    },
    {
      "epoch": 1.3157894736842106,
      "grad_norm": 0.8348822593688965,
      "learning_rate": 0.00016241436925647454,
      "loss": 0.9576,
      "step": 4500
    },
    {
      "epoch": 1.3187134502923976,
      "grad_norm": 0.9237983226776123,
      "learning_rate": 0.00016233082706766918,
      "loss": 1.1926,
      "step": 4510
    },
    {
      "epoch": 1.3216374269005848,
      "grad_norm": 1.1766146421432495,
      "learning_rate": 0.00016224728487886382,
      "loss": 1.1239,
      "step": 4520
    },
    {
      "epoch": 1.3245614035087718,
      "grad_norm": 0.9530872702598572,
      "learning_rate": 0.00016216374269005849,
      "loss": 1.0176,
      "step": 4530
    },
    {
      "epoch": 1.327485380116959,
      "grad_norm": 0.9726871252059937,
      "learning_rate": 0.00016208020050125315,
      "loss": 1.0184,
      "step": 4540
    },
    {
      "epoch": 1.3304093567251463,
      "grad_norm": 0.6211546659469604,
      "learning_rate": 0.0001619966583124478,
      "loss": 0.9901,
      "step": 4550
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.7024686932563782,
      "learning_rate": 0.00016191311612364243,
      "loss": 1.0258,
      "step": 4560
    },
    {
      "epoch": 1.3362573099415205,
      "grad_norm": 1.1740942001342773,
      "learning_rate": 0.0001618295739348371,
      "loss": 1.0256,
      "step": 4570
    },
    {
      "epoch": 1.3391812865497075,
      "grad_norm": 0.7817105054855347,
      "learning_rate": 0.00016174603174603177,
      "loss": 1.1152,
      "step": 4580
    },
    {
      "epoch": 1.3421052631578947,
      "grad_norm": 0.9076012372970581,
      "learning_rate": 0.0001616624895572264,
      "loss": 1.0569,
      "step": 4590
    },
    {
      "epoch": 1.345029239766082,
      "grad_norm": 1.132493019104004,
      "learning_rate": 0.00016157894736842105,
      "loss": 1.0939,
      "step": 4600
    },
    {
      "epoch": 1.3479532163742691,
      "grad_norm": 0.7534472942352295,
      "learning_rate": 0.00016149540517961571,
      "loss": 1.078,
      "step": 4610
    },
    {
      "epoch": 1.3508771929824561,
      "grad_norm": 0.9408011436462402,
      "learning_rate": 0.00016141186299081038,
      "loss": 1.0382,
      "step": 4620
    },
    {
      "epoch": 1.3538011695906433,
      "grad_norm": 0.7369285225868225,
      "learning_rate": 0.00016132832080200502,
      "loss": 0.9758,
      "step": 4630
    },
    {
      "epoch": 1.3567251461988303,
      "grad_norm": 1.0360158681869507,
      "learning_rate": 0.00016124477861319966,
      "loss": 1.028,
      "step": 4640
    },
    {
      "epoch": 1.3596491228070176,
      "grad_norm": 0.7844170331954956,
      "learning_rate": 0.00016116123642439433,
      "loss": 1.0236,
      "step": 4650
    },
    {
      "epoch": 1.3625730994152048,
      "grad_norm": 0.725222647190094,
      "learning_rate": 0.000161077694235589,
      "loss": 1.0465,
      "step": 4660
    },
    {
      "epoch": 1.3654970760233918,
      "grad_norm": 0.8647139668464661,
      "learning_rate": 0.00016099415204678364,
      "loss": 1.0515,
      "step": 4670
    },
    {
      "epoch": 1.368421052631579,
      "grad_norm": 0.8723974227905273,
      "learning_rate": 0.00016091060985797828,
      "loss": 0.979,
      "step": 4680
    },
    {
      "epoch": 1.371345029239766,
      "grad_norm": 0.6132211089134216,
      "learning_rate": 0.00016082706766917294,
      "loss": 0.9928,
      "step": 4690
    },
    {
      "epoch": 1.3742690058479532,
      "grad_norm": 1.5431196689605713,
      "learning_rate": 0.0001607435254803676,
      "loss": 1.2415,
      "step": 4700
    },
    {
      "epoch": 1.3771929824561404,
      "grad_norm": 0.96607905626297,
      "learning_rate": 0.00016065998329156225,
      "loss": 1.0897,
      "step": 4710
    },
    {
      "epoch": 1.3801169590643274,
      "grad_norm": 1.0056003332138062,
      "learning_rate": 0.0001605764411027569,
      "loss": 1.0364,
      "step": 4720
    },
    {
      "epoch": 1.3830409356725146,
      "grad_norm": 0.8771641850471497,
      "learning_rate": 0.00016049289891395156,
      "loss": 1.0908,
      "step": 4730
    },
    {
      "epoch": 1.3859649122807016,
      "grad_norm": 0.7944931387901306,
      "learning_rate": 0.0001604093567251462,
      "loss": 0.9437,
      "step": 4740
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.8234620094299316,
      "learning_rate": 0.00016032581453634086,
      "loss": 1.017,
      "step": 4750
    },
    {
      "epoch": 1.391812865497076,
      "grad_norm": 0.9216353893280029,
      "learning_rate": 0.0001602422723475355,
      "loss": 0.9758,
      "step": 4760
    },
    {
      "epoch": 1.3947368421052633,
      "grad_norm": 0.7656610608100891,
      "learning_rate": 0.00016015873015873017,
      "loss": 1.1028,
      "step": 4770
    },
    {
      "epoch": 1.3976608187134503,
      "grad_norm": 1.0839756727218628,
      "learning_rate": 0.0001600751879699248,
      "loss": 1.0381,
      "step": 4780
    },
    {
      "epoch": 1.4005847953216375,
      "grad_norm": 1.1472158432006836,
      "learning_rate": 0.00015999164578111948,
      "loss": 1.027,
      "step": 4790
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 0.8252745270729065,
      "learning_rate": 0.00015990810359231412,
      "loss": 1.0692,
      "step": 4800
    },
    {
      "epoch": 1.4064327485380117,
      "grad_norm": 1.1108460426330566,
      "learning_rate": 0.00015982456140350878,
      "loss": 1.0558,
      "step": 4810
    },
    {
      "epoch": 1.409356725146199,
      "grad_norm": 0.8115297555923462,
      "learning_rate": 0.00015974101921470342,
      "loss": 1.0085,
      "step": 4820
    },
    {
      "epoch": 1.412280701754386,
      "grad_norm": 0.8936018943786621,
      "learning_rate": 0.0001596574770258981,
      "loss": 1.098,
      "step": 4830
    },
    {
      "epoch": 1.4152046783625731,
      "grad_norm": 0.8985607624053955,
      "learning_rate": 0.00015957393483709273,
      "loss": 1.1056,
      "step": 4840
    },
    {
      "epoch": 1.4181286549707601,
      "grad_norm": 0.9193543195724487,
      "learning_rate": 0.0001594903926482874,
      "loss": 0.9714,
      "step": 4850
    },
    {
      "epoch": 1.4210526315789473,
      "grad_norm": 0.7480224967002869,
      "learning_rate": 0.00015940685045948204,
      "loss": 1.0566,
      "step": 4860
    },
    {
      "epoch": 1.4239766081871346,
      "grad_norm": 0.9418436288833618,
      "learning_rate": 0.0001593233082706767,
      "loss": 1.039,
      "step": 4870
    },
    {
      "epoch": 1.4269005847953216,
      "grad_norm": 0.9795024991035461,
      "learning_rate": 0.00015923976608187135,
      "loss": 0.9661,
      "step": 4880
    },
    {
      "epoch": 1.4298245614035088,
      "grad_norm": 0.9254862666130066,
      "learning_rate": 0.000159156223893066,
      "loss": 1.0014,
      "step": 4890
    },
    {
      "epoch": 1.4327485380116958,
      "grad_norm": 0.8964710235595703,
      "learning_rate": 0.00015907268170426065,
      "loss": 1.0694,
      "step": 4900
    },
    {
      "epoch": 1.435672514619883,
      "grad_norm": 0.8462629318237305,
      "learning_rate": 0.00015898913951545532,
      "loss": 0.9597,
      "step": 4910
    },
    {
      "epoch": 1.4385964912280702,
      "grad_norm": 0.9041296243667603,
      "learning_rate": 0.00015890559732664996,
      "loss": 1.0639,
      "step": 4920
    },
    {
      "epoch": 1.4415204678362574,
      "grad_norm": 0.8468465209007263,
      "learning_rate": 0.00015882205513784463,
      "loss": 1.0206,
      "step": 4930
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.8309330940246582,
      "learning_rate": 0.00015873851294903927,
      "loss": 0.9884,
      "step": 4940
    },
    {
      "epoch": 1.4473684210526316,
      "grad_norm": 0.914452314376831,
      "learning_rate": 0.00015865497076023393,
      "loss": 1.0065,
      "step": 4950
    },
    {
      "epoch": 1.4502923976608186,
      "grad_norm": 0.9611675143241882,
      "learning_rate": 0.00015857142857142857,
      "loss": 1.0514,
      "step": 4960
    },
    {
      "epoch": 1.4532163742690059,
      "grad_norm": 0.7582452297210693,
      "learning_rate": 0.00015848788638262324,
      "loss": 0.9821,
      "step": 4970
    },
    {
      "epoch": 1.456140350877193,
      "grad_norm": 0.825092077255249,
      "learning_rate": 0.00015840434419381788,
      "loss": 1.0034,
      "step": 4980
    },
    {
      "epoch": 1.45906432748538,
      "grad_norm": 0.9482030868530273,
      "learning_rate": 0.00015832080200501255,
      "loss": 1.0985,
      "step": 4990
    },
    {
      "epoch": 1.4619883040935673,
      "grad_norm": 0.8396826386451721,
      "learning_rate": 0.0001582372598162072,
      "loss": 1.1592,
      "step": 5000
    },
    {
      "epoch": 1.4649122807017543,
      "grad_norm": 0.7854163646697998,
      "learning_rate": 0.00015815371762740185,
      "loss": 0.9595,
      "step": 5010
    },
    {
      "epoch": 1.4678362573099415,
      "grad_norm": 1.0267820358276367,
      "learning_rate": 0.0001580701754385965,
      "loss": 0.9915,
      "step": 5020
    },
    {
      "epoch": 1.4707602339181287,
      "grad_norm": 0.8415314555168152,
      "learning_rate": 0.00015798663324979116,
      "loss": 1.0938,
      "step": 5030
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 0.7687568068504333,
      "learning_rate": 0.0001579030910609858,
      "loss": 0.9513,
      "step": 5040
    },
    {
      "epoch": 1.476608187134503,
      "grad_norm": 0.9171237945556641,
      "learning_rate": 0.00015781954887218047,
      "loss": 0.9938,
      "step": 5050
    },
    {
      "epoch": 1.47953216374269,
      "grad_norm": 0.8515141010284424,
      "learning_rate": 0.0001577360066833751,
      "loss": 0.9856,
      "step": 5060
    },
    {
      "epoch": 1.4824561403508771,
      "grad_norm": 0.733814537525177,
      "learning_rate": 0.00015765246449456978,
      "loss": 1.0369,
      "step": 5070
    },
    {
      "epoch": 1.4853801169590644,
      "grad_norm": 0.8221617341041565,
      "learning_rate": 0.00015756892230576442,
      "loss": 1.0203,
      "step": 5080
    },
    {
      "epoch": 1.4883040935672516,
      "grad_norm": 1.0162334442138672,
      "learning_rate": 0.00015748538011695908,
      "loss": 1.052,
      "step": 5090
    },
    {
      "epoch": 1.4912280701754386,
      "grad_norm": 0.754698634147644,
      "learning_rate": 0.00015740183792815372,
      "loss": 0.9795,
      "step": 5100
    },
    {
      "epoch": 1.4941520467836258,
      "grad_norm": 0.6596644520759583,
      "learning_rate": 0.0001573182957393484,
      "loss": 0.932,
      "step": 5110
    },
    {
      "epoch": 1.4970760233918128,
      "grad_norm": 0.7300651669502258,
      "learning_rate": 0.00015723475355054303,
      "loss": 1.0367,
      "step": 5120
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.8900358080863953,
      "learning_rate": 0.0001571512113617377,
      "loss": 0.9309,
      "step": 5130
    },
    {
      "epoch": 1.5029239766081872,
      "grad_norm": 0.6626250743865967,
      "learning_rate": 0.00015706766917293234,
      "loss": 1.0896,
      "step": 5140
    },
    {
      "epoch": 1.5058479532163744,
      "grad_norm": 0.9426572322845459,
      "learning_rate": 0.00015698412698412698,
      "loss": 1.0533,
      "step": 5150
    },
    {
      "epoch": 1.5087719298245614,
      "grad_norm": 0.7673139572143555,
      "learning_rate": 0.00015690058479532164,
      "loss": 0.9798,
      "step": 5160
    },
    {
      "epoch": 1.5116959064327484,
      "grad_norm": 0.8894666433334351,
      "learning_rate": 0.0001568170426065163,
      "loss": 1.0925,
      "step": 5170
    },
    {
      "epoch": 1.5146198830409356,
      "grad_norm": 1.0996743440628052,
      "learning_rate": 0.00015673350041771095,
      "loss": 1.043,
      "step": 5180
    },
    {
      "epoch": 1.5175438596491229,
      "grad_norm": 0.6371501088142395,
      "learning_rate": 0.0001566499582289056,
      "loss": 1.0072,
      "step": 5190
    },
    {
      "epoch": 1.52046783625731,
      "grad_norm": 0.7489977478981018,
      "learning_rate": 0.00015656641604010026,
      "loss": 1.0301,
      "step": 5200
    },
    {
      "epoch": 1.523391812865497,
      "grad_norm": 0.7430573105812073,
      "learning_rate": 0.00015648287385129492,
      "loss": 1.0602,
      "step": 5210
    },
    {
      "epoch": 1.526315789473684,
      "grad_norm": 0.7494414448738098,
      "learning_rate": 0.00015639933166248956,
      "loss": 0.9619,
      "step": 5220
    },
    {
      "epoch": 1.5292397660818713,
      "grad_norm": 0.8324454426765442,
      "learning_rate": 0.0001563157894736842,
      "loss": 0.9753,
      "step": 5230
    },
    {
      "epoch": 1.5321637426900585,
      "grad_norm": 0.799157977104187,
      "learning_rate": 0.00015623224728487887,
      "loss": 1.0269,
      "step": 5240
    },
    {
      "epoch": 1.5350877192982457,
      "grad_norm": 0.9322001338005066,
      "learning_rate": 0.00015614870509607354,
      "loss": 1.0468,
      "step": 5250
    },
    {
      "epoch": 1.5380116959064327,
      "grad_norm": 0.8324552178382874,
      "learning_rate": 0.00015606516290726818,
      "loss": 1.1558,
      "step": 5260
    },
    {
      "epoch": 1.54093567251462,
      "grad_norm": 0.8316504955291748,
      "learning_rate": 0.00015598162071846282,
      "loss": 1.015,
      "step": 5270
    },
    {
      "epoch": 1.543859649122807,
      "grad_norm": 0.9380940198898315,
      "learning_rate": 0.00015589807852965749,
      "loss": 1.1106,
      "step": 5280
    },
    {
      "epoch": 1.5467836257309941,
      "grad_norm": 0.8459790945053101,
      "learning_rate": 0.00015581453634085215,
      "loss": 1.1088,
      "step": 5290
    },
    {
      "epoch": 1.5497076023391814,
      "grad_norm": 0.8001521229743958,
      "learning_rate": 0.0001557309941520468,
      "loss": 0.9776,
      "step": 5300
    },
    {
      "epoch": 1.5526315789473686,
      "grad_norm": 0.8360921144485474,
      "learning_rate": 0.00015564745196324143,
      "loss": 1.0328,
      "step": 5310
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.7472350597381592,
      "learning_rate": 0.0001555639097744361,
      "loss": 1.0499,
      "step": 5320
    },
    {
      "epoch": 1.5584795321637426,
      "grad_norm": 0.9403045177459717,
      "learning_rate": 0.00015548036758563077,
      "loss": 1.1292,
      "step": 5330
    },
    {
      "epoch": 1.5614035087719298,
      "grad_norm": 0.7778987288475037,
      "learning_rate": 0.0001553968253968254,
      "loss": 1.0377,
      "step": 5340
    },
    {
      "epoch": 1.564327485380117,
      "grad_norm": 0.729231059551239,
      "learning_rate": 0.00015531328320802005,
      "loss": 0.9548,
      "step": 5350
    },
    {
      "epoch": 1.5672514619883042,
      "grad_norm": 0.8175607323646545,
      "learning_rate": 0.0001552297410192147,
      "loss": 0.99,
      "step": 5360
    },
    {
      "epoch": 1.5701754385964912,
      "grad_norm": 0.7198352217674255,
      "learning_rate": 0.00015514619883040938,
      "loss": 1.0451,
      "step": 5370
    },
    {
      "epoch": 1.5730994152046782,
      "grad_norm": 0.8740495443344116,
      "learning_rate": 0.00015506265664160402,
      "loss": 0.9816,
      "step": 5380
    },
    {
      "epoch": 1.5760233918128654,
      "grad_norm": 1.0099636316299438,
      "learning_rate": 0.00015497911445279866,
      "loss": 1.0107,
      "step": 5390
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 0.8598712682723999,
      "learning_rate": 0.00015489557226399333,
      "loss": 1.0072,
      "step": 5400
    },
    {
      "epoch": 1.5818713450292399,
      "grad_norm": 0.6054239869117737,
      "learning_rate": 0.000154812030075188,
      "loss": 0.9503,
      "step": 5410
    },
    {
      "epoch": 1.5847953216374269,
      "grad_norm": 1.0178437232971191,
      "learning_rate": 0.00015472848788638263,
      "loss": 1.0548,
      "step": 5420
    },
    {
      "epoch": 1.587719298245614,
      "grad_norm": 0.6921367049217224,
      "learning_rate": 0.00015464494569757727,
      "loss": 1.0149,
      "step": 5430
    },
    {
      "epoch": 1.590643274853801,
      "grad_norm": 0.6651777029037476,
      "learning_rate": 0.00015456140350877194,
      "loss": 0.9919,
      "step": 5440
    },
    {
      "epoch": 1.5935672514619883,
      "grad_norm": 0.75955730676651,
      "learning_rate": 0.0001544778613199666,
      "loss": 0.9914,
      "step": 5450
    },
    {
      "epoch": 1.5964912280701755,
      "grad_norm": 0.7991601228713989,
      "learning_rate": 0.00015439431913116125,
      "loss": 1.0092,
      "step": 5460
    },
    {
      "epoch": 1.5994152046783627,
      "grad_norm": 0.7267422676086426,
      "learning_rate": 0.0001543107769423559,
      "loss": 1.0039,
      "step": 5470
    },
    {
      "epoch": 1.6023391812865497,
      "grad_norm": 0.8206014633178711,
      "learning_rate": 0.00015422723475355056,
      "loss": 1.0658,
      "step": 5480
    },
    {
      "epoch": 1.6052631578947367,
      "grad_norm": 0.7398808598518372,
      "learning_rate": 0.00015414369256474522,
      "loss": 1.0034,
      "step": 5490
    },
    {
      "epoch": 1.608187134502924,
      "grad_norm": 0.8455966114997864,
      "learning_rate": 0.00015406015037593986,
      "loss": 1.1273,
      "step": 5500
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.7957735657691956,
      "learning_rate": 0.0001539766081871345,
      "loss": 0.9522,
      "step": 5510
    },
    {
      "epoch": 1.6140350877192984,
      "grad_norm": 0.844782292842865,
      "learning_rate": 0.00015389306599832917,
      "loss": 0.9768,
      "step": 5520
    },
    {
      "epoch": 1.6169590643274854,
      "grad_norm": 0.6712698936462402,
      "learning_rate": 0.00015380952380952384,
      "loss": 0.9144,
      "step": 5530
    },
    {
      "epoch": 1.6198830409356724,
      "grad_norm": 0.8403223752975464,
      "learning_rate": 0.00015372598162071848,
      "loss": 1.0815,
      "step": 5540
    },
    {
      "epoch": 1.6228070175438596,
      "grad_norm": 0.9703261852264404,
      "learning_rate": 0.00015364243943191312,
      "loss": 1.067,
      "step": 5550
    },
    {
      "epoch": 1.6257309941520468,
      "grad_norm": 0.7757658362388611,
      "learning_rate": 0.00015355889724310776,
      "loss": 1.1053,
      "step": 5560
    },
    {
      "epoch": 1.628654970760234,
      "grad_norm": 0.6908823251724243,
      "learning_rate": 0.00015347535505430245,
      "loss": 1.0194,
      "step": 5570
    },
    {
      "epoch": 1.631578947368421,
      "grad_norm": 0.8235735893249512,
      "learning_rate": 0.0001533918128654971,
      "loss": 1.0398,
      "step": 5580
    },
    {
      "epoch": 1.6345029239766082,
      "grad_norm": 0.7691328525543213,
      "learning_rate": 0.00015330827067669173,
      "loss": 1.058,
      "step": 5590
    },
    {
      "epoch": 1.6374269005847952,
      "grad_norm": 0.9614942669868469,
      "learning_rate": 0.00015322472848788637,
      "loss": 0.9838,
      "step": 5600
    },
    {
      "epoch": 1.6403508771929824,
      "grad_norm": 0.7772682905197144,
      "learning_rate": 0.00015314118629908106,
      "loss": 1.1407,
      "step": 5610
    },
    {
      "epoch": 1.6432748538011697,
      "grad_norm": 0.7986704707145691,
      "learning_rate": 0.0001530576441102757,
      "loss": 1.1254,
      "step": 5620
    },
    {
      "epoch": 1.6461988304093569,
      "grad_norm": 0.7656201124191284,
      "learning_rate": 0.00015297410192147034,
      "loss": 1.0141,
      "step": 5630
    },
    {
      "epoch": 1.6491228070175439,
      "grad_norm": 0.9716926217079163,
      "learning_rate": 0.00015289055973266498,
      "loss": 1.024,
      "step": 5640
    },
    {
      "epoch": 1.6520467836257309,
      "grad_norm": 0.6477646231651306,
      "learning_rate": 0.00015280701754385968,
      "loss": 0.9958,
      "step": 5650
    },
    {
      "epoch": 1.654970760233918,
      "grad_norm": 1.0835981369018555,
      "learning_rate": 0.00015272347535505432,
      "loss": 1.1057,
      "step": 5660
    },
    {
      "epoch": 1.6578947368421053,
      "grad_norm": 0.8405338525772095,
      "learning_rate": 0.00015263993316624896,
      "loss": 0.9984,
      "step": 5670
    },
    {
      "epoch": 1.6608187134502925,
      "grad_norm": 0.7961955070495605,
      "learning_rate": 0.0001525563909774436,
      "loss": 0.9985,
      "step": 5680
    },
    {
      "epoch": 1.6637426900584795,
      "grad_norm": 0.7855185270309448,
      "learning_rate": 0.00015247284878863826,
      "loss": 0.9546,
      "step": 5690
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.8388020992279053,
      "learning_rate": 0.00015238930659983293,
      "loss": 1.0321,
      "step": 5700
    },
    {
      "epoch": 1.6695906432748537,
      "grad_norm": 0.8275948166847229,
      "learning_rate": 0.00015230576441102757,
      "loss": 1.1006,
      "step": 5710
    },
    {
      "epoch": 1.672514619883041,
      "grad_norm": 0.7325748801231384,
      "learning_rate": 0.0001522222222222222,
      "loss": 1.0857,
      "step": 5720
    },
    {
      "epoch": 1.6754385964912282,
      "grad_norm": 0.7707244157791138,
      "learning_rate": 0.00015213868003341688,
      "loss": 1.0196,
      "step": 5730
    },
    {
      "epoch": 1.6783625730994152,
      "grad_norm": 0.7670484185218811,
      "learning_rate": 0.00015205513784461155,
      "loss": 1.0302,
      "step": 5740
    },
    {
      "epoch": 1.6812865497076024,
      "grad_norm": 0.7749447822570801,
      "learning_rate": 0.00015197159565580619,
      "loss": 1.0712,
      "step": 5750
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 0.7668793797492981,
      "learning_rate": 0.00015188805346700083,
      "loss": 0.9519,
      "step": 5760
    },
    {
      "epoch": 1.6871345029239766,
      "grad_norm": 0.9915862083435059,
      "learning_rate": 0.0001518045112781955,
      "loss": 1.0111,
      "step": 5770
    },
    {
      "epoch": 1.6900584795321638,
      "grad_norm": 0.679711639881134,
      "learning_rate": 0.00015172096908939016,
      "loss": 1.0585,
      "step": 5780
    },
    {
      "epoch": 1.692982456140351,
      "grad_norm": 0.7911689281463623,
      "learning_rate": 0.0001516374269005848,
      "loss": 1.0372,
      "step": 5790
    },
    {
      "epoch": 1.695906432748538,
      "grad_norm": 0.8448194861412048,
      "learning_rate": 0.00015155388471177944,
      "loss": 1.1172,
      "step": 5800
    },
    {
      "epoch": 1.698830409356725,
      "grad_norm": 0.9218112230300903,
      "learning_rate": 0.0001514703425229741,
      "loss": 0.9675,
      "step": 5810
    },
    {
      "epoch": 1.7017543859649122,
      "grad_norm": 0.7243985533714294,
      "learning_rate": 0.00015138680033416877,
      "loss": 1.0017,
      "step": 5820
    },
    {
      "epoch": 1.7046783625730995,
      "grad_norm": 0.7459487915039062,
      "learning_rate": 0.00015130325814536341,
      "loss": 0.9394,
      "step": 5830
    },
    {
      "epoch": 1.7076023391812867,
      "grad_norm": 0.7097800970077515,
      "learning_rate": 0.00015121971595655805,
      "loss": 0.9283,
      "step": 5840
    },
    {
      "epoch": 1.7105263157894737,
      "grad_norm": 0.8938723206520081,
      "learning_rate": 0.00015113617376775272,
      "loss": 1.0147,
      "step": 5850
    },
    {
      "epoch": 1.7134502923976607,
      "grad_norm": 0.8506420254707336,
      "learning_rate": 0.0001510526315789474,
      "loss": 1.0891,
      "step": 5860
    },
    {
      "epoch": 1.7163742690058479,
      "grad_norm": 1.0338407754898071,
      "learning_rate": 0.00015096908939014203,
      "loss": 1.0704,
      "step": 5870
    },
    {
      "epoch": 1.719298245614035,
      "grad_norm": 0.8660449981689453,
      "learning_rate": 0.00015088554720133667,
      "loss": 0.9563,
      "step": 5880
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.7327232360839844,
      "learning_rate": 0.00015080200501253133,
      "loss": 1.0079,
      "step": 5890
    },
    {
      "epoch": 1.7251461988304093,
      "grad_norm": 0.8721578121185303,
      "learning_rate": 0.000150718462823726,
      "loss": 1.0456,
      "step": 5900
    },
    {
      "epoch": 1.7280701754385965,
      "grad_norm": 0.7649480700492859,
      "learning_rate": 0.00015063492063492064,
      "loss": 1.0816,
      "step": 5910
    },
    {
      "epoch": 1.7309941520467835,
      "grad_norm": 0.8139697909355164,
      "learning_rate": 0.00015055137844611528,
      "loss": 0.9442,
      "step": 5920
    },
    {
      "epoch": 1.7339181286549707,
      "grad_norm": 28.94810676574707,
      "learning_rate": 0.00015046783625730995,
      "loss": 1.054,
      "step": 5930
    },
    {
      "epoch": 1.736842105263158,
      "grad_norm": 0.9186434745788574,
      "learning_rate": 0.00015038429406850462,
      "loss": 1.221,
      "step": 5940
    },
    {
      "epoch": 1.7397660818713452,
      "grad_norm": 0.7793018221855164,
      "learning_rate": 0.00015030075187969926,
      "loss": 1.0078,
      "step": 5950
    },
    {
      "epoch": 1.7426900584795322,
      "grad_norm": 0.7274805903434753,
      "learning_rate": 0.0001502172096908939,
      "loss": 1.0057,
      "step": 5960
    },
    {
      "epoch": 1.7456140350877192,
      "grad_norm": 0.8955466747283936,
      "learning_rate": 0.00015013366750208856,
      "loss": 1.0067,
      "step": 5970
    },
    {
      "epoch": 1.7485380116959064,
      "grad_norm": 0.8871820569038391,
      "learning_rate": 0.00015005012531328323,
      "loss": 1.017,
      "step": 5980
    },
    {
      "epoch": 1.7514619883040936,
      "grad_norm": 0.9204553365707397,
      "learning_rate": 0.00014996658312447787,
      "loss": 0.9872,
      "step": 5990
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 0.8175688982009888,
      "learning_rate": 0.0001498830409356725,
      "loss": 1.0015,
      "step": 6000
    },
    {
      "epoch": 1.7573099415204678,
      "grad_norm": 0.7423389554023743,
      "learning_rate": 0.00014979949874686718,
      "loss": 1.0354,
      "step": 6010
    },
    {
      "epoch": 1.7602339181286548,
      "grad_norm": 0.8075295090675354,
      "learning_rate": 0.00014971595655806184,
      "loss": 0.9765,
      "step": 6020
    },
    {
      "epoch": 1.763157894736842,
      "grad_norm": 1.096957802772522,
      "learning_rate": 0.00014963241436925648,
      "loss": 1.0299,
      "step": 6030
    },
    {
      "epoch": 1.7660818713450293,
      "grad_norm": 0.9582836031913757,
      "learning_rate": 0.00014954887218045112,
      "loss": 1.0896,
      "step": 6040
    },
    {
      "epoch": 1.7690058479532165,
      "grad_norm": 1.1353545188903809,
      "learning_rate": 0.0001494653299916458,
      "loss": 1.0328,
      "step": 6050
    },
    {
      "epoch": 1.7719298245614035,
      "grad_norm": 0.7497010231018066,
      "learning_rate": 0.00014938178780284046,
      "loss": 1.0097,
      "step": 6060
    },
    {
      "epoch": 1.7748538011695907,
      "grad_norm": 0.8873158693313599,
      "learning_rate": 0.0001492982456140351,
      "loss": 1.0043,
      "step": 6070
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.6526322960853577,
      "learning_rate": 0.00014921470342522974,
      "loss": 1.0395,
      "step": 6080
    },
    {
      "epoch": 1.780701754385965,
      "grad_norm": 0.8233802914619446,
      "learning_rate": 0.0001491311612364244,
      "loss": 1.0808,
      "step": 6090
    },
    {
      "epoch": 1.7836257309941521,
      "grad_norm": 0.7522332668304443,
      "learning_rate": 0.00014904761904761904,
      "loss": 0.9672,
      "step": 6100
    },
    {
      "epoch": 1.7865497076023393,
      "grad_norm": 0.7235121130943298,
      "learning_rate": 0.0001489640768588137,
      "loss": 0.9672,
      "step": 6110
    },
    {
      "epoch": 1.7894736842105263,
      "grad_norm": 0.745445191860199,
      "learning_rate": 0.00014888053467000835,
      "loss": 0.9628,
      "step": 6120
    },
    {
      "epoch": 1.7923976608187133,
      "grad_norm": 0.8039499521255493,
      "learning_rate": 0.00014879699248120302,
      "loss": 1.0173,
      "step": 6130
    },
    {
      "epoch": 1.7953216374269005,
      "grad_norm": 0.7904906272888184,
      "learning_rate": 0.00014871345029239766,
      "loss": 1.0942,
      "step": 6140
    },
    {
      "epoch": 1.7982456140350878,
      "grad_norm": 1.0444998741149902,
      "learning_rate": 0.00014862990810359233,
      "loss": 1.0534,
      "step": 6150
    },
    {
      "epoch": 1.801169590643275,
      "grad_norm": 0.8249980807304382,
      "learning_rate": 0.00014854636591478697,
      "loss": 1.0386,
      "step": 6160
    },
    {
      "epoch": 1.804093567251462,
      "grad_norm": 0.6305757164955139,
      "learning_rate": 0.00014846282372598163,
      "loss": 1.004,
      "step": 6170
    },
    {
      "epoch": 1.807017543859649,
      "grad_norm": 1.035713791847229,
      "learning_rate": 0.00014837928153717627,
      "loss": 1.0619,
      "step": 6180
    },
    {
      "epoch": 1.8099415204678362,
      "grad_norm": 0.731237530708313,
      "learning_rate": 0.00014829573934837094,
      "loss": 1.1054,
      "step": 6190
    },
    {
      "epoch": 1.8128654970760234,
      "grad_norm": 0.9657135009765625,
      "learning_rate": 0.00014821219715956558,
      "loss": 1.0481,
      "step": 6200
    },
    {
      "epoch": 1.8157894736842106,
      "grad_norm": 0.9902162551879883,
      "learning_rate": 0.00014812865497076025,
      "loss": 0.9032,
      "step": 6210
    },
    {
      "epoch": 1.8187134502923976,
      "grad_norm": 0.880795955657959,
      "learning_rate": 0.0001480451127819549,
      "loss": 0.944,
      "step": 6220
    },
    {
      "epoch": 1.8216374269005848,
      "grad_norm": 0.6524759531021118,
      "learning_rate": 0.00014796157059314955,
      "loss": 1.11,
      "step": 6230
    },
    {
      "epoch": 1.8245614035087718,
      "grad_norm": 0.8564992547035217,
      "learning_rate": 0.0001478780284043442,
      "loss": 1.034,
      "step": 6240
    },
    {
      "epoch": 1.827485380116959,
      "grad_norm": 0.8535910844802856,
      "learning_rate": 0.00014779448621553886,
      "loss": 1.1288,
      "step": 6250
    },
    {
      "epoch": 1.8304093567251463,
      "grad_norm": 0.9563493132591248,
      "learning_rate": 0.0001477109440267335,
      "loss": 0.9205,
      "step": 6260
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.8313044309616089,
      "learning_rate": 0.00014762740183792817,
      "loss": 1.0951,
      "step": 6270
    },
    {
      "epoch": 1.8362573099415205,
      "grad_norm": 0.879166841506958,
      "learning_rate": 0.0001475438596491228,
      "loss": 1.0724,
      "step": 6280
    },
    {
      "epoch": 1.8391812865497075,
      "grad_norm": 0.9015559554100037,
      "learning_rate": 0.00014746031746031747,
      "loss": 1.1322,
      "step": 6290
    },
    {
      "epoch": 1.8421052631578947,
      "grad_norm": 0.9113186597824097,
      "learning_rate": 0.00014737677527151211,
      "loss": 1.1045,
      "step": 6300
    },
    {
      "epoch": 1.845029239766082,
      "grad_norm": 0.7433578968048096,
      "learning_rate": 0.00014729323308270678,
      "loss": 1.0036,
      "step": 6310
    },
    {
      "epoch": 1.8479532163742691,
      "grad_norm": 0.7605006098747253,
      "learning_rate": 0.00014720969089390142,
      "loss": 1.0507,
      "step": 6320
    },
    {
      "epoch": 1.8508771929824561,
      "grad_norm": 1.0812294483184814,
      "learning_rate": 0.0001471261487050961,
      "loss": 1.0028,
      "step": 6330
    },
    {
      "epoch": 1.8538011695906431,
      "grad_norm": 0.8304830193519592,
      "learning_rate": 0.00014704260651629073,
      "loss": 1.0305,
      "step": 6340
    },
    {
      "epoch": 1.8567251461988303,
      "grad_norm": 0.8964489698410034,
      "learning_rate": 0.0001469590643274854,
      "loss": 1.1827,
      "step": 6350
    },
    {
      "epoch": 1.8596491228070176,
      "grad_norm": 0.7387428283691406,
      "learning_rate": 0.00014687552213868004,
      "loss": 1.0658,
      "step": 6360
    },
    {
      "epoch": 1.8625730994152048,
      "grad_norm": 0.8713249564170837,
      "learning_rate": 0.0001467919799498747,
      "loss": 1.0041,
      "step": 6370
    },
    {
      "epoch": 1.8654970760233918,
      "grad_norm": 0.7588321566581726,
      "learning_rate": 0.00014670843776106934,
      "loss": 1.0857,
      "step": 6380
    },
    {
      "epoch": 1.868421052631579,
      "grad_norm": 0.8497743010520935,
      "learning_rate": 0.000146624895572264,
      "loss": 0.9882,
      "step": 6390
    },
    {
      "epoch": 1.871345029239766,
      "grad_norm": 0.976786732673645,
      "learning_rate": 0.00014654135338345865,
      "loss": 1.0793,
      "step": 6400
    },
    {
      "epoch": 1.8742690058479532,
      "grad_norm": 0.8824559450149536,
      "learning_rate": 0.00014645781119465332,
      "loss": 1.0513,
      "step": 6410
    },
    {
      "epoch": 1.8771929824561404,
      "grad_norm": 0.9232054352760315,
      "learning_rate": 0.00014637426900584796,
      "loss": 1.0969,
      "step": 6420
    },
    {
      "epoch": 1.8801169590643276,
      "grad_norm": 0.6984622478485107,
      "learning_rate": 0.00014629072681704262,
      "loss": 0.9979,
      "step": 6430
    },
    {
      "epoch": 1.8830409356725146,
      "grad_norm": 0.6715920567512512,
      "learning_rate": 0.00014620718462823726,
      "loss": 1.0663,
      "step": 6440
    },
    {
      "epoch": 1.8859649122807016,
      "grad_norm": 1.0385491847991943,
      "learning_rate": 0.00014612364243943193,
      "loss": 1.0405,
      "step": 6450
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.7705148458480835,
      "learning_rate": 0.00014604010025062657,
      "loss": 1.0178,
      "step": 6460
    },
    {
      "epoch": 1.891812865497076,
      "grad_norm": 1.1990699768066406,
      "learning_rate": 0.00014595655806182124,
      "loss": 1.145,
      "step": 6470
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 0.8977970480918884,
      "learning_rate": 0.00014587301587301588,
      "loss": 1.0006,
      "step": 6480
    },
    {
      "epoch": 1.8976608187134503,
      "grad_norm": 0.84847491979599,
      "learning_rate": 0.00014578947368421054,
      "loss": 0.9627,
      "step": 6490
    },
    {
      "epoch": 1.9005847953216373,
      "grad_norm": 0.7853485345840454,
      "learning_rate": 0.00014570593149540518,
      "loss": 1.035,
      "step": 6500
    },
    {
      "epoch": 1.9035087719298245,
      "grad_norm": 0.7171883583068848,
      "learning_rate": 0.00014562238930659982,
      "loss": 1.1468,
      "step": 6510
    },
    {
      "epoch": 1.9064327485380117,
      "grad_norm": 0.912750780582428,
      "learning_rate": 0.0001455388471177945,
      "loss": 1.112,
      "step": 6520
    },
    {
      "epoch": 1.909356725146199,
      "grad_norm": 0.6815913915634155,
      "learning_rate": 0.00014545530492898916,
      "loss": 1.1001,
      "step": 6530
    },
    {
      "epoch": 1.912280701754386,
      "grad_norm": 0.7445448637008667,
      "learning_rate": 0.0001453717627401838,
      "loss": 0.9461,
      "step": 6540
    },
    {
      "epoch": 1.9152046783625731,
      "grad_norm": 0.8485434055328369,
      "learning_rate": 0.00014528822055137844,
      "loss": 1.0071,
      "step": 6550
    },
    {
      "epoch": 1.9181286549707601,
      "grad_norm": 0.7984713315963745,
      "learning_rate": 0.0001452046783625731,
      "loss": 1.0156,
      "step": 6560
    },
    {
      "epoch": 1.9210526315789473,
      "grad_norm": 0.7794836759567261,
      "learning_rate": 0.00014512113617376777,
      "loss": 1.0373,
      "step": 6570
    },
    {
      "epoch": 1.9239766081871346,
      "grad_norm": 0.7026998996734619,
      "learning_rate": 0.0001450375939849624,
      "loss": 1.027,
      "step": 6580
    },
    {
      "epoch": 1.9269005847953218,
      "grad_norm": 1.0947906970977783,
      "learning_rate": 0.00014495405179615705,
      "loss": 1.0261,
      "step": 6590
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 0.8670282363891602,
      "learning_rate": 0.00014487050960735172,
      "loss": 1.027,
      "step": 6600
    },
    {
      "epoch": 1.9327485380116958,
      "grad_norm": 0.7868095636367798,
      "learning_rate": 0.0001447869674185464,
      "loss": 0.9686,
      "step": 6610
    },
    {
      "epoch": 1.935672514619883,
      "grad_norm": 0.653630256652832,
      "learning_rate": 0.00014470342522974103,
      "loss": 1.0432,
      "step": 6620
    },
    {
      "epoch": 1.9385964912280702,
      "grad_norm": 0.8747307062149048,
      "learning_rate": 0.00014461988304093567,
      "loss": 1.0209,
      "step": 6630
    },
    {
      "epoch": 1.9415204678362574,
      "grad_norm": 0.9032412171363831,
      "learning_rate": 0.00014453634085213033,
      "loss": 0.9318,
      "step": 6640
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.6919759511947632,
      "learning_rate": 0.000144452798663325,
      "loss": 1.0375,
      "step": 6650
    },
    {
      "epoch": 1.9473684210526314,
      "grad_norm": 0.6976118683815002,
      "learning_rate": 0.00014436925647451964,
      "loss": 1.0088,
      "step": 6660
    },
    {
      "epoch": 1.9502923976608186,
      "grad_norm": 0.7861925363540649,
      "learning_rate": 0.00014428571428571428,
      "loss": 1.02,
      "step": 6670
    },
    {
      "epoch": 1.9532163742690059,
      "grad_norm": 0.8684263825416565,
      "learning_rate": 0.00014420217209690895,
      "loss": 0.936,
      "step": 6680
    },
    {
      "epoch": 1.956140350877193,
      "grad_norm": 0.7881571054458618,
      "learning_rate": 0.00014411862990810361,
      "loss": 1.0052,
      "step": 6690
    },
    {
      "epoch": 1.95906432748538,
      "grad_norm": 0.7526548504829407,
      "learning_rate": 0.00014403508771929825,
      "loss": 0.9389,
      "step": 6700
    },
    {
      "epoch": 1.9619883040935673,
      "grad_norm": 0.8197621703147888,
      "learning_rate": 0.0001439515455304929,
      "loss": 1.03,
      "step": 6710
    },
    {
      "epoch": 1.9649122807017543,
      "grad_norm": 0.6680834293365479,
      "learning_rate": 0.00014386800334168756,
      "loss": 0.9784,
      "step": 6720
    },
    {
      "epoch": 1.9678362573099415,
      "grad_norm": 0.7489207983016968,
      "learning_rate": 0.00014378446115288223,
      "loss": 1.0595,
      "step": 6730
    },
    {
      "epoch": 1.9707602339181287,
      "grad_norm": 0.8241118788719177,
      "learning_rate": 0.00014370091896407687,
      "loss": 1.0216,
      "step": 6740
    },
    {
      "epoch": 1.973684210526316,
      "grad_norm": 0.8151628971099854,
      "learning_rate": 0.0001436173767752715,
      "loss": 0.9613,
      "step": 6750
    },
    {
      "epoch": 1.976608187134503,
      "grad_norm": 0.7601003050804138,
      "learning_rate": 0.00014353383458646618,
      "loss": 1.061,
      "step": 6760
    },
    {
      "epoch": 1.97953216374269,
      "grad_norm": 0.8381856083869934,
      "learning_rate": 0.00014345029239766084,
      "loss": 0.9519,
      "step": 6770
    },
    {
      "epoch": 1.9824561403508771,
      "grad_norm": 0.9738539457321167,
      "learning_rate": 0.00014336675020885548,
      "loss": 0.972,
      "step": 6780
    },
    {
      "epoch": 1.9853801169590644,
      "grad_norm": 0.766400933265686,
      "learning_rate": 0.00014328320802005012,
      "loss": 1.0761,
      "step": 6790
    },
    {
      "epoch": 1.9883040935672516,
      "grad_norm": 0.6503599286079407,
      "learning_rate": 0.0001431996658312448,
      "loss": 1.0594,
      "step": 6800
    },
    {
      "epoch": 1.9912280701754386,
      "grad_norm": 0.7473729252815247,
      "learning_rate": 0.00014311612364243946,
      "loss": 1.0744,
      "step": 6810
    },
    {
      "epoch": 1.9941520467836256,
      "grad_norm": 0.8831413388252258,
      "learning_rate": 0.0001430325814536341,
      "loss": 1.1009,
      "step": 6820
    },
    {
      "epoch": 1.9970760233918128,
      "grad_norm": 1.0257941484451294,
      "learning_rate": 0.00014294903926482874,
      "loss": 0.9401,
      "step": 6830
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.9703757762908936,
      "learning_rate": 0.0001428654970760234,
      "loss": 1.0854,
      "step": 6840
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9323145151138306,
      "eval_runtime": 75.9414,
      "eval_samples_per_second": 11.706,
      "eval_steps_per_second": 1.475,
      "step": 6840
    },
    {
      "epoch": 2.002923976608187,
      "grad_norm": 0.7176637053489685,
      "learning_rate": 0.00014278195488721807,
      "loss": 0.9818,
      "step": 6850
    },
    {
      "epoch": 2.0058479532163744,
      "grad_norm": 0.8591043949127197,
      "learning_rate": 0.0001426984126984127,
      "loss": 0.9544,
      "step": 6860
    },
    {
      "epoch": 2.008771929824561,
      "grad_norm": 0.9553587436676025,
      "learning_rate": 0.00014261487050960735,
      "loss": 1.1555,
      "step": 6870
    },
    {
      "epoch": 2.0116959064327484,
      "grad_norm": 0.786310076713562,
      "learning_rate": 0.00014253132832080202,
      "loss": 0.9252,
      "step": 6880
    },
    {
      "epoch": 2.0146198830409356,
      "grad_norm": 0.8434907793998718,
      "learning_rate": 0.00014244778613199668,
      "loss": 0.9629,
      "step": 6890
    },
    {
      "epoch": 2.017543859649123,
      "grad_norm": 0.8413931727409363,
      "learning_rate": 0.00014236424394319132,
      "loss": 0.9501,
      "step": 6900
    },
    {
      "epoch": 2.02046783625731,
      "grad_norm": 1.010655403137207,
      "learning_rate": 0.00014228070175438596,
      "loss": 0.9647,
      "step": 6910
    },
    {
      "epoch": 2.023391812865497,
      "grad_norm": 0.6711873412132263,
      "learning_rate": 0.0001421971595655806,
      "loss": 0.904,
      "step": 6920
    },
    {
      "epoch": 2.026315789473684,
      "grad_norm": 0.6996467113494873,
      "learning_rate": 0.0001421136173767753,
      "loss": 0.9382,
      "step": 6930
    },
    {
      "epoch": 2.0292397660818713,
      "grad_norm": 0.9451034069061279,
      "learning_rate": 0.00014203007518796994,
      "loss": 1.0021,
      "step": 6940
    },
    {
      "epoch": 2.0321637426900585,
      "grad_norm": 0.8980836272239685,
      "learning_rate": 0.00014194653299916458,
      "loss": 0.9206,
      "step": 6950
    },
    {
      "epoch": 2.0350877192982457,
      "grad_norm": 0.8019334673881531,
      "learning_rate": 0.00014186299081035922,
      "loss": 0.9948,
      "step": 6960
    },
    {
      "epoch": 2.038011695906433,
      "grad_norm": 0.668053388595581,
      "learning_rate": 0.0001417794486215539,
      "loss": 0.9789,
      "step": 6970
    },
    {
      "epoch": 2.0409356725146197,
      "grad_norm": 0.8943573236465454,
      "learning_rate": 0.00014169590643274855,
      "loss": 0.942,
      "step": 6980
    },
    {
      "epoch": 2.043859649122807,
      "grad_norm": 0.8275060057640076,
      "learning_rate": 0.0001416123642439432,
      "loss": 1.0641,
      "step": 6990
    },
    {
      "epoch": 2.046783625730994,
      "grad_norm": 0.6938260793685913,
      "learning_rate": 0.00014152882205513783,
      "loss": 0.9542,
      "step": 7000
    },
    {
      "epoch": 2.0497076023391814,
      "grad_norm": 0.931510865688324,
      "learning_rate": 0.00014144527986633253,
      "loss": 1.0016,
      "step": 7010
    },
    {
      "epoch": 2.0526315789473686,
      "grad_norm": 0.7387519478797913,
      "learning_rate": 0.00014136173767752717,
      "loss": 1.042,
      "step": 7020
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 0.8827378749847412,
      "learning_rate": 0.0001412781954887218,
      "loss": 1.084,
      "step": 7030
    },
    {
      "epoch": 2.0584795321637426,
      "grad_norm": 1.178123116493225,
      "learning_rate": 0.00014119465329991645,
      "loss": 0.9463,
      "step": 7040
    },
    {
      "epoch": 2.06140350877193,
      "grad_norm": 0.7217741012573242,
      "learning_rate": 0.00014111111111111111,
      "loss": 0.9224,
      "step": 7050
    },
    {
      "epoch": 2.064327485380117,
      "grad_norm": 0.9185336828231812,
      "learning_rate": 0.00014102756892230578,
      "loss": 1.0674,
      "step": 7060
    },
    {
      "epoch": 2.0672514619883042,
      "grad_norm": 0.9472691416740417,
      "learning_rate": 0.00014094402673350042,
      "loss": 0.9526,
      "step": 7070
    },
    {
      "epoch": 2.0701754385964914,
      "grad_norm": 0.8298478722572327,
      "learning_rate": 0.00014086048454469506,
      "loss": 0.9755,
      "step": 7080
    },
    {
      "epoch": 2.073099415204678,
      "grad_norm": 0.7849639058113098,
      "learning_rate": 0.00014077694235588973,
      "loss": 0.8783,
      "step": 7090
    },
    {
      "epoch": 2.0760233918128654,
      "grad_norm": 0.7152796387672424,
      "learning_rate": 0.0001406934001670844,
      "loss": 0.9464,
      "step": 7100
    },
    {
      "epoch": 2.0789473684210527,
      "grad_norm": 0.8841535449028015,
      "learning_rate": 0.00014060985797827903,
      "loss": 0.9737,
      "step": 7110
    },
    {
      "epoch": 2.08187134502924,
      "grad_norm": 0.7102460861206055,
      "learning_rate": 0.00014052631578947367,
      "loss": 0.8911,
      "step": 7120
    },
    {
      "epoch": 2.084795321637427,
      "grad_norm": 0.9300104379653931,
      "learning_rate": 0.00014044277360066834,
      "loss": 0.9822,
      "step": 7130
    },
    {
      "epoch": 2.087719298245614,
      "grad_norm": 0.9082975387573242,
      "learning_rate": 0.000140359231411863,
      "loss": 0.9712,
      "step": 7140
    },
    {
      "epoch": 2.090643274853801,
      "grad_norm": 0.8085709810256958,
      "learning_rate": 0.00014027568922305765,
      "loss": 0.9616,
      "step": 7150
    },
    {
      "epoch": 2.0935672514619883,
      "grad_norm": 0.7139132618904114,
      "learning_rate": 0.0001401921470342523,
      "loss": 0.9833,
      "step": 7160
    },
    {
      "epoch": 2.0964912280701755,
      "grad_norm": 0.6489682793617249,
      "learning_rate": 0.00014010860484544696,
      "loss": 0.9231,
      "step": 7170
    },
    {
      "epoch": 2.0994152046783627,
      "grad_norm": 0.8561878204345703,
      "learning_rate": 0.00014002506265664162,
      "loss": 0.9214,
      "step": 7180
    },
    {
      "epoch": 2.1023391812865495,
      "grad_norm": 0.6860496401786804,
      "learning_rate": 0.00013994152046783626,
      "loss": 0.9894,
      "step": 7190
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 1.0589509010314941,
      "learning_rate": 0.0001398579782790309,
      "loss": 0.9535,
      "step": 7200
    },
    {
      "epoch": 2.108187134502924,
      "grad_norm": 0.6518213152885437,
      "learning_rate": 0.00013977443609022557,
      "loss": 1.008,
      "step": 7210
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.8875017762184143,
      "learning_rate": 0.00013969089390142024,
      "loss": 0.9848,
      "step": 7220
    },
    {
      "epoch": 2.1140350877192984,
      "grad_norm": 0.8581036925315857,
      "learning_rate": 0.00013960735171261488,
      "loss": 0.8953,
      "step": 7230
    },
    {
      "epoch": 2.116959064327485,
      "grad_norm": 0.7246058583259583,
      "learning_rate": 0.00013952380952380952,
      "loss": 0.8919,
      "step": 7240
    },
    {
      "epoch": 2.1198830409356724,
      "grad_norm": 0.7212415933609009,
      "learning_rate": 0.00013944026733500418,
      "loss": 1.0069,
      "step": 7250
    },
    {
      "epoch": 2.1228070175438596,
      "grad_norm": 0.7360502481460571,
      "learning_rate": 0.00013935672514619885,
      "loss": 0.9687,
      "step": 7260
    },
    {
      "epoch": 2.125730994152047,
      "grad_norm": 0.6970664858818054,
      "learning_rate": 0.0001392731829573935,
      "loss": 0.9693,
      "step": 7270
    },
    {
      "epoch": 2.128654970760234,
      "grad_norm": 0.8209109902381897,
      "learning_rate": 0.00013918964076858813,
      "loss": 0.9537,
      "step": 7280
    },
    {
      "epoch": 2.1315789473684212,
      "grad_norm": 0.8982468247413635,
      "learning_rate": 0.0001391060985797828,
      "loss": 0.992,
      "step": 7290
    },
    {
      "epoch": 2.134502923976608,
      "grad_norm": 0.7057675719261169,
      "learning_rate": 0.00013902255639097746,
      "loss": 0.9111,
      "step": 7300
    },
    {
      "epoch": 2.1374269005847952,
      "grad_norm": 0.809746503829956,
      "learning_rate": 0.0001389390142021721,
      "loss": 0.9884,
      "step": 7310
    },
    {
      "epoch": 2.1403508771929824,
      "grad_norm": 0.8210552930831909,
      "learning_rate": 0.00013885547201336674,
      "loss": 0.9538,
      "step": 7320
    },
    {
      "epoch": 2.1432748538011697,
      "grad_norm": 1.0128579139709473,
      "learning_rate": 0.0001387719298245614,
      "loss": 0.9763,
      "step": 7330
    },
    {
      "epoch": 2.146198830409357,
      "grad_norm": 0.8679237961769104,
      "learning_rate": 0.00013868838763575608,
      "loss": 0.9226,
      "step": 7340
    },
    {
      "epoch": 2.1491228070175437,
      "grad_norm": 0.8105077743530273,
      "learning_rate": 0.00013860484544695072,
      "loss": 0.9756,
      "step": 7350
    },
    {
      "epoch": 2.152046783625731,
      "grad_norm": 0.8234046697616577,
      "learning_rate": 0.00013852130325814536,
      "loss": 0.9169,
      "step": 7360
    },
    {
      "epoch": 2.154970760233918,
      "grad_norm": 0.8611997961997986,
      "learning_rate": 0.00013843776106934003,
      "loss": 0.9861,
      "step": 7370
    },
    {
      "epoch": 2.1578947368421053,
      "grad_norm": 0.628132164478302,
      "learning_rate": 0.0001383542188805347,
      "loss": 0.8898,
      "step": 7380
    },
    {
      "epoch": 2.1608187134502925,
      "grad_norm": 0.7680389881134033,
      "learning_rate": 0.00013827067669172933,
      "loss": 0.9888,
      "step": 7390
    },
    {
      "epoch": 2.1637426900584797,
      "grad_norm": 0.9237713813781738,
      "learning_rate": 0.00013818713450292397,
      "loss": 0.9821,
      "step": 7400
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 2.054170608520508,
      "learning_rate": 0.00013810359231411864,
      "loss": 1.1122,
      "step": 7410
    },
    {
      "epoch": 2.1695906432748537,
      "grad_norm": 0.7880015969276428,
      "learning_rate": 0.0001380200501253133,
      "loss": 0.9578,
      "step": 7420
    },
    {
      "epoch": 2.172514619883041,
      "grad_norm": 0.972385585308075,
      "learning_rate": 0.00013793650793650795,
      "loss": 1.0693,
      "step": 7430
    },
    {
      "epoch": 2.175438596491228,
      "grad_norm": 0.9721779227256775,
      "learning_rate": 0.0001378529657477026,
      "loss": 1.0986,
      "step": 7440
    },
    {
      "epoch": 2.1783625730994154,
      "grad_norm": 0.770631730556488,
      "learning_rate": 0.00013776942355889725,
      "loss": 0.9859,
      "step": 7450
    },
    {
      "epoch": 2.181286549707602,
      "grad_norm": 0.8838427662849426,
      "learning_rate": 0.0001376858813700919,
      "loss": 0.9331,
      "step": 7460
    },
    {
      "epoch": 2.1842105263157894,
      "grad_norm": 0.9655161499977112,
      "learning_rate": 0.00013760233918128656,
      "loss": 1.0203,
      "step": 7470
    },
    {
      "epoch": 2.1871345029239766,
      "grad_norm": 0.9556739330291748,
      "learning_rate": 0.0001375187969924812,
      "loss": 0.9282,
      "step": 7480
    },
    {
      "epoch": 2.190058479532164,
      "grad_norm": 0.830755352973938,
      "learning_rate": 0.00013743525480367587,
      "loss": 0.9943,
      "step": 7490
    },
    {
      "epoch": 2.192982456140351,
      "grad_norm": 0.8549703359603882,
      "learning_rate": 0.0001373517126148705,
      "loss": 1.0133,
      "step": 7500
    },
    {
      "epoch": 2.195906432748538,
      "grad_norm": 0.7623694539070129,
      "learning_rate": 0.00013726817042606517,
      "loss": 0.9565,
      "step": 7510
    },
    {
      "epoch": 2.198830409356725,
      "grad_norm": 0.827236533164978,
      "learning_rate": 0.00013718462823725981,
      "loss": 1.0424,
      "step": 7520
    },
    {
      "epoch": 2.2017543859649122,
      "grad_norm": 0.8913207054138184,
      "learning_rate": 0.00013710108604845448,
      "loss": 1.0427,
      "step": 7530
    },
    {
      "epoch": 2.2046783625730995,
      "grad_norm": 0.6612926125526428,
      "learning_rate": 0.00013701754385964912,
      "loss": 0.869,
      "step": 7540
    },
    {
      "epoch": 2.2076023391812867,
      "grad_norm": 0.9370080828666687,
      "learning_rate": 0.0001369340016708438,
      "loss": 1.0111,
      "step": 7550
    },
    {
      "epoch": 2.2105263157894735,
      "grad_norm": 0.7659274339675903,
      "learning_rate": 0.00013685045948203843,
      "loss": 0.9332,
      "step": 7560
    },
    {
      "epoch": 2.2134502923976607,
      "grad_norm": 0.9509808421134949,
      "learning_rate": 0.0001367669172932331,
      "loss": 0.9544,
      "step": 7570
    },
    {
      "epoch": 2.216374269005848,
      "grad_norm": 0.6715356111526489,
      "learning_rate": 0.00013668337510442774,
      "loss": 0.9071,
      "step": 7580
    },
    {
      "epoch": 2.219298245614035,
      "grad_norm": 0.590428352355957,
      "learning_rate": 0.0001365998329156224,
      "loss": 1.0249,
      "step": 7590
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.76816725730896,
      "learning_rate": 0.00013651629072681704,
      "loss": 0.8971,
      "step": 7600
    },
    {
      "epoch": 2.2251461988304095,
      "grad_norm": 0.7557430863380432,
      "learning_rate": 0.0001364327485380117,
      "loss": 0.9066,
      "step": 7610
    },
    {
      "epoch": 2.2280701754385963,
      "grad_norm": 0.6815080642700195,
      "learning_rate": 0.00013634920634920635,
      "loss": 0.9354,
      "step": 7620
    },
    {
      "epoch": 2.2309941520467835,
      "grad_norm": 0.8453577756881714,
      "learning_rate": 0.00013626566416040102,
      "loss": 0.9878,
      "step": 7630
    },
    {
      "epoch": 2.2339181286549707,
      "grad_norm": 0.8028250932693481,
      "learning_rate": 0.00013618212197159566,
      "loss": 0.9111,
      "step": 7640
    },
    {
      "epoch": 2.236842105263158,
      "grad_norm": 0.7564089894294739,
      "learning_rate": 0.00013609857978279032,
      "loss": 1.0008,
      "step": 7650
    },
    {
      "epoch": 2.239766081871345,
      "grad_norm": 0.7801581025123596,
      "learning_rate": 0.00013601503759398496,
      "loss": 0.9829,
      "step": 7660
    },
    {
      "epoch": 2.242690058479532,
      "grad_norm": 0.9070150256156921,
      "learning_rate": 0.00013593149540517963,
      "loss": 0.9772,
      "step": 7670
    },
    {
      "epoch": 2.245614035087719,
      "grad_norm": 0.9742885231971741,
      "learning_rate": 0.00013584795321637427,
      "loss": 0.9154,
      "step": 7680
    },
    {
      "epoch": 2.2485380116959064,
      "grad_norm": 0.8035116195678711,
      "learning_rate": 0.00013576441102756894,
      "loss": 0.9015,
      "step": 7690
    },
    {
      "epoch": 2.2514619883040936,
      "grad_norm": 1.1086434125900269,
      "learning_rate": 0.00013568086883876358,
      "loss": 0.9213,
      "step": 7700
    },
    {
      "epoch": 2.254385964912281,
      "grad_norm": 0.7680662870407104,
      "learning_rate": 0.00013559732664995824,
      "loss": 0.927,
      "step": 7710
    },
    {
      "epoch": 2.257309941520468,
      "grad_norm": 0.7511858940124512,
      "learning_rate": 0.00013551378446115288,
      "loss": 0.9654,
      "step": 7720
    },
    {
      "epoch": 2.260233918128655,
      "grad_norm": 0.7547529339790344,
      "learning_rate": 0.00013543024227234755,
      "loss": 1.0347,
      "step": 7730
    },
    {
      "epoch": 2.263157894736842,
      "grad_norm": 0.683839738368988,
      "learning_rate": 0.0001353467000835422,
      "loss": 0.9242,
      "step": 7740
    },
    {
      "epoch": 2.2660818713450293,
      "grad_norm": 1.0659252405166626,
      "learning_rate": 0.00013526315789473686,
      "loss": 0.9423,
      "step": 7750
    },
    {
      "epoch": 2.2690058479532165,
      "grad_norm": 0.7823368906974792,
      "learning_rate": 0.0001351796157059315,
      "loss": 0.9324,
      "step": 7760
    },
    {
      "epoch": 2.2719298245614037,
      "grad_norm": 0.9173340797424316,
      "learning_rate": 0.00013509607351712617,
      "loss": 0.9698,
      "step": 7770
    },
    {
      "epoch": 2.2748538011695905,
      "grad_norm": 0.8542994856834412,
      "learning_rate": 0.0001350125313283208,
      "loss": 0.9855,
      "step": 7780
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 0.8800181746482849,
      "learning_rate": 0.00013492898913951547,
      "loss": 0.9812,
      "step": 7790
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 0.7974490523338318,
      "learning_rate": 0.0001348454469507101,
      "loss": 0.9496,
      "step": 7800
    },
    {
      "epoch": 2.283625730994152,
      "grad_norm": 0.7278539538383484,
      "learning_rate": 0.00013476190476190478,
      "loss": 0.8964,
      "step": 7810
    },
    {
      "epoch": 2.2865497076023393,
      "grad_norm": 1.1071090698242188,
      "learning_rate": 0.00013467836257309942,
      "loss": 0.9794,
      "step": 7820
    },
    {
      "epoch": 2.2894736842105265,
      "grad_norm": 0.9082510471343994,
      "learning_rate": 0.0001345948203842941,
      "loss": 0.9426,
      "step": 7830
    },
    {
      "epoch": 2.2923976608187133,
      "grad_norm": 0.8852719664573669,
      "learning_rate": 0.00013451127819548873,
      "loss": 0.9347,
      "step": 7840
    },
    {
      "epoch": 2.2953216374269005,
      "grad_norm": 0.9304066300392151,
      "learning_rate": 0.0001344277360066834,
      "loss": 0.9613,
      "step": 7850
    },
    {
      "epoch": 2.2982456140350878,
      "grad_norm": 1.089821219444275,
      "learning_rate": 0.00013434419381787803,
      "loss": 1.0227,
      "step": 7860
    },
    {
      "epoch": 2.301169590643275,
      "grad_norm": 1.0199503898620605,
      "learning_rate": 0.00013426065162907267,
      "loss": 0.9423,
      "step": 7870
    },
    {
      "epoch": 2.3040935672514617,
      "grad_norm": 0.8094996809959412,
      "learning_rate": 0.00013417710944026734,
      "loss": 0.9721,
      "step": 7880
    },
    {
      "epoch": 2.307017543859649,
      "grad_norm": 0.8198239207267761,
      "learning_rate": 0.000134093567251462,
      "loss": 1.006,
      "step": 7890
    },
    {
      "epoch": 2.309941520467836,
      "grad_norm": 0.9192975163459778,
      "learning_rate": 0.00013401002506265665,
      "loss": 1.1349,
      "step": 7900
    },
    {
      "epoch": 2.3128654970760234,
      "grad_norm": 0.8897823691368103,
      "learning_rate": 0.0001339264828738513,
      "loss": 0.9305,
      "step": 7910
    },
    {
      "epoch": 2.3157894736842106,
      "grad_norm": 0.7915451526641846,
      "learning_rate": 0.00013384294068504595,
      "loss": 0.9825,
      "step": 7920
    },
    {
      "epoch": 2.318713450292398,
      "grad_norm": 0.8684319853782654,
      "learning_rate": 0.00013375939849624062,
      "loss": 0.9479,
      "step": 7930
    },
    {
      "epoch": 2.3216374269005846,
      "grad_norm": 0.6825398802757263,
      "learning_rate": 0.00013367585630743526,
      "loss": 0.9232,
      "step": 7940
    },
    {
      "epoch": 2.324561403508772,
      "grad_norm": 0.7687299847602844,
      "learning_rate": 0.0001335923141186299,
      "loss": 1.0209,
      "step": 7950
    },
    {
      "epoch": 2.327485380116959,
      "grad_norm": 0.7661171555519104,
      "learning_rate": 0.00013350877192982457,
      "loss": 0.8841,
      "step": 7960
    },
    {
      "epoch": 2.3304093567251463,
      "grad_norm": 0.7516762614250183,
      "learning_rate": 0.00013342522974101924,
      "loss": 0.9774,
      "step": 7970
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.9168811440467834,
      "learning_rate": 0.00013334168755221388,
      "loss": 0.9773,
      "step": 7980
    },
    {
      "epoch": 2.3362573099415203,
      "grad_norm": 0.9598943591117859,
      "learning_rate": 0.00013325814536340852,
      "loss": 0.9755,
      "step": 7990
    },
    {
      "epoch": 2.3391812865497075,
      "grad_norm": 1.0008716583251953,
      "learning_rate": 0.00013317460317460318,
      "loss": 0.9986,
      "step": 8000
    },
    {
      "epoch": 2.3421052631578947,
      "grad_norm": 0.9342864155769348,
      "learning_rate": 0.00013309106098579785,
      "loss": 1.0297,
      "step": 8010
    },
    {
      "epoch": 2.345029239766082,
      "grad_norm": 0.7686882615089417,
      "learning_rate": 0.0001330075187969925,
      "loss": 0.9519,
      "step": 8020
    },
    {
      "epoch": 2.347953216374269,
      "grad_norm": 0.6718041300773621,
      "learning_rate": 0.00013292397660818713,
      "loss": 0.856,
      "step": 8030
    },
    {
      "epoch": 2.3508771929824563,
      "grad_norm": 0.8743940591812134,
      "learning_rate": 0.0001328404344193818,
      "loss": 0.9794,
      "step": 8040
    },
    {
      "epoch": 2.353801169590643,
      "grad_norm": 0.8532882332801819,
      "learning_rate": 0.00013275689223057646,
      "loss": 0.9881,
      "step": 8050
    },
    {
      "epoch": 2.3567251461988303,
      "grad_norm": 0.7556646466255188,
      "learning_rate": 0.0001326733500417711,
      "loss": 0.9019,
      "step": 8060
    },
    {
      "epoch": 2.3596491228070176,
      "grad_norm": 0.7895520925521851,
      "learning_rate": 0.00013258980785296574,
      "loss": 1.04,
      "step": 8070
    },
    {
      "epoch": 2.3625730994152048,
      "grad_norm": 0.6237192749977112,
      "learning_rate": 0.0001325062656641604,
      "loss": 1.0463,
      "step": 8080
    },
    {
      "epoch": 2.365497076023392,
      "grad_norm": 0.8309014439582825,
      "learning_rate": 0.00013242272347535508,
      "loss": 0.9738,
      "step": 8090
    },
    {
      "epoch": 2.3684210526315788,
      "grad_norm": 0.7722139954566956,
      "learning_rate": 0.00013233918128654972,
      "loss": 0.9535,
      "step": 8100
    },
    {
      "epoch": 2.371345029239766,
      "grad_norm": 0.8799992799758911,
      "learning_rate": 0.00013225563909774436,
      "loss": 0.9095,
      "step": 8110
    },
    {
      "epoch": 2.374269005847953,
      "grad_norm": 0.8488622903823853,
      "learning_rate": 0.00013217209690893902,
      "loss": 0.8709,
      "step": 8120
    },
    {
      "epoch": 2.3771929824561404,
      "grad_norm": 0.87410569190979,
      "learning_rate": 0.0001320885547201337,
      "loss": 0.9906,
      "step": 8130
    },
    {
      "epoch": 2.3801169590643276,
      "grad_norm": 0.9718879461288452,
      "learning_rate": 0.00013200501253132833,
      "loss": 0.9757,
      "step": 8140
    },
    {
      "epoch": 2.383040935672515,
      "grad_norm": 0.755785346031189,
      "learning_rate": 0.00013192147034252297,
      "loss": 0.9217,
      "step": 8150
    },
    {
      "epoch": 2.3859649122807016,
      "grad_norm": 0.7591664791107178,
      "learning_rate": 0.00013183792815371764,
      "loss": 0.9855,
      "step": 8160
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 0.7008959650993347,
      "learning_rate": 0.0001317543859649123,
      "loss": 0.9621,
      "step": 8170
    },
    {
      "epoch": 2.391812865497076,
      "grad_norm": 1.0269105434417725,
      "learning_rate": 0.00013167084377610695,
      "loss": 0.9883,
      "step": 8180
    },
    {
      "epoch": 2.3947368421052633,
      "grad_norm": 0.7175800204277039,
      "learning_rate": 0.00013158730158730159,
      "loss": 1.0024,
      "step": 8190
    },
    {
      "epoch": 2.39766081871345,
      "grad_norm": 0.8280737400054932,
      "learning_rate": 0.00013150375939849625,
      "loss": 0.9406,
      "step": 8200
    },
    {
      "epoch": 2.4005847953216373,
      "grad_norm": 0.7692899107933044,
      "learning_rate": 0.00013142021720969092,
      "loss": 0.9802,
      "step": 8210
    },
    {
      "epoch": 2.4035087719298245,
      "grad_norm": 0.6854656934738159,
      "learning_rate": 0.00013133667502088556,
      "loss": 0.8769,
      "step": 8220
    },
    {
      "epoch": 2.4064327485380117,
      "grad_norm": 0.6654913425445557,
      "learning_rate": 0.0001312531328320802,
      "loss": 0.9943,
      "step": 8230
    },
    {
      "epoch": 2.409356725146199,
      "grad_norm": 0.9693780541419983,
      "learning_rate": 0.00013116959064327487,
      "loss": 1.0086,
      "step": 8240
    },
    {
      "epoch": 2.412280701754386,
      "grad_norm": 0.7363418936729431,
      "learning_rate": 0.00013108604845446953,
      "loss": 0.9743,
      "step": 8250
    },
    {
      "epoch": 2.415204678362573,
      "grad_norm": 0.7549769282341003,
      "learning_rate": 0.00013100250626566417,
      "loss": 0.9021,
      "step": 8260
    },
    {
      "epoch": 2.41812865497076,
      "grad_norm": 0.9484103322029114,
      "learning_rate": 0.00013091896407685881,
      "loss": 0.9231,
      "step": 8270
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 0.7866630554199219,
      "learning_rate": 0.00013083542188805345,
      "loss": 0.8979,
      "step": 8280
    },
    {
      "epoch": 2.4239766081871346,
      "grad_norm": 0.7608885169029236,
      "learning_rate": 0.00013075187969924815,
      "loss": 0.9708,
      "step": 8290
    },
    {
      "epoch": 2.426900584795322,
      "grad_norm": 0.7056848406791687,
      "learning_rate": 0.0001306683375104428,
      "loss": 0.9771,
      "step": 8300
    },
    {
      "epoch": 2.4298245614035086,
      "grad_norm": 0.662995457649231,
      "learning_rate": 0.00013058479532163743,
      "loss": 0.9632,
      "step": 8310
    },
    {
      "epoch": 2.4327485380116958,
      "grad_norm": 0.902508556842804,
      "learning_rate": 0.00013050125313283207,
      "loss": 0.9941,
      "step": 8320
    },
    {
      "epoch": 2.435672514619883,
      "grad_norm": 0.7614319920539856,
      "learning_rate": 0.00013041771094402676,
      "loss": 0.9545,
      "step": 8330
    },
    {
      "epoch": 2.43859649122807,
      "grad_norm": 0.6190306544303894,
      "learning_rate": 0.0001303341687552214,
      "loss": 0.9641,
      "step": 8340
    },
    {
      "epoch": 2.4415204678362574,
      "grad_norm": 1.061598539352417,
      "learning_rate": 0.00013025062656641604,
      "loss": 1.0317,
      "step": 8350
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.8361424207687378,
      "learning_rate": 0.00013016708437761068,
      "loss": 0.9726,
      "step": 8360
    },
    {
      "epoch": 2.4473684210526314,
      "grad_norm": 0.7511029839515686,
      "learning_rate": 0.00013008354218880538,
      "loss": 0.9594,
      "step": 8370
    },
    {
      "epoch": 2.4502923976608186,
      "grad_norm": 0.8784418106079102,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.9456,
      "step": 8380
    },
    {
      "epoch": 2.453216374269006,
      "grad_norm": 0.7912513017654419,
      "learning_rate": 0.00012991645781119466,
      "loss": 0.927,
      "step": 8390
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 0.8258870840072632,
      "learning_rate": 0.0001298329156223893,
      "loss": 0.995,
      "step": 8400
    },
    {
      "epoch": 2.4590643274853803,
      "grad_norm": 0.748275101184845,
      "learning_rate": 0.00012974937343358396,
      "loss": 0.9514,
      "step": 8410
    },
    {
      "epoch": 2.461988304093567,
      "grad_norm": 0.6970164775848389,
      "learning_rate": 0.00012966583124477863,
      "loss": 0.9619,
      "step": 8420
    },
    {
      "epoch": 2.4649122807017543,
      "grad_norm": 0.7118269205093384,
      "learning_rate": 0.00012958228905597327,
      "loss": 0.9399,
      "step": 8430
    },
    {
      "epoch": 2.4678362573099415,
      "grad_norm": 0.8009153604507446,
      "learning_rate": 0.0001294987468671679,
      "loss": 0.9776,
      "step": 8440
    },
    {
      "epoch": 2.4707602339181287,
      "grad_norm": 0.7252700924873352,
      "learning_rate": 0.00012941520467836258,
      "loss": 1.0405,
      "step": 8450
    },
    {
      "epoch": 2.473684210526316,
      "grad_norm": 0.7493851184844971,
      "learning_rate": 0.00012933166248955724,
      "loss": 0.8957,
      "step": 8460
    },
    {
      "epoch": 2.476608187134503,
      "grad_norm": 0.8446624875068665,
      "learning_rate": 0.00012924812030075188,
      "loss": 0.9357,
      "step": 8470
    },
    {
      "epoch": 2.47953216374269,
      "grad_norm": 0.7914843559265137,
      "learning_rate": 0.00012916457811194652,
      "loss": 1.0437,
      "step": 8480
    },
    {
      "epoch": 2.482456140350877,
      "grad_norm": 0.8720637559890747,
      "learning_rate": 0.0001290810359231412,
      "loss": 1.0508,
      "step": 8490
    },
    {
      "epoch": 2.4853801169590644,
      "grad_norm": 0.8992743492126465,
      "learning_rate": 0.00012899749373433586,
      "loss": 0.9539,
      "step": 8500
    },
    {
      "epoch": 2.4883040935672516,
      "grad_norm": 0.7965790033340454,
      "learning_rate": 0.0001289139515455305,
      "loss": 0.9614,
      "step": 8510
    },
    {
      "epoch": 2.4912280701754383,
      "grad_norm": 1.0132074356079102,
      "learning_rate": 0.00012883040935672514,
      "loss": 1.0462,
      "step": 8520
    },
    {
      "epoch": 2.4941520467836256,
      "grad_norm": 0.7090038061141968,
      "learning_rate": 0.0001287468671679198,
      "loss": 0.9397,
      "step": 8530
    },
    {
      "epoch": 2.497076023391813,
      "grad_norm": 0.772222101688385,
      "learning_rate": 0.00012866332497911447,
      "loss": 0.9502,
      "step": 8540
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.671099066734314,
      "learning_rate": 0.0001285797827903091,
      "loss": 1.0169,
      "step": 8550
    },
    {
      "epoch": 2.502923976608187,
      "grad_norm": 0.7796770334243774,
      "learning_rate": 0.00012849624060150375,
      "loss": 0.9677,
      "step": 8560
    },
    {
      "epoch": 2.5058479532163744,
      "grad_norm": 0.7221289277076721,
      "learning_rate": 0.00012841269841269842,
      "loss": 0.9056,
      "step": 8570
    },
    {
      "epoch": 2.5087719298245617,
      "grad_norm": 0.935187041759491,
      "learning_rate": 0.00012832915622389309,
      "loss": 0.9787,
      "step": 8580
    },
    {
      "epoch": 2.5116959064327484,
      "grad_norm": 0.8940660357475281,
      "learning_rate": 0.00012824561403508773,
      "loss": 0.932,
      "step": 8590
    },
    {
      "epoch": 2.5146198830409356,
      "grad_norm": 0.730470597743988,
      "learning_rate": 0.00012816207184628237,
      "loss": 0.989,
      "step": 8600
    },
    {
      "epoch": 2.517543859649123,
      "grad_norm": 0.7625110745429993,
      "learning_rate": 0.00012807852965747703,
      "loss": 0.8844,
      "step": 8610
    },
    {
      "epoch": 2.52046783625731,
      "grad_norm": 0.7530778646469116,
      "learning_rate": 0.0001279949874686717,
      "loss": 1.0267,
      "step": 8620
    },
    {
      "epoch": 2.523391812865497,
      "grad_norm": 0.6979343295097351,
      "learning_rate": 0.00012791144527986634,
      "loss": 0.8815,
      "step": 8630
    },
    {
      "epoch": 2.526315789473684,
      "grad_norm": 0.7363889813423157,
      "learning_rate": 0.00012782790309106098,
      "loss": 1.0277,
      "step": 8640
    },
    {
      "epoch": 2.5292397660818713,
      "grad_norm": 0.7659358382225037,
      "learning_rate": 0.00012774436090225565,
      "loss": 1.0018,
      "step": 8650
    },
    {
      "epoch": 2.5321637426900585,
      "grad_norm": 0.8748064637184143,
      "learning_rate": 0.00012766081871345031,
      "loss": 0.9447,
      "step": 8660
    },
    {
      "epoch": 2.5350877192982457,
      "grad_norm": 0.8520611524581909,
      "learning_rate": 0.00012757727652464495,
      "loss": 0.9655,
      "step": 8670
    },
    {
      "epoch": 2.538011695906433,
      "grad_norm": 0.8451917767524719,
      "learning_rate": 0.0001274937343358396,
      "loss": 0.9323,
      "step": 8680
    },
    {
      "epoch": 2.54093567251462,
      "grad_norm": 1.1998094320297241,
      "learning_rate": 0.00012741019214703426,
      "loss": 0.9638,
      "step": 8690
    },
    {
      "epoch": 2.543859649122807,
      "grad_norm": 0.7850636839866638,
      "learning_rate": 0.00012732664995822893,
      "loss": 0.9848,
      "step": 8700
    },
    {
      "epoch": 2.546783625730994,
      "grad_norm": 0.7682269215583801,
      "learning_rate": 0.00012724310776942357,
      "loss": 0.9422,
      "step": 8710
    },
    {
      "epoch": 2.5497076023391814,
      "grad_norm": 0.7904132604598999,
      "learning_rate": 0.0001271595655806182,
      "loss": 0.9001,
      "step": 8720
    },
    {
      "epoch": 2.5526315789473686,
      "grad_norm": 0.86562579870224,
      "learning_rate": 0.00012707602339181287,
      "loss": 0.8695,
      "step": 8730
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.825471043586731,
      "learning_rate": 0.00012699248120300754,
      "loss": 0.93,
      "step": 8740
    },
    {
      "epoch": 2.5584795321637426,
      "grad_norm": 0.7891761660575867,
      "learning_rate": 0.00012690893901420218,
      "loss": 0.9947,
      "step": 8750
    },
    {
      "epoch": 2.56140350877193,
      "grad_norm": 0.7280564308166504,
      "learning_rate": 0.00012682539682539682,
      "loss": 0.9161,
      "step": 8760
    },
    {
      "epoch": 2.564327485380117,
      "grad_norm": 1.0038259029388428,
      "learning_rate": 0.0001267418546365915,
      "loss": 0.9133,
      "step": 8770
    },
    {
      "epoch": 2.5672514619883042,
      "grad_norm": 0.9360396862030029,
      "learning_rate": 0.00012665831244778616,
      "loss": 1.0033,
      "step": 8780
    },
    {
      "epoch": 2.5701754385964914,
      "grad_norm": 0.8412564992904663,
      "learning_rate": 0.0001265747702589808,
      "loss": 0.9595,
      "step": 8790
    },
    {
      "epoch": 2.573099415204678,
      "grad_norm": 0.7474446296691895,
      "learning_rate": 0.00012649122807017544,
      "loss": 0.9457,
      "step": 8800
    },
    {
      "epoch": 2.5760233918128654,
      "grad_norm": 0.9159872531890869,
      "learning_rate": 0.0001264076858813701,
      "loss": 0.9677,
      "step": 8810
    },
    {
      "epoch": 2.5789473684210527,
      "grad_norm": 1.1841014623641968,
      "learning_rate": 0.00012632414369256474,
      "loss": 0.9161,
      "step": 8820
    },
    {
      "epoch": 2.58187134502924,
      "grad_norm": 0.6611688137054443,
      "learning_rate": 0.0001262406015037594,
      "loss": 0.9848,
      "step": 8830
    },
    {
      "epoch": 2.5847953216374266,
      "grad_norm": 0.8381764888763428,
      "learning_rate": 0.00012615705931495405,
      "loss": 1.038,
      "step": 8840
    },
    {
      "epoch": 2.587719298245614,
      "grad_norm": 0.6964812874794006,
      "learning_rate": 0.00012607351712614872,
      "loss": 1.0118,
      "step": 8850
    },
    {
      "epoch": 2.590643274853801,
      "grad_norm": 0.9338375926017761,
      "learning_rate": 0.00012598997493734336,
      "loss": 0.88,
      "step": 8860
    },
    {
      "epoch": 2.5935672514619883,
      "grad_norm": 0.9868183732032776,
      "learning_rate": 0.00012590643274853802,
      "loss": 0.9726,
      "step": 8870
    },
    {
      "epoch": 2.5964912280701755,
      "grad_norm": 0.8224624991416931,
      "learning_rate": 0.00012582289055973266,
      "loss": 0.9414,
      "step": 8880
    },
    {
      "epoch": 2.5994152046783627,
      "grad_norm": 0.8258638381958008,
      "learning_rate": 0.00012573934837092733,
      "loss": 0.9087,
      "step": 8890
    },
    {
      "epoch": 2.60233918128655,
      "grad_norm": 0.9833992123603821,
      "learning_rate": 0.00012565580618212197,
      "loss": 1.026,
      "step": 8900
    },
    {
      "epoch": 2.6052631578947367,
      "grad_norm": 0.9046753644943237,
      "learning_rate": 0.00012557226399331664,
      "loss": 0.9826,
      "step": 8910
    },
    {
      "epoch": 2.608187134502924,
      "grad_norm": 0.8385810256004333,
      "learning_rate": 0.00012548872180451128,
      "loss": 1.0347,
      "step": 8920
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 0.6733437776565552,
      "learning_rate": 0.00012540517961570594,
      "loss": 0.8624,
      "step": 8930
    },
    {
      "epoch": 2.6140350877192984,
      "grad_norm": 1.050733208656311,
      "learning_rate": 0.00012532163742690058,
      "loss": 1.0483,
      "step": 8940
    },
    {
      "epoch": 2.616959064327485,
      "grad_norm": 0.8435072302818298,
      "learning_rate": 0.00012523809523809525,
      "loss": 0.9824,
      "step": 8950
    },
    {
      "epoch": 2.6198830409356724,
      "grad_norm": 0.8639011383056641,
      "learning_rate": 0.0001251545530492899,
      "loss": 0.9297,
      "step": 8960
    },
    {
      "epoch": 2.6228070175438596,
      "grad_norm": 0.7795498371124268,
      "learning_rate": 0.00012507101086048456,
      "loss": 0.9798,
      "step": 8970
    },
    {
      "epoch": 2.625730994152047,
      "grad_norm": 0.9254611730575562,
      "learning_rate": 0.0001249874686716792,
      "loss": 1.002,
      "step": 8980
    },
    {
      "epoch": 2.628654970760234,
      "grad_norm": 0.9281430244445801,
      "learning_rate": 0.00012490392648287387,
      "loss": 1.0441,
      "step": 8990
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 0.9116429090499878,
      "learning_rate": 0.0001248203842940685,
      "loss": 0.9416,
      "step": 9000
    },
    {
      "epoch": 2.6345029239766085,
      "grad_norm": 0.764339029788971,
      "learning_rate": 0.00012473684210526317,
      "loss": 0.9405,
      "step": 9010
    },
    {
      "epoch": 2.6374269005847952,
      "grad_norm": 0.7849200367927551,
      "learning_rate": 0.0001246532999164578,
      "loss": 0.9462,
      "step": 9020
    },
    {
      "epoch": 2.6403508771929824,
      "grad_norm": 0.9406718611717224,
      "learning_rate": 0.00012456975772765248,
      "loss": 0.9446,
      "step": 9030
    },
    {
      "epoch": 2.6432748538011697,
      "grad_norm": 0.8950589299201965,
      "learning_rate": 0.00012448621553884712,
      "loss": 0.9982,
      "step": 9040
    },
    {
      "epoch": 2.646198830409357,
      "grad_norm": 0.9562406539916992,
      "learning_rate": 0.0001244026733500418,
      "loss": 1.0123,
      "step": 9050
    },
    {
      "epoch": 2.6491228070175437,
      "grad_norm": 0.6569317579269409,
      "learning_rate": 0.00012431913116123643,
      "loss": 0.9025,
      "step": 9060
    },
    {
      "epoch": 2.652046783625731,
      "grad_norm": 0.8448731303215027,
      "learning_rate": 0.0001242355889724311,
      "loss": 0.9825,
      "step": 9070
    },
    {
      "epoch": 2.654970760233918,
      "grad_norm": 1.0724436044692993,
      "learning_rate": 0.00012415204678362573,
      "loss": 1.0171,
      "step": 9080
    },
    {
      "epoch": 2.6578947368421053,
      "grad_norm": 0.7341883182525635,
      "learning_rate": 0.0001240685045948204,
      "loss": 0.9401,
      "step": 9090
    },
    {
      "epoch": 2.6608187134502925,
      "grad_norm": 0.7295026779174805,
      "learning_rate": 0.00012398496240601504,
      "loss": 0.8822,
      "step": 9100
    },
    {
      "epoch": 2.6637426900584797,
      "grad_norm": 0.792858898639679,
      "learning_rate": 0.0001239014202172097,
      "loss": 0.8695,
      "step": 9110
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.690966010093689,
      "learning_rate": 0.00012381787802840435,
      "loss": 1.0209,
      "step": 9120
    },
    {
      "epoch": 2.6695906432748537,
      "grad_norm": 0.9781247973442078,
      "learning_rate": 0.00012373433583959901,
      "loss": 0.9836,
      "step": 9130
    },
    {
      "epoch": 2.672514619883041,
      "grad_norm": 0.6545352935791016,
      "learning_rate": 0.00012365079365079365,
      "loss": 0.9057,
      "step": 9140
    },
    {
      "epoch": 2.675438596491228,
      "grad_norm": 0.7876794338226318,
      "learning_rate": 0.00012356725146198832,
      "loss": 1.0287,
      "step": 9150
    },
    {
      "epoch": 2.678362573099415,
      "grad_norm": 1.0505369901657104,
      "learning_rate": 0.00012348370927318296,
      "loss": 1.0323,
      "step": 9160
    },
    {
      "epoch": 2.681286549707602,
      "grad_norm": 0.9333749413490295,
      "learning_rate": 0.00012340016708437763,
      "loss": 1.0158,
      "step": 9170
    },
    {
      "epoch": 2.6842105263157894,
      "grad_norm": 0.8518600463867188,
      "learning_rate": 0.00012331662489557227,
      "loss": 0.9854,
      "step": 9180
    },
    {
      "epoch": 2.6871345029239766,
      "grad_norm": 0.8926156163215637,
      "learning_rate": 0.00012323308270676694,
      "loss": 0.9885,
      "step": 9190
    },
    {
      "epoch": 2.690058479532164,
      "grad_norm": 0.8772556185722351,
      "learning_rate": 0.00012314954051796158,
      "loss": 1.0214,
      "step": 9200
    },
    {
      "epoch": 2.692982456140351,
      "grad_norm": 0.6737682223320007,
      "learning_rate": 0.00012306599832915624,
      "loss": 0.9477,
      "step": 9210
    },
    {
      "epoch": 2.6959064327485383,
      "grad_norm": 0.9380785822868347,
      "learning_rate": 0.00012298245614035088,
      "loss": 0.9496,
      "step": 9220
    },
    {
      "epoch": 2.698830409356725,
      "grad_norm": 0.6703010201454163,
      "learning_rate": 0.00012289891395154552,
      "loss": 1.0355,
      "step": 9230
    },
    {
      "epoch": 2.7017543859649122,
      "grad_norm": 0.7104382514953613,
      "learning_rate": 0.0001228153717627402,
      "loss": 0.8818,
      "step": 9240
    },
    {
      "epoch": 2.7046783625730995,
      "grad_norm": 0.6690494418144226,
      "learning_rate": 0.00012273182957393486,
      "loss": 0.9419,
      "step": 9250
    },
    {
      "epoch": 2.7076023391812867,
      "grad_norm": 0.7538533210754395,
      "learning_rate": 0.0001226482873851295,
      "loss": 0.9871,
      "step": 9260
    },
    {
      "epoch": 2.7105263157894735,
      "grad_norm": 0.9763433337211609,
      "learning_rate": 0.00012256474519632414,
      "loss": 1.1,
      "step": 9270
    },
    {
      "epoch": 2.7134502923976607,
      "grad_norm": 0.9023247957229614,
      "learning_rate": 0.0001224812030075188,
      "loss": 1.0013,
      "step": 9280
    },
    {
      "epoch": 2.716374269005848,
      "grad_norm": 0.991348147392273,
      "learning_rate": 0.00012239766081871347,
      "loss": 0.9747,
      "step": 9290
    },
    {
      "epoch": 2.719298245614035,
      "grad_norm": 0.9355800747871399,
      "learning_rate": 0.0001223141186299081,
      "loss": 0.9934,
      "step": 9300
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 0.8984585404396057,
      "learning_rate": 0.00012223057644110275,
      "loss": 1.1611,
      "step": 9310
    },
    {
      "epoch": 2.7251461988304095,
      "grad_norm": 1.1239984035491943,
      "learning_rate": 0.00012214703425229742,
      "loss": 0.9712,
      "step": 9320
    },
    {
      "epoch": 2.7280701754385968,
      "grad_norm": 0.9705925583839417,
      "learning_rate": 0.00012206349206349208,
      "loss": 0.996,
      "step": 9330
    },
    {
      "epoch": 2.7309941520467835,
      "grad_norm": 0.9541921615600586,
      "learning_rate": 0.00012197994987468672,
      "loss": 1.0697,
      "step": 9340
    },
    {
      "epoch": 2.7339181286549707,
      "grad_norm": 0.9995162487030029,
      "learning_rate": 0.00012189640768588138,
      "loss": 1.0549,
      "step": 9350
    },
    {
      "epoch": 2.736842105263158,
      "grad_norm": 0.7623310089111328,
      "learning_rate": 0.00012181286549707602,
      "loss": 0.9517,
      "step": 9360
    },
    {
      "epoch": 2.739766081871345,
      "grad_norm": 0.6602184176445007,
      "learning_rate": 0.0001217293233082707,
      "loss": 0.9497,
      "step": 9370
    },
    {
      "epoch": 2.742690058479532,
      "grad_norm": 0.8239392638206482,
      "learning_rate": 0.00012164578111946534,
      "loss": 0.9492,
      "step": 9380
    },
    {
      "epoch": 2.745614035087719,
      "grad_norm": 0.8185771703720093,
      "learning_rate": 0.00012156223893065999,
      "loss": 0.9088,
      "step": 9390
    },
    {
      "epoch": 2.7485380116959064,
      "grad_norm": 0.8561277389526367,
      "learning_rate": 0.00012147869674185463,
      "loss": 1.0577,
      "step": 9400
    },
    {
      "epoch": 2.7514619883040936,
      "grad_norm": 0.7020263671875,
      "learning_rate": 0.00012139515455304931,
      "loss": 0.9506,
      "step": 9410
    },
    {
      "epoch": 2.754385964912281,
      "grad_norm": 0.7394257187843323,
      "learning_rate": 0.00012131161236424395,
      "loss": 1.0416,
      "step": 9420
    },
    {
      "epoch": 2.757309941520468,
      "grad_norm": 0.7509098649024963,
      "learning_rate": 0.0001212280701754386,
      "loss": 0.9725,
      "step": 9430
    },
    {
      "epoch": 2.760233918128655,
      "grad_norm": 0.8549236059188843,
      "learning_rate": 0.00012114452798663325,
      "loss": 0.9455,
      "step": 9440
    },
    {
      "epoch": 2.763157894736842,
      "grad_norm": 0.6985024809837341,
      "learning_rate": 0.00012106098579782791,
      "loss": 1.0851,
      "step": 9450
    },
    {
      "epoch": 2.7660818713450293,
      "grad_norm": 0.8187515735626221,
      "learning_rate": 0.00012097744360902257,
      "loss": 0.9991,
      "step": 9460
    },
    {
      "epoch": 2.7690058479532165,
      "grad_norm": 0.8045031428337097,
      "learning_rate": 0.00012089390142021722,
      "loss": 0.936,
      "step": 9470
    },
    {
      "epoch": 2.7719298245614032,
      "grad_norm": 0.9378985166549683,
      "learning_rate": 0.00012081035923141186,
      "loss": 0.9388,
      "step": 9480
    },
    {
      "epoch": 2.7748538011695905,
      "grad_norm": 1.2257816791534424,
      "learning_rate": 0.00012072681704260653,
      "loss": 1.0255,
      "step": 9490
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.7537302374839783,
      "learning_rate": 0.00012064327485380118,
      "loss": 0.9156,
      "step": 9500
    },
    {
      "epoch": 2.780701754385965,
      "grad_norm": 0.7594866156578064,
      "learning_rate": 0.00012055973266499583,
      "loss": 0.8885,
      "step": 9510
    },
    {
      "epoch": 2.783625730994152,
      "grad_norm": 0.5588944554328918,
      "learning_rate": 0.00012047619047619047,
      "loss": 0.9866,
      "step": 9520
    },
    {
      "epoch": 2.7865497076023393,
      "grad_norm": 0.807692289352417,
      "learning_rate": 0.00012039264828738514,
      "loss": 0.9959,
      "step": 9530
    },
    {
      "epoch": 2.7894736842105265,
      "grad_norm": 0.8129580616950989,
      "learning_rate": 0.0001203091060985798,
      "loss": 1.1036,
      "step": 9540
    },
    {
      "epoch": 2.7923976608187133,
      "grad_norm": 0.7731061577796936,
      "learning_rate": 0.00012022556390977443,
      "loss": 1.067,
      "step": 9550
    },
    {
      "epoch": 2.7953216374269005,
      "grad_norm": 1.0240483283996582,
      "learning_rate": 0.00012014202172096909,
      "loss": 0.9618,
      "step": 9560
    },
    {
      "epoch": 2.7982456140350878,
      "grad_norm": 0.7408033609390259,
      "learning_rate": 0.00012005847953216376,
      "loss": 0.9147,
      "step": 9570
    },
    {
      "epoch": 2.801169590643275,
      "grad_norm": 0.727760910987854,
      "learning_rate": 0.00011997493734335841,
      "loss": 0.9809,
      "step": 9580
    },
    {
      "epoch": 2.8040935672514617,
      "grad_norm": 0.7445927858352661,
      "learning_rate": 0.00011989139515455305,
      "loss": 1.0179,
      "step": 9590
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 0.6500542759895325,
      "learning_rate": 0.0001198078529657477,
      "loss": 0.856,
      "step": 9600
    },
    {
      "epoch": 2.809941520467836,
      "grad_norm": 0.8352130055427551,
      "learning_rate": 0.00011972431077694237,
      "loss": 0.9671,
      "step": 9610
    },
    {
      "epoch": 2.8128654970760234,
      "grad_norm": 0.9132212400436401,
      "learning_rate": 0.00011964076858813702,
      "loss": 0.9324,
      "step": 9620
    },
    {
      "epoch": 2.8157894736842106,
      "grad_norm": 0.6801409721374512,
      "learning_rate": 0.00011955722639933166,
      "loss": 0.954,
      "step": 9630
    },
    {
      "epoch": 2.818713450292398,
      "grad_norm": 0.6743291616439819,
      "learning_rate": 0.00011947368421052632,
      "loss": 0.9254,
      "step": 9640
    },
    {
      "epoch": 2.821637426900585,
      "grad_norm": 0.7799340486526489,
      "learning_rate": 0.00011939014202172098,
      "loss": 0.9613,
      "step": 9650
    },
    {
      "epoch": 2.824561403508772,
      "grad_norm": 0.8168100714683533,
      "learning_rate": 0.00011930659983291564,
      "loss": 1.0145,
      "step": 9660
    },
    {
      "epoch": 2.827485380116959,
      "grad_norm": 0.9671028852462769,
      "learning_rate": 0.00011922305764411028,
      "loss": 1.0606,
      "step": 9670
    },
    {
      "epoch": 2.8304093567251463,
      "grad_norm": 0.864289402961731,
      "learning_rate": 0.00011913951545530493,
      "loss": 0.9764,
      "step": 9680
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.9472163319587708,
      "learning_rate": 0.0001190559732664996,
      "loss": 0.9667,
      "step": 9690
    },
    {
      "epoch": 2.8362573099415203,
      "grad_norm": 0.875649094581604,
      "learning_rate": 0.00011897243107769425,
      "loss": 0.9041,
      "step": 9700
    },
    {
      "epoch": 2.8391812865497075,
      "grad_norm": 0.7109161019325256,
      "learning_rate": 0.00011888888888888889,
      "loss": 0.9511,
      "step": 9710
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 0.8159549236297607,
      "learning_rate": 0.00011880534670008354,
      "loss": 0.9345,
      "step": 9720
    },
    {
      "epoch": 2.845029239766082,
      "grad_norm": 0.6803777813911438,
      "learning_rate": 0.00011872180451127821,
      "loss": 0.9481,
      "step": 9730
    },
    {
      "epoch": 2.847953216374269,
      "grad_norm": 0.8253275752067566,
      "learning_rate": 0.00011863826232247286,
      "loss": 0.9513,
      "step": 9740
    },
    {
      "epoch": 2.8508771929824563,
      "grad_norm": 0.9019206762313843,
      "learning_rate": 0.0001185547201336675,
      "loss": 1.044,
      "step": 9750
    },
    {
      "epoch": 2.853801169590643,
      "grad_norm": 0.7654926180839539,
      "learning_rate": 0.00011847117794486216,
      "loss": 0.916,
      "step": 9760
    },
    {
      "epoch": 2.8567251461988303,
      "grad_norm": 1.0203596353530884,
      "learning_rate": 0.00011838763575605683,
      "loss": 0.9743,
      "step": 9770
    },
    {
      "epoch": 2.8596491228070176,
      "grad_norm": 0.7049197554588318,
      "learning_rate": 0.00011830409356725148,
      "loss": 0.9633,
      "step": 9780
    },
    {
      "epoch": 2.8625730994152048,
      "grad_norm": 0.8346469402313232,
      "learning_rate": 0.00011822055137844612,
      "loss": 1.0088,
      "step": 9790
    },
    {
      "epoch": 2.8654970760233915,
      "grad_norm": 0.8785699009895325,
      "learning_rate": 0.00011813700918964077,
      "loss": 0.9816,
      "step": 9800
    },
    {
      "epoch": 2.8684210526315788,
      "grad_norm": 0.8301141858100891,
      "learning_rate": 0.00011805346700083544,
      "loss": 0.9402,
      "step": 9810
    },
    {
      "epoch": 2.871345029239766,
      "grad_norm": 0.7101846933364868,
      "learning_rate": 0.00011796992481203009,
      "loss": 0.8918,
      "step": 9820
    },
    {
      "epoch": 2.874269005847953,
      "grad_norm": 0.7707456350326538,
      "learning_rate": 0.00011788638262322473,
      "loss": 0.8851,
      "step": 9830
    },
    {
      "epoch": 2.8771929824561404,
      "grad_norm": 0.7179772257804871,
      "learning_rate": 0.00011780284043441939,
      "loss": 0.961,
      "step": 9840
    },
    {
      "epoch": 2.8801169590643276,
      "grad_norm": 0.5908941030502319,
      "learning_rate": 0.00011771929824561405,
      "loss": 0.9586,
      "step": 9850
    },
    {
      "epoch": 2.883040935672515,
      "grad_norm": 0.8445120453834534,
      "learning_rate": 0.00011763575605680869,
      "loss": 1.0253,
      "step": 9860
    },
    {
      "epoch": 2.8859649122807016,
      "grad_norm": 0.6929834485054016,
      "learning_rate": 0.00011755221386800335,
      "loss": 0.9824,
      "step": 9870
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.7850040793418884,
      "learning_rate": 0.000117468671679198,
      "loss": 0.9203,
      "step": 9880
    },
    {
      "epoch": 2.891812865497076,
      "grad_norm": 0.8467140197753906,
      "learning_rate": 0.00011738512949039267,
      "loss": 1.0084,
      "step": 9890
    },
    {
      "epoch": 2.8947368421052633,
      "grad_norm": 0.73317551612854,
      "learning_rate": 0.00011730158730158731,
      "loss": 0.9155,
      "step": 9900
    },
    {
      "epoch": 2.89766081871345,
      "grad_norm": 0.7638460397720337,
      "learning_rate": 0.00011721804511278196,
      "loss": 0.951,
      "step": 9910
    },
    {
      "epoch": 2.9005847953216373,
      "grad_norm": 0.6796032190322876,
      "learning_rate": 0.00011713450292397661,
      "loss": 0.8786,
      "step": 9920
    },
    {
      "epoch": 2.9035087719298245,
      "grad_norm": 0.8510611057281494,
      "learning_rate": 0.00011705096073517128,
      "loss": 1.0024,
      "step": 9930
    },
    {
      "epoch": 2.9064327485380117,
      "grad_norm": 1.1711267232894897,
      "learning_rate": 0.00011696741854636592,
      "loss": 0.958,
      "step": 9940
    },
    {
      "epoch": 2.909356725146199,
      "grad_norm": 0.896590530872345,
      "learning_rate": 0.00011688387635756057,
      "loss": 1.0705,
      "step": 9950
    },
    {
      "epoch": 2.912280701754386,
      "grad_norm": 0.7881660461425781,
      "learning_rate": 0.00011680033416875521,
      "loss": 0.9211,
      "step": 9960
    },
    {
      "epoch": 2.9152046783625734,
      "grad_norm": 0.7821551561355591,
      "learning_rate": 0.0001167167919799499,
      "loss": 1.0129,
      "step": 9970
    },
    {
      "epoch": 2.91812865497076,
      "grad_norm": 0.7375504970550537,
      "learning_rate": 0.00011663324979114454,
      "loss": 0.8557,
      "step": 9980
    },
    {
      "epoch": 2.9210526315789473,
      "grad_norm": 0.708835780620575,
      "learning_rate": 0.00011654970760233919,
      "loss": 0.9014,
      "step": 9990
    },
    {
      "epoch": 2.9239766081871346,
      "grad_norm": 0.6323844790458679,
      "learning_rate": 0.00011646616541353383,
      "loss": 0.9984,
      "step": 10000
    },
    {
      "epoch": 2.926900584795322,
      "grad_norm": 1.0429555177688599,
      "learning_rate": 0.00011638262322472851,
      "loss": 0.9608,
      "step": 10010
    },
    {
      "epoch": 2.9298245614035086,
      "grad_norm": 0.6977607011795044,
      "learning_rate": 0.00011629908103592315,
      "loss": 1.0223,
      "step": 10020
    },
    {
      "epoch": 2.9327485380116958,
      "grad_norm": 0.7168135643005371,
      "learning_rate": 0.0001162155388471178,
      "loss": 0.9317,
      "step": 10030
    },
    {
      "epoch": 2.935672514619883,
      "grad_norm": 1.1087888479232788,
      "learning_rate": 0.00011613199665831244,
      "loss": 0.8849,
      "step": 10040
    },
    {
      "epoch": 2.93859649122807,
      "grad_norm": 0.9062398672103882,
      "learning_rate": 0.00011604845446950712,
      "loss": 0.957,
      "step": 10050
    },
    {
      "epoch": 2.9415204678362574,
      "grad_norm": 0.7554019689559937,
      "learning_rate": 0.00011596491228070176,
      "loss": 0.9659,
      "step": 10060
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 0.8470156788825989,
      "learning_rate": 0.00011588137009189642,
      "loss": 0.874,
      "step": 10070
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 0.786718487739563,
      "learning_rate": 0.00011579782790309106,
      "loss": 0.9071,
      "step": 10080
    },
    {
      "epoch": 2.9502923976608186,
      "grad_norm": 0.6303544044494629,
      "learning_rate": 0.00011571428571428574,
      "loss": 0.9432,
      "step": 10090
    },
    {
      "epoch": 2.953216374269006,
      "grad_norm": 0.8259913325309753,
      "learning_rate": 0.00011563074352548038,
      "loss": 0.9532,
      "step": 10100
    },
    {
      "epoch": 2.956140350877193,
      "grad_norm": 0.8086109161376953,
      "learning_rate": 0.00011554720133667503,
      "loss": 0.9093,
      "step": 10110
    },
    {
      "epoch": 2.95906432748538,
      "grad_norm": 0.93733149766922,
      "learning_rate": 0.00011546365914786967,
      "loss": 0.9823,
      "step": 10120
    },
    {
      "epoch": 2.961988304093567,
      "grad_norm": 1.0848575830459595,
      "learning_rate": 0.00011538011695906434,
      "loss": 1.0063,
      "step": 10130
    },
    {
      "epoch": 2.9649122807017543,
      "grad_norm": 0.9822468161582947,
      "learning_rate": 0.00011529657477025899,
      "loss": 0.9245,
      "step": 10140
    },
    {
      "epoch": 2.9678362573099415,
      "grad_norm": 0.75648033618927,
      "learning_rate": 0.00011521303258145364,
      "loss": 0.8697,
      "step": 10150
    },
    {
      "epoch": 2.9707602339181287,
      "grad_norm": 0.7348679304122925,
      "learning_rate": 0.00011512949039264828,
      "loss": 1.0081,
      "step": 10160
    },
    {
      "epoch": 2.973684210526316,
      "grad_norm": 0.8090600967407227,
      "learning_rate": 0.00011504594820384295,
      "loss": 0.9363,
      "step": 10170
    },
    {
      "epoch": 2.976608187134503,
      "grad_norm": 0.80430006980896,
      "learning_rate": 0.0001149624060150376,
      "loss": 0.9593,
      "step": 10180
    },
    {
      "epoch": 2.97953216374269,
      "grad_norm": 0.8292717933654785,
      "learning_rate": 0.00011487886382623226,
      "loss": 0.978,
      "step": 10190
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 0.7563539743423462,
      "learning_rate": 0.0001147953216374269,
      "loss": 0.9466,
      "step": 10200
    },
    {
      "epoch": 2.9853801169590644,
      "grad_norm": 0.8389729261398315,
      "learning_rate": 0.00011471177944862157,
      "loss": 0.938,
      "step": 10210
    },
    {
      "epoch": 2.9883040935672516,
      "grad_norm": 0.7313776612281799,
      "learning_rate": 0.00011462823725981622,
      "loss": 0.9712,
      "step": 10220
    },
    {
      "epoch": 2.9912280701754383,
      "grad_norm": 0.7432543635368347,
      "learning_rate": 0.00011454469507101086,
      "loss": 0.9501,
      "step": 10230
    },
    {
      "epoch": 2.9941520467836256,
      "grad_norm": 0.6383310556411743,
      "learning_rate": 0.00011446115288220551,
      "loss": 1.0056,
      "step": 10240
    },
    {
      "epoch": 2.997076023391813,
      "grad_norm": 0.8999522924423218,
      "learning_rate": 0.00011437761069340018,
      "loss": 0.9646,
      "step": 10250
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.9176303744316101,
      "learning_rate": 0.00011429406850459483,
      "loss": 0.9407,
      "step": 10260
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.8756673336029053,
      "eval_runtime": 76.1777,
      "eval_samples_per_second": 11.67,
      "eval_steps_per_second": 1.47,
      "step": 10260
    },
    {
      "epoch": 3.002923976608187,
      "grad_norm": 0.6965935826301575,
      "learning_rate": 0.00011421052631578947,
      "loss": 0.9148,
      "step": 10270
    },
    {
      "epoch": 3.0058479532163744,
      "grad_norm": 0.6091078519821167,
      "learning_rate": 0.00011412698412698413,
      "loss": 0.8731,
      "step": 10280
    },
    {
      "epoch": 3.008771929824561,
      "grad_norm": 0.686223030090332,
      "learning_rate": 0.0001140434419381788,
      "loss": 0.8123,
      "step": 10290
    },
    {
      "epoch": 3.0116959064327484,
      "grad_norm": 0.7465790510177612,
      "learning_rate": 0.00011395989974937345,
      "loss": 0.9719,
      "step": 10300
    },
    {
      "epoch": 3.0146198830409356,
      "grad_norm": 0.9576109647750854,
      "learning_rate": 0.00011387635756056809,
      "loss": 0.9721,
      "step": 10310
    },
    {
      "epoch": 3.017543859649123,
      "grad_norm": 0.9623693227767944,
      "learning_rate": 0.00011379281537176274,
      "loss": 1.0074,
      "step": 10320
    },
    {
      "epoch": 3.02046783625731,
      "grad_norm": 0.7958047986030579,
      "learning_rate": 0.00011370927318295741,
      "loss": 0.8675,
      "step": 10330
    },
    {
      "epoch": 3.023391812865497,
      "grad_norm": 0.908292293548584,
      "learning_rate": 0.00011362573099415206,
      "loss": 0.8893,
      "step": 10340
    },
    {
      "epoch": 3.026315789473684,
      "grad_norm": 0.5864022374153137,
      "learning_rate": 0.0001135421888053467,
      "loss": 0.8662,
      "step": 10350
    },
    {
      "epoch": 3.0292397660818713,
      "grad_norm": 0.8526861667633057,
      "learning_rate": 0.00011345864661654135,
      "loss": 0.9279,
      "step": 10360
    },
    {
      "epoch": 3.0321637426900585,
      "grad_norm": 0.7450278997421265,
      "learning_rate": 0.00011337510442773602,
      "loss": 0.8862,
      "step": 10370
    },
    {
      "epoch": 3.0350877192982457,
      "grad_norm": 0.8199626207351685,
      "learning_rate": 0.00011329156223893068,
      "loss": 0.8659,
      "step": 10380
    },
    {
      "epoch": 3.038011695906433,
      "grad_norm": 0.702360212802887,
      "learning_rate": 0.00011320802005012531,
      "loss": 0.9319,
      "step": 10390
    },
    {
      "epoch": 3.0409356725146197,
      "grad_norm": 0.932608425617218,
      "learning_rate": 0.00011312447786131997,
      "loss": 0.9244,
      "step": 10400
    },
    {
      "epoch": 3.043859649122807,
      "grad_norm": 0.8209539651870728,
      "learning_rate": 0.00011304093567251464,
      "loss": 0.8909,
      "step": 10410
    },
    {
      "epoch": 3.046783625730994,
      "grad_norm": 0.6734448075294495,
      "learning_rate": 0.00011295739348370929,
      "loss": 0.9131,
      "step": 10420
    },
    {
      "epoch": 3.0497076023391814,
      "grad_norm": 0.8506901860237122,
      "learning_rate": 0.00011287385129490393,
      "loss": 0.8436,
      "step": 10430
    },
    {
      "epoch": 3.0526315789473686,
      "grad_norm": 0.6355741620063782,
      "learning_rate": 0.00011279030910609858,
      "loss": 0.9716,
      "step": 10440
    },
    {
      "epoch": 3.0555555555555554,
      "grad_norm": 0.8484413027763367,
      "learning_rate": 0.00011270676691729325,
      "loss": 0.8868,
      "step": 10450
    },
    {
      "epoch": 3.0584795321637426,
      "grad_norm": 0.8151317834854126,
      "learning_rate": 0.0001126232247284879,
      "loss": 0.9652,
      "step": 10460
    },
    {
      "epoch": 3.06140350877193,
      "grad_norm": 0.9031457901000977,
      "learning_rate": 0.00011253968253968254,
      "loss": 0.9437,
      "step": 10470
    },
    {
      "epoch": 3.064327485380117,
      "grad_norm": 0.7199203968048096,
      "learning_rate": 0.0001124561403508772,
      "loss": 0.9417,
      "step": 10480
    },
    {
      "epoch": 3.0672514619883042,
      "grad_norm": 0.6213617324829102,
      "learning_rate": 0.00011237259816207186,
      "loss": 0.9198,
      "step": 10490
    },
    {
      "epoch": 3.0701754385964914,
      "grad_norm": 0.8738766312599182,
      "learning_rate": 0.00011228905597326652,
      "loss": 1.2175,
      "step": 10500
    },
    {
      "epoch": 3.073099415204678,
      "grad_norm": 0.6288086175918579,
      "learning_rate": 0.00011220551378446116,
      "loss": 0.9551,
      "step": 10510
    },
    {
      "epoch": 3.0760233918128654,
      "grad_norm": 0.8236770033836365,
      "learning_rate": 0.00011212197159565581,
      "loss": 0.8062,
      "step": 10520
    },
    {
      "epoch": 3.0789473684210527,
      "grad_norm": 0.6741359233856201,
      "learning_rate": 0.00011203842940685048,
      "loss": 0.9953,
      "step": 10530
    },
    {
      "epoch": 3.08187134502924,
      "grad_norm": 0.8230187296867371,
      "learning_rate": 0.00011195488721804512,
      "loss": 1.0471,
      "step": 10540
    },
    {
      "epoch": 3.084795321637427,
      "grad_norm": 0.9784225821495056,
      "learning_rate": 0.00011187134502923977,
      "loss": 0.8164,
      "step": 10550
    },
    {
      "epoch": 3.087719298245614,
      "grad_norm": 0.7893361449241638,
      "learning_rate": 0.00011178780284043442,
      "loss": 0.9314,
      "step": 10560
    },
    {
      "epoch": 3.090643274853801,
      "grad_norm": 0.8773661255836487,
      "learning_rate": 0.00011170426065162909,
      "loss": 0.936,
      "step": 10570
    },
    {
      "epoch": 3.0935672514619883,
      "grad_norm": 0.7331729531288147,
      "learning_rate": 0.00011162071846282373,
      "loss": 0.9423,
      "step": 10580
    },
    {
      "epoch": 3.0964912280701755,
      "grad_norm": 0.8566672801971436,
      "learning_rate": 0.00011153717627401838,
      "loss": 0.8936,
      "step": 10590
    },
    {
      "epoch": 3.0994152046783627,
      "grad_norm": 0.9557786583900452,
      "learning_rate": 0.00011145363408521304,
      "loss": 0.8775,
      "step": 10600
    },
    {
      "epoch": 3.1023391812865495,
      "grad_norm": 0.8463602662086487,
      "learning_rate": 0.0001113700918964077,
      "loss": 0.916,
      "step": 10610
    },
    {
      "epoch": 3.1052631578947367,
      "grad_norm": 0.902962327003479,
      "learning_rate": 0.00011128654970760235,
      "loss": 0.9241,
      "step": 10620
    },
    {
      "epoch": 3.108187134502924,
      "grad_norm": 0.7979579567909241,
      "learning_rate": 0.000111203007518797,
      "loss": 0.9438,
      "step": 10630
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 1.0161207914352417,
      "learning_rate": 0.00011111946532999164,
      "loss": 0.8343,
      "step": 10640
    },
    {
      "epoch": 3.1140350877192984,
      "grad_norm": 0.7623852491378784,
      "learning_rate": 0.00011103592314118632,
      "loss": 0.9605,
      "step": 10650
    },
    {
      "epoch": 3.116959064327485,
      "grad_norm": 0.8570054769515991,
      "learning_rate": 0.00011095238095238096,
      "loss": 0.9112,
      "step": 10660
    },
    {
      "epoch": 3.1198830409356724,
      "grad_norm": 0.7946211695671082,
      "learning_rate": 0.00011086883876357561,
      "loss": 0.7878,
      "step": 10670
    },
    {
      "epoch": 3.1228070175438596,
      "grad_norm": 0.7146732807159424,
      "learning_rate": 0.00011078529657477025,
      "loss": 0.9116,
      "step": 10680
    },
    {
      "epoch": 3.125730994152047,
      "grad_norm": 0.7857660055160522,
      "learning_rate": 0.00011070175438596493,
      "loss": 0.8909,
      "step": 10690
    },
    {
      "epoch": 3.128654970760234,
      "grad_norm": 0.881761372089386,
      "learning_rate": 0.00011061821219715957,
      "loss": 0.9271,
      "step": 10700
    },
    {
      "epoch": 3.1315789473684212,
      "grad_norm": 1.0876250267028809,
      "learning_rate": 0.00011053467000835423,
      "loss": 0.9669,
      "step": 10710
    },
    {
      "epoch": 3.134502923976608,
      "grad_norm": 0.8181765675544739,
      "learning_rate": 0.00011045112781954887,
      "loss": 0.9241,
      "step": 10720
    },
    {
      "epoch": 3.1374269005847952,
      "grad_norm": 0.799318790435791,
      "learning_rate": 0.00011036758563074355,
      "loss": 0.8895,
      "step": 10730
    },
    {
      "epoch": 3.1403508771929824,
      "grad_norm": 0.7185671329498291,
      "learning_rate": 0.00011028404344193819,
      "loss": 0.875,
      "step": 10740
    },
    {
      "epoch": 3.1432748538011697,
      "grad_norm": 0.9120291471481323,
      "learning_rate": 0.00011020050125313284,
      "loss": 0.8479,
      "step": 10750
    },
    {
      "epoch": 3.146198830409357,
      "grad_norm": 0.6921749711036682,
      "learning_rate": 0.00011011695906432748,
      "loss": 0.89,
      "step": 10760
    },
    {
      "epoch": 3.1491228070175437,
      "grad_norm": 0.9646793007850647,
      "learning_rate": 0.00011003341687552216,
      "loss": 0.9482,
      "step": 10770
    },
    {
      "epoch": 3.152046783625731,
      "grad_norm": 0.9349635243415833,
      "learning_rate": 0.0001099498746867168,
      "loss": 0.8621,
      "step": 10780
    },
    {
      "epoch": 3.154970760233918,
      "grad_norm": 0.7488329410552979,
      "learning_rate": 0.00010986633249791145,
      "loss": 0.9029,
      "step": 10790
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 0.8124001026153564,
      "learning_rate": 0.0001097827903091061,
      "loss": 0.9198,
      "step": 10800
    },
    {
      "epoch": 3.1608187134502925,
      "grad_norm": 0.7906988859176636,
      "learning_rate": 0.00010969924812030075,
      "loss": 0.8697,
      "step": 10810
    },
    {
      "epoch": 3.1637426900584797,
      "grad_norm": 0.8875930905342102,
      "learning_rate": 0.00010961570593149542,
      "loss": 0.9952,
      "step": 10820
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.7855125665664673,
      "learning_rate": 0.00010953216374269007,
      "loss": 0.9345,
      "step": 10830
    },
    {
      "epoch": 3.1695906432748537,
      "grad_norm": 0.8168303370475769,
      "learning_rate": 0.00010944862155388471,
      "loss": 0.8752,
      "step": 10840
    },
    {
      "epoch": 3.172514619883041,
      "grad_norm": 0.7662606239318848,
      "learning_rate": 0.00010936507936507936,
      "loss": 0.9391,
      "step": 10850
    },
    {
      "epoch": 3.175438596491228,
      "grad_norm": 0.8816714882850647,
      "learning_rate": 0.00010928153717627403,
      "loss": 0.8984,
      "step": 10860
    },
    {
      "epoch": 3.1783625730994154,
      "grad_norm": 0.6687739491462708,
      "learning_rate": 0.00010919799498746868,
      "loss": 0.8964,
      "step": 10870
    },
    {
      "epoch": 3.181286549707602,
      "grad_norm": 0.9157571792602539,
      "learning_rate": 0.00010911445279866332,
      "loss": 0.9059,
      "step": 10880
    },
    {
      "epoch": 3.1842105263157894,
      "grad_norm": 0.7030515670776367,
      "learning_rate": 0.00010903091060985798,
      "loss": 0.8492,
      "step": 10890
    },
    {
      "epoch": 3.1871345029239766,
      "grad_norm": 0.9913464188575745,
      "learning_rate": 0.00010894736842105264,
      "loss": 0.9371,
      "step": 10900
    },
    {
      "epoch": 3.190058479532164,
      "grad_norm": 0.7962360382080078,
      "learning_rate": 0.0001088638262322473,
      "loss": 0.841,
      "step": 10910
    },
    {
      "epoch": 3.192982456140351,
      "grad_norm": 0.7078633308410645,
      "learning_rate": 0.00010878028404344194,
      "loss": 0.9683,
      "step": 10920
    },
    {
      "epoch": 3.195906432748538,
      "grad_norm": 1.018474817276001,
      "learning_rate": 0.00010869674185463659,
      "loss": 0.8949,
      "step": 10930
    },
    {
      "epoch": 3.198830409356725,
      "grad_norm": 1.0135220289230347,
      "learning_rate": 0.00010861319966583126,
      "loss": 0.9873,
      "step": 10940
    },
    {
      "epoch": 3.2017543859649122,
      "grad_norm": 0.8412733674049377,
      "learning_rate": 0.0001085296574770259,
      "loss": 0.8756,
      "step": 10950
    },
    {
      "epoch": 3.2046783625730995,
      "grad_norm": 0.784698486328125,
      "learning_rate": 0.00010844611528822055,
      "loss": 0.903,
      "step": 10960
    },
    {
      "epoch": 3.2076023391812867,
      "grad_norm": 0.8807885050773621,
      "learning_rate": 0.0001083625730994152,
      "loss": 0.8773,
      "step": 10970
    },
    {
      "epoch": 3.2105263157894735,
      "grad_norm": 0.8453699946403503,
      "learning_rate": 0.00010827903091060987,
      "loss": 0.9275,
      "step": 10980
    },
    {
      "epoch": 3.2134502923976607,
      "grad_norm": 0.8865674734115601,
      "learning_rate": 0.00010819548872180451,
      "loss": 0.9338,
      "step": 10990
    },
    {
      "epoch": 3.216374269005848,
      "grad_norm": 0.8770244121551514,
      "learning_rate": 0.00010811194653299916,
      "loss": 0.9027,
      "step": 11000
    },
    {
      "epoch": 3.219298245614035,
      "grad_norm": 0.7649328112602234,
      "learning_rate": 0.00010802840434419382,
      "loss": 0.8947,
      "step": 11010
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.7098599076271057,
      "learning_rate": 0.00010794486215538849,
      "loss": 0.9258,
      "step": 11020
    },
    {
      "epoch": 3.2251461988304095,
      "grad_norm": 0.8109949827194214,
      "learning_rate": 0.00010786131996658313,
      "loss": 0.8356,
      "step": 11030
    },
    {
      "epoch": 3.2280701754385963,
      "grad_norm": 0.9088932275772095,
      "learning_rate": 0.00010777777777777778,
      "loss": 0.9853,
      "step": 11040
    },
    {
      "epoch": 3.2309941520467835,
      "grad_norm": 0.7024179100990295,
      "learning_rate": 0.00010769423558897242,
      "loss": 0.887,
      "step": 11050
    },
    {
      "epoch": 3.2339181286549707,
      "grad_norm": 1.1591466665267944,
      "learning_rate": 0.0001076106934001671,
      "loss": 0.993,
      "step": 11060
    },
    {
      "epoch": 3.236842105263158,
      "grad_norm": 0.9981894493103027,
      "learning_rate": 0.00010752715121136174,
      "loss": 0.9004,
      "step": 11070
    },
    {
      "epoch": 3.239766081871345,
      "grad_norm": 0.8086007237434387,
      "learning_rate": 0.00010744360902255639,
      "loss": 0.8816,
      "step": 11080
    },
    {
      "epoch": 3.242690058479532,
      "grad_norm": 0.7720463275909424,
      "learning_rate": 0.00010736006683375103,
      "loss": 0.9211,
      "step": 11090
    },
    {
      "epoch": 3.245614035087719,
      "grad_norm": 0.8111171126365662,
      "learning_rate": 0.00010727652464494571,
      "loss": 0.9119,
      "step": 11100
    },
    {
      "epoch": 3.2485380116959064,
      "grad_norm": 0.7451402544975281,
      "learning_rate": 0.00010719298245614035,
      "loss": 0.9179,
      "step": 11110
    },
    {
      "epoch": 3.2514619883040936,
      "grad_norm": 0.9232035279273987,
      "learning_rate": 0.00010710944026733501,
      "loss": 0.9514,
      "step": 11120
    },
    {
      "epoch": 3.254385964912281,
      "grad_norm": 0.9936192035675049,
      "learning_rate": 0.00010702589807852965,
      "loss": 0.915,
      "step": 11130
    },
    {
      "epoch": 3.257309941520468,
      "grad_norm": 0.7849332690238953,
      "learning_rate": 0.00010694235588972433,
      "loss": 0.8727,
      "step": 11140
    },
    {
      "epoch": 3.260233918128655,
      "grad_norm": 0.7838749289512634,
      "learning_rate": 0.00010685881370091897,
      "loss": 0.9016,
      "step": 11150
    },
    {
      "epoch": 3.263157894736842,
      "grad_norm": 0.8105223774909973,
      "learning_rate": 0.00010677527151211362,
      "loss": 0.8853,
      "step": 11160
    },
    {
      "epoch": 3.2660818713450293,
      "grad_norm": 0.7930618524551392,
      "learning_rate": 0.00010669172932330826,
      "loss": 0.9256,
      "step": 11170
    },
    {
      "epoch": 3.2690058479532165,
      "grad_norm": 0.7495575547218323,
      "learning_rate": 0.00010660818713450294,
      "loss": 0.8855,
      "step": 11180
    },
    {
      "epoch": 3.2719298245614037,
      "grad_norm": 0.7758169770240784,
      "learning_rate": 0.00010652464494569758,
      "loss": 0.865,
      "step": 11190
    },
    {
      "epoch": 3.2748538011695905,
      "grad_norm": 0.7931631803512573,
      "learning_rate": 0.00010644110275689223,
      "loss": 0.9283,
      "step": 11200
    },
    {
      "epoch": 3.2777777777777777,
      "grad_norm": 0.7292703986167908,
      "learning_rate": 0.00010635756056808687,
      "loss": 0.9385,
      "step": 11210
    },
    {
      "epoch": 3.280701754385965,
      "grad_norm": 0.9076365828514099,
      "learning_rate": 0.00010627401837928154,
      "loss": 0.9136,
      "step": 11220
    },
    {
      "epoch": 3.283625730994152,
      "grad_norm": 0.883217990398407,
      "learning_rate": 0.0001061904761904762,
      "loss": 0.9603,
      "step": 11230
    },
    {
      "epoch": 3.2865497076023393,
      "grad_norm": 0.7625993490219116,
      "learning_rate": 0.00010610693400167085,
      "loss": 0.8256,
      "step": 11240
    },
    {
      "epoch": 3.2894736842105265,
      "grad_norm": 0.7921348810195923,
      "learning_rate": 0.00010602339181286549,
      "loss": 0.8887,
      "step": 11250
    },
    {
      "epoch": 3.2923976608187133,
      "grad_norm": 0.7694780230522156,
      "learning_rate": 0.00010593984962406016,
      "loss": 0.8907,
      "step": 11260
    },
    {
      "epoch": 3.2953216374269005,
      "grad_norm": 0.8545554280281067,
      "learning_rate": 0.00010585630743525481,
      "loss": 1.0796,
      "step": 11270
    },
    {
      "epoch": 3.2982456140350878,
      "grad_norm": 0.9879639744758606,
      "learning_rate": 0.00010577276524644946,
      "loss": 0.9033,
      "step": 11280
    },
    {
      "epoch": 3.301169590643275,
      "grad_norm": 0.8812465667724609,
      "learning_rate": 0.0001056892230576441,
      "loss": 0.897,
      "step": 11290
    },
    {
      "epoch": 3.3040935672514617,
      "grad_norm": 1.3696852922439575,
      "learning_rate": 0.00010560568086883877,
      "loss": 0.9129,
      "step": 11300
    },
    {
      "epoch": 3.307017543859649,
      "grad_norm": 1.065731167793274,
      "learning_rate": 0.00010552213868003342,
      "loss": 0.9047,
      "step": 11310
    },
    {
      "epoch": 3.309941520467836,
      "grad_norm": 0.8215894103050232,
      "learning_rate": 0.00010543859649122806,
      "loss": 0.9159,
      "step": 11320
    },
    {
      "epoch": 3.3128654970760234,
      "grad_norm": 0.737277626991272,
      "learning_rate": 0.00010535505430242272,
      "loss": 0.8759,
      "step": 11330
    },
    {
      "epoch": 3.3157894736842106,
      "grad_norm": 0.64422607421875,
      "learning_rate": 0.00010527151211361738,
      "loss": 0.87,
      "step": 11340
    },
    {
      "epoch": 3.318713450292398,
      "grad_norm": 0.8294669985771179,
      "learning_rate": 0.00010518796992481204,
      "loss": 0.9044,
      "step": 11350
    },
    {
      "epoch": 3.3216374269005846,
      "grad_norm": 0.9011407494544983,
      "learning_rate": 0.00010510442773600668,
      "loss": 0.8846,
      "step": 11360
    },
    {
      "epoch": 3.324561403508772,
      "grad_norm": 0.8178644776344299,
      "learning_rate": 0.00010502088554720133,
      "loss": 0.9584,
      "step": 11370
    },
    {
      "epoch": 3.327485380116959,
      "grad_norm": 0.8220809102058411,
      "learning_rate": 0.000104937343358396,
      "loss": 0.9693,
      "step": 11380
    },
    {
      "epoch": 3.3304093567251463,
      "grad_norm": 0.7271997928619385,
      "learning_rate": 0.00010485380116959065,
      "loss": 0.8921,
      "step": 11390
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.7871641516685486,
      "learning_rate": 0.00010477025898078529,
      "loss": 0.9524,
      "step": 11400
    },
    {
      "epoch": 3.3362573099415203,
      "grad_norm": 0.5751303434371948,
      "learning_rate": 0.00010468671679197994,
      "loss": 0.8241,
      "step": 11410
    },
    {
      "epoch": 3.3391812865497075,
      "grad_norm": 0.9969089031219482,
      "learning_rate": 0.00010460317460317461,
      "loss": 0.886,
      "step": 11420
    },
    {
      "epoch": 3.3421052631578947,
      "grad_norm": 0.734865128993988,
      "learning_rate": 0.00010451963241436927,
      "loss": 0.8564,
      "step": 11430
    },
    {
      "epoch": 3.345029239766082,
      "grad_norm": 0.7896857261657715,
      "learning_rate": 0.0001044360902255639,
      "loss": 0.8238,
      "step": 11440
    },
    {
      "epoch": 3.347953216374269,
      "grad_norm": 0.8409510850906372,
      "learning_rate": 0.00010435254803675856,
      "loss": 0.9155,
      "step": 11450
    },
    {
      "epoch": 3.3508771929824563,
      "grad_norm": 0.7648969292640686,
      "learning_rate": 0.00010426900584795323,
      "loss": 0.8507,
      "step": 11460
    },
    {
      "epoch": 3.353801169590643,
      "grad_norm": 0.8726041913032532,
      "learning_rate": 0.00010418546365914788,
      "loss": 0.8863,
      "step": 11470
    },
    {
      "epoch": 3.3567251461988303,
      "grad_norm": 0.7658041715621948,
      "learning_rate": 0.00010410192147034252,
      "loss": 0.9331,
      "step": 11480
    },
    {
      "epoch": 3.3596491228070176,
      "grad_norm": 0.9641531705856323,
      "learning_rate": 0.00010401837928153717,
      "loss": 0.9197,
      "step": 11490
    },
    {
      "epoch": 3.3625730994152048,
      "grad_norm": 0.9149304628372192,
      "learning_rate": 0.00010393483709273184,
      "loss": 0.8722,
      "step": 11500
    },
    {
      "epoch": 3.365497076023392,
      "grad_norm": 0.8128212094306946,
      "learning_rate": 0.0001038512949039265,
      "loss": 0.8909,
      "step": 11510
    },
    {
      "epoch": 3.3684210526315788,
      "grad_norm": 0.9213204979896545,
      "learning_rate": 0.00010376775271512113,
      "loss": 0.9757,
      "step": 11520
    },
    {
      "epoch": 3.371345029239766,
      "grad_norm": 1.3337560892105103,
      "learning_rate": 0.00010368421052631579,
      "loss": 0.8987,
      "step": 11530
    },
    {
      "epoch": 3.374269005847953,
      "grad_norm": 0.7137324810028076,
      "learning_rate": 0.00010360066833751045,
      "loss": 0.8396,
      "step": 11540
    },
    {
      "epoch": 3.3771929824561404,
      "grad_norm": 0.6240653395652771,
      "learning_rate": 0.00010351712614870511,
      "loss": 0.9469,
      "step": 11550
    },
    {
      "epoch": 3.3801169590643276,
      "grad_norm": 0.8590059280395508,
      "learning_rate": 0.00010343358395989975,
      "loss": 0.9594,
      "step": 11560
    },
    {
      "epoch": 3.383040935672515,
      "grad_norm": 0.7237604856491089,
      "learning_rate": 0.0001033500417710944,
      "loss": 0.9132,
      "step": 11570
    },
    {
      "epoch": 3.3859649122807016,
      "grad_norm": 0.9827312231063843,
      "learning_rate": 0.00010326649958228907,
      "loss": 0.887,
      "step": 11580
    },
    {
      "epoch": 3.388888888888889,
      "grad_norm": 0.9346542358398438,
      "learning_rate": 0.00010318295739348372,
      "loss": 0.9935,
      "step": 11590
    },
    {
      "epoch": 3.391812865497076,
      "grad_norm": 0.8479600548744202,
      "learning_rate": 0.00010309941520467836,
      "loss": 0.9368,
      "step": 11600
    },
    {
      "epoch": 3.3947368421052633,
      "grad_norm": 0.8311553597450256,
      "learning_rate": 0.00010301587301587301,
      "loss": 0.8543,
      "step": 11610
    },
    {
      "epoch": 3.39766081871345,
      "grad_norm": 0.8127429485321045,
      "learning_rate": 0.00010293233082706768,
      "loss": 1.0155,
      "step": 11620
    },
    {
      "epoch": 3.4005847953216373,
      "grad_norm": 0.9101866483688354,
      "learning_rate": 0.00010284878863826232,
      "loss": 0.9933,
      "step": 11630
    },
    {
      "epoch": 3.4035087719298245,
      "grad_norm": 0.9813865423202515,
      "learning_rate": 0.00010276524644945698,
      "loss": 0.9269,
      "step": 11640
    },
    {
      "epoch": 3.4064327485380117,
      "grad_norm": 1.063334584236145,
      "learning_rate": 0.00010268170426065163,
      "loss": 0.863,
      "step": 11650
    },
    {
      "epoch": 3.409356725146199,
      "grad_norm": 0.638275682926178,
      "learning_rate": 0.0001025981620718463,
      "loss": 0.9043,
      "step": 11660
    },
    {
      "epoch": 3.412280701754386,
      "grad_norm": 0.8259644508361816,
      "learning_rate": 0.00010251461988304094,
      "loss": 0.9098,
      "step": 11670
    },
    {
      "epoch": 3.415204678362573,
      "grad_norm": 0.8586992025375366,
      "learning_rate": 0.00010243107769423559,
      "loss": 0.8627,
      "step": 11680
    },
    {
      "epoch": 3.41812865497076,
      "grad_norm": 0.9789396524429321,
      "learning_rate": 0.00010234753550543024,
      "loss": 0.9604,
      "step": 11690
    },
    {
      "epoch": 3.4210526315789473,
      "grad_norm": 0.901614248752594,
      "learning_rate": 0.00010226399331662491,
      "loss": 0.9471,
      "step": 11700
    },
    {
      "epoch": 3.4239766081871346,
      "grad_norm": 0.8252519965171814,
      "learning_rate": 0.00010218045112781955,
      "loss": 0.9296,
      "step": 11710
    },
    {
      "epoch": 3.426900584795322,
      "grad_norm": 1.048729419708252,
      "learning_rate": 0.0001020969089390142,
      "loss": 0.9538,
      "step": 11720
    },
    {
      "epoch": 3.4298245614035086,
      "grad_norm": 3.2584567070007324,
      "learning_rate": 0.00010201336675020884,
      "loss": 0.9606,
      "step": 11730
    },
    {
      "epoch": 3.4327485380116958,
      "grad_norm": 0.9205173850059509,
      "learning_rate": 0.00010192982456140352,
      "loss": 0.8973,
      "step": 11740
    },
    {
      "epoch": 3.435672514619883,
      "grad_norm": 0.7158408164978027,
      "learning_rate": 0.00010184628237259816,
      "loss": 0.927,
      "step": 11750
    },
    {
      "epoch": 3.43859649122807,
      "grad_norm": 0.8734086155891418,
      "learning_rate": 0.00010176274018379282,
      "loss": 0.9271,
      "step": 11760
    },
    {
      "epoch": 3.4415204678362574,
      "grad_norm": 0.8530991077423096,
      "learning_rate": 0.00010167919799498746,
      "loss": 0.8985,
      "step": 11770
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.6830211877822876,
      "learning_rate": 0.00010159565580618214,
      "loss": 0.8122,
      "step": 11780
    },
    {
      "epoch": 3.4473684210526314,
      "grad_norm": 1.0771490335464478,
      "learning_rate": 0.00010151211361737678,
      "loss": 0.8919,
      "step": 11790
    },
    {
      "epoch": 3.4502923976608186,
      "grad_norm": 0.8250571489334106,
      "learning_rate": 0.00010142857142857143,
      "loss": 0.8943,
      "step": 11800
    },
    {
      "epoch": 3.453216374269006,
      "grad_norm": 0.8110216856002808,
      "learning_rate": 0.00010134502923976607,
      "loss": 0.8875,
      "step": 11810
    },
    {
      "epoch": 3.456140350877193,
      "grad_norm": 0.7942895889282227,
      "learning_rate": 0.00010126148705096075,
      "loss": 0.9261,
      "step": 11820
    },
    {
      "epoch": 3.4590643274853803,
      "grad_norm": 0.8647850751876831,
      "learning_rate": 0.00010117794486215539,
      "loss": 0.9414,
      "step": 11830
    },
    {
      "epoch": 3.461988304093567,
      "grad_norm": 0.7559020519256592,
      "learning_rate": 0.00010109440267335005,
      "loss": 0.9164,
      "step": 11840
    },
    {
      "epoch": 3.4649122807017543,
      "grad_norm": 0.8911786079406738,
      "learning_rate": 0.00010101086048454469,
      "loss": 0.9137,
      "step": 11850
    },
    {
      "epoch": 3.4678362573099415,
      "grad_norm": 0.719404399394989,
      "learning_rate": 0.00010092731829573937,
      "loss": 0.8719,
      "step": 11860
    },
    {
      "epoch": 3.4707602339181287,
      "grad_norm": 0.9756779074668884,
      "learning_rate": 0.000100843776106934,
      "loss": 0.9159,
      "step": 11870
    },
    {
      "epoch": 3.473684210526316,
      "grad_norm": 0.7804193496704102,
      "learning_rate": 0.00010076023391812866,
      "loss": 0.9553,
      "step": 11880
    },
    {
      "epoch": 3.476608187134503,
      "grad_norm": 0.8321222066879272,
      "learning_rate": 0.0001006766917293233,
      "loss": 0.8956,
      "step": 11890
    },
    {
      "epoch": 3.47953216374269,
      "grad_norm": 0.9851781725883484,
      "learning_rate": 0.00010059314954051797,
      "loss": 0.9195,
      "step": 11900
    },
    {
      "epoch": 3.482456140350877,
      "grad_norm": 0.6354114413261414,
      "learning_rate": 0.00010050960735171262,
      "loss": 0.89,
      "step": 11910
    },
    {
      "epoch": 3.4853801169590644,
      "grad_norm": 1.0626050233840942,
      "learning_rate": 0.00010042606516290727,
      "loss": 0.9614,
      "step": 11920
    },
    {
      "epoch": 3.4883040935672516,
      "grad_norm": 1.2047145366668701,
      "learning_rate": 0.00010034252297410191,
      "loss": 0.9232,
      "step": 11930
    },
    {
      "epoch": 3.4912280701754383,
      "grad_norm": 0.7467257976531982,
      "learning_rate": 0.00010025898078529658,
      "loss": 0.9187,
      "step": 11940
    },
    {
      "epoch": 3.4941520467836256,
      "grad_norm": 0.7360184788703918,
      "learning_rate": 0.00010017543859649123,
      "loss": 0.9994,
      "step": 11950
    },
    {
      "epoch": 3.497076023391813,
      "grad_norm": 0.8859644532203674,
      "learning_rate": 0.00010009189640768589,
      "loss": 0.9097,
      "step": 11960
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.8114973306655884,
      "learning_rate": 0.00010000835421888053,
      "loss": 0.9222,
      "step": 11970
    },
    {
      "epoch": 3.502923976608187,
      "grad_norm": 1.0980339050292969,
      "learning_rate": 9.99248120300752e-05,
      "loss": 0.882,
      "step": 11980
    },
    {
      "epoch": 3.5058479532163744,
      "grad_norm": 0.6526875495910645,
      "learning_rate": 9.984126984126985e-05,
      "loss": 0.9125,
      "step": 11990
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 0.833218514919281,
      "learning_rate": 9.975772765246449e-05,
      "loss": 0.8806,
      "step": 12000
    },
    {
      "epoch": 3.5116959064327484,
      "grad_norm": 0.823230504989624,
      "learning_rate": 9.967418546365915e-05,
      "loss": 0.9953,
      "step": 12010
    },
    {
      "epoch": 3.5146198830409356,
      "grad_norm": 0.865462601184845,
      "learning_rate": 9.95906432748538e-05,
      "loss": 0.9138,
      "step": 12020
    },
    {
      "epoch": 3.517543859649123,
      "grad_norm": 0.9502744674682617,
      "learning_rate": 9.950710108604846e-05,
      "loss": 0.9773,
      "step": 12030
    },
    {
      "epoch": 3.52046783625731,
      "grad_norm": 0.8781160116195679,
      "learning_rate": 9.94235588972431e-05,
      "loss": 0.9187,
      "step": 12040
    },
    {
      "epoch": 3.523391812865497,
      "grad_norm": 0.8887084722518921,
      "learning_rate": 9.934001670843777e-05,
      "loss": 0.8525,
      "step": 12050
    },
    {
      "epoch": 3.526315789473684,
      "grad_norm": 0.7817806601524353,
      "learning_rate": 9.925647451963241e-05,
      "loss": 0.903,
      "step": 12060
    },
    {
      "epoch": 3.5292397660818713,
      "grad_norm": 0.7523115277290344,
      "learning_rate": 9.917293233082708e-05,
      "loss": 0.8888,
      "step": 12070
    },
    {
      "epoch": 3.5321637426900585,
      "grad_norm": 0.883924126625061,
      "learning_rate": 9.908939014202172e-05,
      "loss": 0.9573,
      "step": 12080
    },
    {
      "epoch": 3.5350877192982457,
      "grad_norm": 0.7790533304214478,
      "learning_rate": 9.900584795321638e-05,
      "loss": 0.8909,
      "step": 12090
    },
    {
      "epoch": 3.538011695906433,
      "grad_norm": 0.815155565738678,
      "learning_rate": 9.892230576441102e-05,
      "loss": 1.0204,
      "step": 12100
    },
    {
      "epoch": 3.54093567251462,
      "grad_norm": 0.7442652583122253,
      "learning_rate": 9.883876357560569e-05,
      "loss": 0.9627,
      "step": 12110
    },
    {
      "epoch": 3.543859649122807,
      "grad_norm": 0.7912565469741821,
      "learning_rate": 9.875522138680033e-05,
      "loss": 0.9038,
      "step": 12120
    },
    {
      "epoch": 3.546783625730994,
      "grad_norm": 0.7442408800125122,
      "learning_rate": 9.8671679197995e-05,
      "loss": 0.9314,
      "step": 12130
    },
    {
      "epoch": 3.5497076023391814,
      "grad_norm": 0.6490673422813416,
      "learning_rate": 9.858813700918964e-05,
      "loss": 0.9512,
      "step": 12140
    },
    {
      "epoch": 3.5526315789473686,
      "grad_norm": 0.6903819441795349,
      "learning_rate": 9.85045948203843e-05,
      "loss": 0.896,
      "step": 12150
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.7655144333839417,
      "learning_rate": 9.842105263157894e-05,
      "loss": 0.9259,
      "step": 12160
    },
    {
      "epoch": 3.5584795321637426,
      "grad_norm": 0.9517795443534851,
      "learning_rate": 9.833751044277361e-05,
      "loss": 0.8541,
      "step": 12170
    },
    {
      "epoch": 3.56140350877193,
      "grad_norm": 1.2357221841812134,
      "learning_rate": 9.825396825396825e-05,
      "loss": 0.9479,
      "step": 12180
    },
    {
      "epoch": 3.564327485380117,
      "grad_norm": 0.8097981214523315,
      "learning_rate": 9.817042606516292e-05,
      "loss": 0.992,
      "step": 12190
    },
    {
      "epoch": 3.5672514619883042,
      "grad_norm": 0.7892739176750183,
      "learning_rate": 9.808688387635756e-05,
      "loss": 0.9513,
      "step": 12200
    },
    {
      "epoch": 3.5701754385964914,
      "grad_norm": 0.9122539758682251,
      "learning_rate": 9.800334168755222e-05,
      "loss": 0.9196,
      "step": 12210
    },
    {
      "epoch": 3.573099415204678,
      "grad_norm": 1.0662206411361694,
      "learning_rate": 9.791979949874686e-05,
      "loss": 0.8974,
      "step": 12220
    },
    {
      "epoch": 3.5760233918128654,
      "grad_norm": 0.8573712110519409,
      "learning_rate": 9.783625730994153e-05,
      "loss": 0.9255,
      "step": 12230
    },
    {
      "epoch": 3.5789473684210527,
      "grad_norm": 0.8763179183006287,
      "learning_rate": 9.775271512113617e-05,
      "loss": 0.8741,
      "step": 12240
    },
    {
      "epoch": 3.58187134502924,
      "grad_norm": 0.8454317450523376,
      "learning_rate": 9.766917293233084e-05,
      "loss": 0.8989,
      "step": 12250
    },
    {
      "epoch": 3.5847953216374266,
      "grad_norm": 0.746486485004425,
      "learning_rate": 9.758563074352548e-05,
      "loss": 0.9101,
      "step": 12260
    },
    {
      "epoch": 3.587719298245614,
      "grad_norm": 0.8718119263648987,
      "learning_rate": 9.750208855472015e-05,
      "loss": 0.9465,
      "step": 12270
    },
    {
      "epoch": 3.590643274853801,
      "grad_norm": 0.8644924163818359,
      "learning_rate": 9.741854636591479e-05,
      "loss": 0.8845,
      "step": 12280
    },
    {
      "epoch": 3.5935672514619883,
      "grad_norm": 0.6946834325790405,
      "learning_rate": 9.733500417710944e-05,
      "loss": 0.8591,
      "step": 12290
    },
    {
      "epoch": 3.5964912280701755,
      "grad_norm": 0.7095650434494019,
      "learning_rate": 9.725146198830409e-05,
      "loss": 0.8918,
      "step": 12300
    },
    {
      "epoch": 3.5994152046783627,
      "grad_norm": 0.8235481977462769,
      "learning_rate": 9.716791979949875e-05,
      "loss": 0.8531,
      "step": 12310
    },
    {
      "epoch": 3.60233918128655,
      "grad_norm": 1.120700478553772,
      "learning_rate": 9.70843776106934e-05,
      "loss": 1.0281,
      "step": 12320
    },
    {
      "epoch": 3.6052631578947367,
      "grad_norm": 0.8796460628509521,
      "learning_rate": 9.700083542188805e-05,
      "loss": 0.8826,
      "step": 12330
    },
    {
      "epoch": 3.608187134502924,
      "grad_norm": 0.7524154186248779,
      "learning_rate": 9.69172932330827e-05,
      "loss": 0.8596,
      "step": 12340
    },
    {
      "epoch": 3.611111111111111,
      "grad_norm": 0.7837436199188232,
      "learning_rate": 9.683375104427736e-05,
      "loss": 1.025,
      "step": 12350
    },
    {
      "epoch": 3.6140350877192984,
      "grad_norm": 0.9020234942436218,
      "learning_rate": 9.675020885547201e-05,
      "loss": 1.1881,
      "step": 12360
    },
    {
      "epoch": 3.616959064327485,
      "grad_norm": 0.8680074214935303,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.9224,
      "step": 12370
    },
    {
      "epoch": 3.6198830409356724,
      "grad_norm": 0.7264263033866882,
      "learning_rate": 9.658312447786132e-05,
      "loss": 0.8645,
      "step": 12380
    },
    {
      "epoch": 3.6228070175438596,
      "grad_norm": 1.0950145721435547,
      "learning_rate": 9.649958228905597e-05,
      "loss": 0.9233,
      "step": 12390
    },
    {
      "epoch": 3.625730994152047,
      "grad_norm": 0.8288534283638,
      "learning_rate": 9.641604010025063e-05,
      "loss": 0.9055,
      "step": 12400
    },
    {
      "epoch": 3.628654970760234,
      "grad_norm": 0.840042233467102,
      "learning_rate": 9.633249791144528e-05,
      "loss": 0.9283,
      "step": 12410
    },
    {
      "epoch": 3.6315789473684212,
      "grad_norm": 1.009525179862976,
      "learning_rate": 9.624895572263993e-05,
      "loss": 0.9355,
      "step": 12420
    },
    {
      "epoch": 3.6345029239766085,
      "grad_norm": 0.6164930462837219,
      "learning_rate": 9.616541353383459e-05,
      "loss": 0.8671,
      "step": 12430
    },
    {
      "epoch": 3.6374269005847952,
      "grad_norm": 0.8162983655929565,
      "learning_rate": 9.608187134502924e-05,
      "loss": 0.9638,
      "step": 12440
    },
    {
      "epoch": 3.6403508771929824,
      "grad_norm": 0.8386042714118958,
      "learning_rate": 9.59983291562239e-05,
      "loss": 0.9468,
      "step": 12450
    },
    {
      "epoch": 3.6432748538011697,
      "grad_norm": 0.9185606837272644,
      "learning_rate": 9.591478696741855e-05,
      "loss": 0.8616,
      "step": 12460
    },
    {
      "epoch": 3.646198830409357,
      "grad_norm": 0.8450839519500732,
      "learning_rate": 9.58312447786132e-05,
      "loss": 0.9768,
      "step": 12470
    },
    {
      "epoch": 3.6491228070175437,
      "grad_norm": 0.6184044480323792,
      "learning_rate": 9.574770258980786e-05,
      "loss": 0.9273,
      "step": 12480
    },
    {
      "epoch": 3.652046783625731,
      "grad_norm": 0.7308557033538818,
      "learning_rate": 9.566416040100251e-05,
      "loss": 0.8559,
      "step": 12490
    },
    {
      "epoch": 3.654970760233918,
      "grad_norm": 1.0404152870178223,
      "learning_rate": 9.558061821219716e-05,
      "loss": 0.924,
      "step": 12500
    },
    {
      "epoch": 3.6578947368421053,
      "grad_norm": 1.0172879695892334,
      "learning_rate": 9.549707602339182e-05,
      "loss": 0.9582,
      "step": 12510
    },
    {
      "epoch": 3.6608187134502925,
      "grad_norm": 0.8704904913902283,
      "learning_rate": 9.541353383458647e-05,
      "loss": 0.8951,
      "step": 12520
    },
    {
      "epoch": 3.6637426900584797,
      "grad_norm": 0.8816823363304138,
      "learning_rate": 9.532999164578112e-05,
      "loss": 0.9217,
      "step": 12530
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.8418276906013489,
      "learning_rate": 9.524644945697578e-05,
      "loss": 0.86,
      "step": 12540
    },
    {
      "epoch": 3.6695906432748537,
      "grad_norm": 0.7725452184677124,
      "learning_rate": 9.516290726817043e-05,
      "loss": 0.9283,
      "step": 12550
    },
    {
      "epoch": 3.672514619883041,
      "grad_norm": 1.0668164491653442,
      "learning_rate": 9.507936507936508e-05,
      "loss": 1.0122,
      "step": 12560
    },
    {
      "epoch": 3.675438596491228,
      "grad_norm": 0.9934494495391846,
      "learning_rate": 9.499582289055974e-05,
      "loss": 0.9033,
      "step": 12570
    },
    {
      "epoch": 3.678362573099415,
      "grad_norm": 0.9240531325340271,
      "learning_rate": 9.491228070175439e-05,
      "loss": 0.9691,
      "step": 12580
    },
    {
      "epoch": 3.681286549707602,
      "grad_norm": 0.9816067218780518,
      "learning_rate": 9.482873851294904e-05,
      "loss": 0.9778,
      "step": 12590
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 0.9045825600624084,
      "learning_rate": 9.47451963241437e-05,
      "loss": 0.9146,
      "step": 12600
    },
    {
      "epoch": 3.6871345029239766,
      "grad_norm": 0.7473419904708862,
      "learning_rate": 9.466165413533835e-05,
      "loss": 0.9621,
      "step": 12610
    },
    {
      "epoch": 3.690058479532164,
      "grad_norm": 0.8341697454452515,
      "learning_rate": 9.4578111946533e-05,
      "loss": 0.9636,
      "step": 12620
    },
    {
      "epoch": 3.692982456140351,
      "grad_norm": 0.6677048206329346,
      "learning_rate": 9.449456975772766e-05,
      "loss": 0.9022,
      "step": 12630
    },
    {
      "epoch": 3.6959064327485383,
      "grad_norm": 0.7760108113288879,
      "learning_rate": 9.441102756892231e-05,
      "loss": 0.887,
      "step": 12640
    },
    {
      "epoch": 3.698830409356725,
      "grad_norm": 0.6533352732658386,
      "learning_rate": 9.432748538011697e-05,
      "loss": 0.8651,
      "step": 12650
    },
    {
      "epoch": 3.7017543859649122,
      "grad_norm": 0.8952114582061768,
      "learning_rate": 9.424394319131162e-05,
      "loss": 0.8949,
      "step": 12660
    },
    {
      "epoch": 3.7046783625730995,
      "grad_norm": 0.6198782920837402,
      "learning_rate": 9.416040100250627e-05,
      "loss": 0.8845,
      "step": 12670
    },
    {
      "epoch": 3.7076023391812867,
      "grad_norm": 0.7954405546188354,
      "learning_rate": 9.407685881370091e-05,
      "loss": 0.9086,
      "step": 12680
    },
    {
      "epoch": 3.7105263157894735,
      "grad_norm": 0.9376554489135742,
      "learning_rate": 9.399331662489558e-05,
      "loss": 0.9915,
      "step": 12690
    },
    {
      "epoch": 3.7134502923976607,
      "grad_norm": 0.7865757942199707,
      "learning_rate": 9.390977443609022e-05,
      "loss": 0.8141,
      "step": 12700
    },
    {
      "epoch": 3.716374269005848,
      "grad_norm": 0.6669210195541382,
      "learning_rate": 9.382623224728489e-05,
      "loss": 0.8804,
      "step": 12710
    },
    {
      "epoch": 3.719298245614035,
      "grad_norm": 1.039330244064331,
      "learning_rate": 9.374269005847953e-05,
      "loss": 1.0004,
      "step": 12720
    },
    {
      "epoch": 3.7222222222222223,
      "grad_norm": 0.8001920580863953,
      "learning_rate": 9.365914786967419e-05,
      "loss": 0.9288,
      "step": 12730
    },
    {
      "epoch": 3.7251461988304095,
      "grad_norm": 0.9643344879150391,
      "learning_rate": 9.357560568086883e-05,
      "loss": 0.9465,
      "step": 12740
    },
    {
      "epoch": 3.7280701754385968,
      "grad_norm": 0.9272522926330566,
      "learning_rate": 9.34920634920635e-05,
      "loss": 0.8708,
      "step": 12750
    },
    {
      "epoch": 3.7309941520467835,
      "grad_norm": 0.8795456886291504,
      "learning_rate": 9.340852130325814e-05,
      "loss": 0.917,
      "step": 12760
    },
    {
      "epoch": 3.7339181286549707,
      "grad_norm": 0.643703818321228,
      "learning_rate": 9.332497911445281e-05,
      "loss": 0.8507,
      "step": 12770
    },
    {
      "epoch": 3.736842105263158,
      "grad_norm": 0.812562882900238,
      "learning_rate": 9.324143692564745e-05,
      "loss": 0.9719,
      "step": 12780
    },
    {
      "epoch": 3.739766081871345,
      "grad_norm": 0.8004171252250671,
      "learning_rate": 9.315789473684211e-05,
      "loss": 0.9901,
      "step": 12790
    },
    {
      "epoch": 3.742690058479532,
      "grad_norm": 1.0169157981872559,
      "learning_rate": 9.307435254803675e-05,
      "loss": 0.8823,
      "step": 12800
    },
    {
      "epoch": 3.745614035087719,
      "grad_norm": 0.852351725101471,
      "learning_rate": 9.299081035923142e-05,
      "loss": 0.8907,
      "step": 12810
    },
    {
      "epoch": 3.7485380116959064,
      "grad_norm": 0.8420584201812744,
      "learning_rate": 9.290726817042606e-05,
      "loss": 0.9379,
      "step": 12820
    },
    {
      "epoch": 3.7514619883040936,
      "grad_norm": 0.8207793831825256,
      "learning_rate": 9.282372598162073e-05,
      "loss": 0.8635,
      "step": 12830
    },
    {
      "epoch": 3.754385964912281,
      "grad_norm": 0.774459719657898,
      "learning_rate": 9.274018379281537e-05,
      "loss": 0.9208,
      "step": 12840
    },
    {
      "epoch": 3.757309941520468,
      "grad_norm": 0.9682783484458923,
      "learning_rate": 9.265664160401004e-05,
      "loss": 0.9276,
      "step": 12850
    },
    {
      "epoch": 3.760233918128655,
      "grad_norm": 1.1248326301574707,
      "learning_rate": 9.257309941520468e-05,
      "loss": 0.8659,
      "step": 12860
    },
    {
      "epoch": 3.763157894736842,
      "grad_norm": 0.8888601660728455,
      "learning_rate": 9.248955722639934e-05,
      "loss": 0.903,
      "step": 12870
    },
    {
      "epoch": 3.7660818713450293,
      "grad_norm": 0.8240700364112854,
      "learning_rate": 9.240601503759398e-05,
      "loss": 0.885,
      "step": 12880
    },
    {
      "epoch": 3.7690058479532165,
      "grad_norm": 0.7478064298629761,
      "learning_rate": 9.232247284878865e-05,
      "loss": 0.9063,
      "step": 12890
    },
    {
      "epoch": 3.7719298245614032,
      "grad_norm": 0.7653393149375916,
      "learning_rate": 9.223893065998329e-05,
      "loss": 0.8926,
      "step": 12900
    },
    {
      "epoch": 3.7748538011695905,
      "grad_norm": 1.024966835975647,
      "learning_rate": 9.215538847117796e-05,
      "loss": 0.9051,
      "step": 12910
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.9427524209022522,
      "learning_rate": 9.20718462823726e-05,
      "loss": 0.8979,
      "step": 12920
    },
    {
      "epoch": 3.780701754385965,
      "grad_norm": 1.0401477813720703,
      "learning_rate": 9.198830409356726e-05,
      "loss": 0.9189,
      "step": 12930
    },
    {
      "epoch": 3.783625730994152,
      "grad_norm": 0.9349235892295837,
      "learning_rate": 9.19047619047619e-05,
      "loss": 0.9347,
      "step": 12940
    },
    {
      "epoch": 3.7865497076023393,
      "grad_norm": 1.164652943611145,
      "learning_rate": 9.182121971595657e-05,
      "loss": 0.9509,
      "step": 12950
    },
    {
      "epoch": 3.7894736842105265,
      "grad_norm": 0.9054650664329529,
      "learning_rate": 9.173767752715121e-05,
      "loss": 0.935,
      "step": 12960
    },
    {
      "epoch": 3.7923976608187133,
      "grad_norm": 1.1547383069992065,
      "learning_rate": 9.165413533834586e-05,
      "loss": 0.9569,
      "step": 12970
    },
    {
      "epoch": 3.7953216374269005,
      "grad_norm": 0.6466187238693237,
      "learning_rate": 9.157059314954052e-05,
      "loss": 0.9089,
      "step": 12980
    },
    {
      "epoch": 3.7982456140350878,
      "grad_norm": 0.8426269888877869,
      "learning_rate": 9.148705096073517e-05,
      "loss": 0.8777,
      "step": 12990
    },
    {
      "epoch": 3.801169590643275,
      "grad_norm": 0.903399646282196,
      "learning_rate": 9.140350877192982e-05,
      "loss": 0.9202,
      "step": 13000
    },
    {
      "epoch": 3.8040935672514617,
      "grad_norm": 0.6621943116188049,
      "learning_rate": 9.131996658312448e-05,
      "loss": 0.8686,
      "step": 13010
    },
    {
      "epoch": 3.807017543859649,
      "grad_norm": 0.8370321989059448,
      "learning_rate": 9.123642439431913e-05,
      "loss": 0.9537,
      "step": 13020
    },
    {
      "epoch": 3.809941520467836,
      "grad_norm": 1.2599706649780273,
      "learning_rate": 9.115288220551378e-05,
      "loss": 0.9984,
      "step": 13030
    },
    {
      "epoch": 3.8128654970760234,
      "grad_norm": 0.9407501220703125,
      "learning_rate": 9.106934001670844e-05,
      "loss": 0.9257,
      "step": 13040
    },
    {
      "epoch": 3.8157894736842106,
      "grad_norm": 0.9420874714851379,
      "learning_rate": 9.098579782790309e-05,
      "loss": 0.885,
      "step": 13050
    },
    {
      "epoch": 3.818713450292398,
      "grad_norm": 0.6638734936714172,
      "learning_rate": 9.090225563909775e-05,
      "loss": 0.9057,
      "step": 13060
    },
    {
      "epoch": 3.821637426900585,
      "grad_norm": 0.7704898715019226,
      "learning_rate": 9.08187134502924e-05,
      "loss": 0.9288,
      "step": 13070
    },
    {
      "epoch": 3.824561403508772,
      "grad_norm": 0.728088915348053,
      "learning_rate": 9.073517126148705e-05,
      "loss": 0.9885,
      "step": 13080
    },
    {
      "epoch": 3.827485380116959,
      "grad_norm": 0.9184361100196838,
      "learning_rate": 9.06516290726817e-05,
      "loss": 1.0023,
      "step": 13090
    },
    {
      "epoch": 3.8304093567251463,
      "grad_norm": 0.7706950902938843,
      "learning_rate": 9.056808688387636e-05,
      "loss": 0.9337,
      "step": 13100
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.9990882873535156,
      "learning_rate": 9.048454469507101e-05,
      "loss": 0.9213,
      "step": 13110
    },
    {
      "epoch": 3.8362573099415203,
      "grad_norm": 0.9872136116027832,
      "learning_rate": 9.040100250626567e-05,
      "loss": 0.9048,
      "step": 13120
    },
    {
      "epoch": 3.8391812865497075,
      "grad_norm": 0.9493791460990906,
      "learning_rate": 9.031746031746032e-05,
      "loss": 0.9335,
      "step": 13130
    },
    {
      "epoch": 3.8421052631578947,
      "grad_norm": 1.1270161867141724,
      "learning_rate": 9.023391812865497e-05,
      "loss": 0.9373,
      "step": 13140
    },
    {
      "epoch": 3.845029239766082,
      "grad_norm": 0.7652223110198975,
      "learning_rate": 9.015037593984963e-05,
      "loss": 0.8854,
      "step": 13150
    },
    {
      "epoch": 3.847953216374269,
      "grad_norm": 0.6643037796020508,
      "learning_rate": 9.006683375104428e-05,
      "loss": 0.9476,
      "step": 13160
    },
    {
      "epoch": 3.8508771929824563,
      "grad_norm": 0.703561544418335,
      "learning_rate": 8.998329156223893e-05,
      "loss": 0.9154,
      "step": 13170
    },
    {
      "epoch": 3.853801169590643,
      "grad_norm": 0.9265167713165283,
      "learning_rate": 8.989974937343359e-05,
      "loss": 0.9274,
      "step": 13180
    },
    {
      "epoch": 3.8567251461988303,
      "grad_norm": 0.9073787331581116,
      "learning_rate": 8.981620718462824e-05,
      "loss": 0.9026,
      "step": 13190
    },
    {
      "epoch": 3.8596491228070176,
      "grad_norm": 0.9802895188331604,
      "learning_rate": 8.97326649958229e-05,
      "loss": 0.9672,
      "step": 13200
    },
    {
      "epoch": 3.8625730994152048,
      "grad_norm": 0.8935441374778748,
      "learning_rate": 8.964912280701755e-05,
      "loss": 0.8686,
      "step": 13210
    },
    {
      "epoch": 3.8654970760233915,
      "grad_norm": 1.0472522974014282,
      "learning_rate": 8.95655806182122e-05,
      "loss": 0.9032,
      "step": 13220
    },
    {
      "epoch": 3.8684210526315788,
      "grad_norm": 0.968239426612854,
      "learning_rate": 8.948203842940685e-05,
      "loss": 0.8376,
      "step": 13230
    },
    {
      "epoch": 3.871345029239766,
      "grad_norm": 0.891904354095459,
      "learning_rate": 8.939849624060151e-05,
      "loss": 0.9328,
      "step": 13240
    },
    {
      "epoch": 3.874269005847953,
      "grad_norm": 0.8683125972747803,
      "learning_rate": 8.931495405179616e-05,
      "loss": 0.9211,
      "step": 13250
    },
    {
      "epoch": 3.8771929824561404,
      "grad_norm": 0.917330801486969,
      "learning_rate": 8.923141186299082e-05,
      "loss": 0.9313,
      "step": 13260
    },
    {
      "epoch": 3.8801169590643276,
      "grad_norm": 0.986976146697998,
      "learning_rate": 8.914786967418547e-05,
      "loss": 0.9677,
      "step": 13270
    },
    {
      "epoch": 3.883040935672515,
      "grad_norm": 0.848884642124176,
      "learning_rate": 8.906432748538012e-05,
      "loss": 0.8552,
      "step": 13280
    },
    {
      "epoch": 3.8859649122807016,
      "grad_norm": 0.8987370729446411,
      "learning_rate": 8.898078529657478e-05,
      "loss": 0.9481,
      "step": 13290
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.8975310921669006,
      "learning_rate": 8.889724310776943e-05,
      "loss": 0.8572,
      "step": 13300
    },
    {
      "epoch": 3.891812865497076,
      "grad_norm": 0.7317846417427063,
      "learning_rate": 8.881370091896408e-05,
      "loss": 0.8878,
      "step": 13310
    },
    {
      "epoch": 3.8947368421052633,
      "grad_norm": 0.7086337208747864,
      "learning_rate": 8.873015873015874e-05,
      "loss": 0.8776,
      "step": 13320
    },
    {
      "epoch": 3.89766081871345,
      "grad_norm": 0.7043942809104919,
      "learning_rate": 8.864661654135339e-05,
      "loss": 0.9214,
      "step": 13330
    },
    {
      "epoch": 3.9005847953216373,
      "grad_norm": 0.6452730298042297,
      "learning_rate": 8.856307435254804e-05,
      "loss": 0.8941,
      "step": 13340
    },
    {
      "epoch": 3.9035087719298245,
      "grad_norm": 0.8934682011604309,
      "learning_rate": 8.84795321637427e-05,
      "loss": 0.9538,
      "step": 13350
    },
    {
      "epoch": 3.9064327485380117,
      "grad_norm": 0.7452316284179688,
      "learning_rate": 8.839598997493734e-05,
      "loss": 0.876,
      "step": 13360
    },
    {
      "epoch": 3.909356725146199,
      "grad_norm": 0.878581702709198,
      "learning_rate": 8.8312447786132e-05,
      "loss": 0.8587,
      "step": 13370
    },
    {
      "epoch": 3.912280701754386,
      "grad_norm": 0.7460280060768127,
      "learning_rate": 8.822890559732664e-05,
      "loss": 0.9027,
      "step": 13380
    },
    {
      "epoch": 3.9152046783625734,
      "grad_norm": 0.6838979721069336,
      "learning_rate": 8.814536340852131e-05,
      "loss": 0.9266,
      "step": 13390
    },
    {
      "epoch": 3.91812865497076,
      "grad_norm": 0.8596288561820984,
      "learning_rate": 8.806182121971595e-05,
      "loss": 0.8878,
      "step": 13400
    },
    {
      "epoch": 3.9210526315789473,
      "grad_norm": 0.8400358557701111,
      "learning_rate": 8.797827903091062e-05,
      "loss": 0.8932,
      "step": 13410
    },
    {
      "epoch": 3.9239766081871346,
      "grad_norm": 0.8881117105484009,
      "learning_rate": 8.789473684210526e-05,
      "loss": 0.982,
      "step": 13420
    },
    {
      "epoch": 3.926900584795322,
      "grad_norm": 0.759926974773407,
      "learning_rate": 8.781119465329992e-05,
      "loss": 0.8713,
      "step": 13430
    },
    {
      "epoch": 3.9298245614035086,
      "grad_norm": 0.9284433126449585,
      "learning_rate": 8.772765246449456e-05,
      "loss": 0.9538,
      "step": 13440
    },
    {
      "epoch": 3.9327485380116958,
      "grad_norm": 0.8314181566238403,
      "learning_rate": 8.764411027568923e-05,
      "loss": 0.9597,
      "step": 13450
    },
    {
      "epoch": 3.935672514619883,
      "grad_norm": 0.7643162608146667,
      "learning_rate": 8.756056808688387e-05,
      "loss": 1.0262,
      "step": 13460
    },
    {
      "epoch": 3.93859649122807,
      "grad_norm": 0.823020875453949,
      "learning_rate": 8.747702589807854e-05,
      "loss": 0.9358,
      "step": 13470
    },
    {
      "epoch": 3.9415204678362574,
      "grad_norm": 0.8407874703407288,
      "learning_rate": 8.739348370927318e-05,
      "loss": 0.9298,
      "step": 13480
    },
    {
      "epoch": 3.9444444444444446,
      "grad_norm": 0.9137330651283264,
      "learning_rate": 8.730994152046785e-05,
      "loss": 0.9175,
      "step": 13490
    },
    {
      "epoch": 3.9473684210526314,
      "grad_norm": 0.8559569120407104,
      "learning_rate": 8.722639933166249e-05,
      "loss": 0.9354,
      "step": 13500
    },
    {
      "epoch": 3.9502923976608186,
      "grad_norm": 0.852657675743103,
      "learning_rate": 8.714285714285715e-05,
      "loss": 0.9274,
      "step": 13510
    },
    {
      "epoch": 3.953216374269006,
      "grad_norm": 0.7617670893669128,
      "learning_rate": 8.705931495405179e-05,
      "loss": 0.9214,
      "step": 13520
    },
    {
      "epoch": 3.956140350877193,
      "grad_norm": 1.0019956827163696,
      "learning_rate": 8.697577276524646e-05,
      "loss": 0.8753,
      "step": 13530
    },
    {
      "epoch": 3.95906432748538,
      "grad_norm": 0.8761691451072693,
      "learning_rate": 8.68922305764411e-05,
      "loss": 0.8823,
      "step": 13540
    },
    {
      "epoch": 3.961988304093567,
      "grad_norm": 0.9767100214958191,
      "learning_rate": 8.680868838763577e-05,
      "loss": 0.9799,
      "step": 13550
    },
    {
      "epoch": 3.9649122807017543,
      "grad_norm": 0.7448477745056152,
      "learning_rate": 8.67251461988304e-05,
      "loss": 0.8157,
      "step": 13560
    },
    {
      "epoch": 3.9678362573099415,
      "grad_norm": 0.7622866630554199,
      "learning_rate": 8.664160401002507e-05,
      "loss": 0.9368,
      "step": 13570
    },
    {
      "epoch": 3.9707602339181287,
      "grad_norm": 0.9529246091842651,
      "learning_rate": 8.655806182121971e-05,
      "loss": 1.0058,
      "step": 13580
    },
    {
      "epoch": 3.973684210526316,
      "grad_norm": 0.894108772277832,
      "learning_rate": 8.647451963241438e-05,
      "loss": 0.9544,
      "step": 13590
    },
    {
      "epoch": 3.976608187134503,
      "grad_norm": 1.0850882530212402,
      "learning_rate": 8.639097744360902e-05,
      "loss": 1.0072,
      "step": 13600
    },
    {
      "epoch": 3.97953216374269,
      "grad_norm": 0.7325168251991272,
      "learning_rate": 8.630743525480369e-05,
      "loss": 0.8406,
      "step": 13610
    },
    {
      "epoch": 3.982456140350877,
      "grad_norm": 0.751126766204834,
      "learning_rate": 8.622389306599833e-05,
      "loss": 0.9475,
      "step": 13620
    },
    {
      "epoch": 3.9853801169590644,
      "grad_norm": 1.0316258668899536,
      "learning_rate": 8.6140350877193e-05,
      "loss": 0.8696,
      "step": 13630
    },
    {
      "epoch": 3.9883040935672516,
      "grad_norm": 0.9279578924179077,
      "learning_rate": 8.605680868838763e-05,
      "loss": 0.8677,
      "step": 13640
    },
    {
      "epoch": 3.9912280701754383,
      "grad_norm": 0.9300860166549683,
      "learning_rate": 8.597326649958229e-05,
      "loss": 0.9699,
      "step": 13650
    },
    {
      "epoch": 3.9941520467836256,
      "grad_norm": 0.7932716012001038,
      "learning_rate": 8.588972431077694e-05,
      "loss": 0.9402,
      "step": 13660
    },
    {
      "epoch": 3.997076023391813,
      "grad_norm": 0.8699155449867249,
      "learning_rate": 8.58061821219716e-05,
      "loss": 0.8162,
      "step": 13670
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.9809001088142395,
      "learning_rate": 8.572263993316625e-05,
      "loss": 0.9387,
      "step": 13680
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.8290194272994995,
      "eval_runtime": 76.2202,
      "eval_samples_per_second": 11.664,
      "eval_steps_per_second": 1.469,
      "step": 13680
    },
    {
      "epoch": 4.002923976608187,
      "grad_norm": 1.082058310508728,
      "learning_rate": 8.56390977443609e-05,
      "loss": 0.8945,
      "step": 13690
    },
    {
      "epoch": 4.005847953216374,
      "grad_norm": 0.655863344669342,
      "learning_rate": 8.555555555555556e-05,
      "loss": 0.838,
      "step": 13700
    },
    {
      "epoch": 4.008771929824562,
      "grad_norm": 0.8115760087966919,
      "learning_rate": 8.547201336675021e-05,
      "loss": 0.8462,
      "step": 13710
    },
    {
      "epoch": 4.011695906432749,
      "grad_norm": 0.8370761871337891,
      "learning_rate": 8.538847117794486e-05,
      "loss": 0.8728,
      "step": 13720
    },
    {
      "epoch": 4.014619883040936,
      "grad_norm": 0.7383832931518555,
      "learning_rate": 8.530492898913952e-05,
      "loss": 0.8665,
      "step": 13730
    },
    {
      "epoch": 4.017543859649122,
      "grad_norm": 0.7040742039680481,
      "learning_rate": 8.522138680033417e-05,
      "loss": 0.8396,
      "step": 13740
    },
    {
      "epoch": 4.02046783625731,
      "grad_norm": 0.6268284320831299,
      "learning_rate": 8.513784461152882e-05,
      "loss": 0.8548,
      "step": 13750
    },
    {
      "epoch": 4.023391812865497,
      "grad_norm": 0.8663976192474365,
      "learning_rate": 8.505430242272348e-05,
      "loss": 0.9087,
      "step": 13760
    },
    {
      "epoch": 4.026315789473684,
      "grad_norm": 0.7036945819854736,
      "learning_rate": 8.497076023391813e-05,
      "loss": 0.9227,
      "step": 13770
    },
    {
      "epoch": 4.029239766081871,
      "grad_norm": 0.8115586042404175,
      "learning_rate": 8.488721804511278e-05,
      "loss": 0.8591,
      "step": 13780
    },
    {
      "epoch": 4.0321637426900585,
      "grad_norm": 0.8629992604255676,
      "learning_rate": 8.480367585630744e-05,
      "loss": 0.9564,
      "step": 13790
    },
    {
      "epoch": 4.035087719298246,
      "grad_norm": 0.8438169956207275,
      "learning_rate": 8.472013366750209e-05,
      "loss": 0.8449,
      "step": 13800
    },
    {
      "epoch": 4.038011695906433,
      "grad_norm": 1.260191798210144,
      "learning_rate": 8.463659147869674e-05,
      "loss": 0.8512,
      "step": 13810
    },
    {
      "epoch": 4.04093567251462,
      "grad_norm": 0.8151761293411255,
      "learning_rate": 8.45530492898914e-05,
      "loss": 0.8735,
      "step": 13820
    },
    {
      "epoch": 4.043859649122807,
      "grad_norm": 1.027880072593689,
      "learning_rate": 8.446950710108605e-05,
      "loss": 0.8473,
      "step": 13830
    },
    {
      "epoch": 4.046783625730994,
      "grad_norm": 0.8110801577568054,
      "learning_rate": 8.43859649122807e-05,
      "loss": 0.8333,
      "step": 13840
    },
    {
      "epoch": 4.049707602339181,
      "grad_norm": 0.987842321395874,
      "learning_rate": 8.430242272347536e-05,
      "loss": 0.8512,
      "step": 13850
    },
    {
      "epoch": 4.052631578947368,
      "grad_norm": 1.1087279319763184,
      "learning_rate": 8.421888053467001e-05,
      "loss": 0.8367,
      "step": 13860
    },
    {
      "epoch": 4.055555555555555,
      "grad_norm": 0.7842984199523926,
      "learning_rate": 8.413533834586467e-05,
      "loss": 0.9196,
      "step": 13870
    },
    {
      "epoch": 4.058479532163743,
      "grad_norm": 0.809722900390625,
      "learning_rate": 8.405179615705932e-05,
      "loss": 0.8829,
      "step": 13880
    },
    {
      "epoch": 4.06140350877193,
      "grad_norm": 0.9608472585678101,
      "learning_rate": 8.396825396825397e-05,
      "loss": 0.7886,
      "step": 13890
    },
    {
      "epoch": 4.064327485380117,
      "grad_norm": 0.8343858122825623,
      "learning_rate": 8.388471177944863e-05,
      "loss": 0.8744,
      "step": 13900
    },
    {
      "epoch": 4.067251461988304,
      "grad_norm": 1.0770399570465088,
      "learning_rate": 8.380116959064328e-05,
      "loss": 0.9385,
      "step": 13910
    },
    {
      "epoch": 4.0701754385964914,
      "grad_norm": 0.8722090721130371,
      "learning_rate": 8.371762740183793e-05,
      "loss": 0.8499,
      "step": 13920
    },
    {
      "epoch": 4.073099415204679,
      "grad_norm": 0.8869257569313049,
      "learning_rate": 8.363408521303259e-05,
      "loss": 0.8779,
      "step": 13930
    },
    {
      "epoch": 4.076023391812866,
      "grad_norm": 0.8204383254051208,
      "learning_rate": 8.355054302422724e-05,
      "loss": 0.9177,
      "step": 13940
    },
    {
      "epoch": 4.078947368421052,
      "grad_norm": 0.9200872778892517,
      "learning_rate": 8.346700083542189e-05,
      "loss": 0.8034,
      "step": 13950
    },
    {
      "epoch": 4.081871345029239,
      "grad_norm": 0.7823057770729065,
      "learning_rate": 8.338345864661655e-05,
      "loss": 0.7724,
      "step": 13960
    },
    {
      "epoch": 4.084795321637427,
      "grad_norm": 0.9426265954971313,
      "learning_rate": 8.32999164578112e-05,
      "loss": 0.8754,
      "step": 13970
    },
    {
      "epoch": 4.087719298245614,
      "grad_norm": 0.7265809774398804,
      "learning_rate": 8.321637426900585e-05,
      "loss": 0.7932,
      "step": 13980
    },
    {
      "epoch": 4.090643274853801,
      "grad_norm": 0.7034643292427063,
      "learning_rate": 8.313283208020051e-05,
      "loss": 0.7978,
      "step": 13990
    },
    {
      "epoch": 4.093567251461988,
      "grad_norm": 0.8749716877937317,
      "learning_rate": 8.304928989139516e-05,
      "loss": 0.9795,
      "step": 14000
    },
    {
      "epoch": 4.0964912280701755,
      "grad_norm": 0.7467492818832397,
      "learning_rate": 8.296574770258981e-05,
      "loss": 0.8976,
      "step": 14010
    },
    {
      "epoch": 4.099415204678363,
      "grad_norm": 0.9279254674911499,
      "learning_rate": 8.288220551378447e-05,
      "loss": 0.9033,
      "step": 14020
    },
    {
      "epoch": 4.10233918128655,
      "grad_norm": 0.7438749074935913,
      "learning_rate": 8.279866332497912e-05,
      "loss": 0.8594,
      "step": 14030
    },
    {
      "epoch": 4.105263157894737,
      "grad_norm": 0.7862396836280823,
      "learning_rate": 8.271512113617376e-05,
      "loss": 0.8569,
      "step": 14040
    },
    {
      "epoch": 4.108187134502924,
      "grad_norm": 0.8780322074890137,
      "learning_rate": 8.263157894736843e-05,
      "loss": 0.8154,
      "step": 14050
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.8075574040412903,
      "learning_rate": 8.254803675856307e-05,
      "loss": 0.9379,
      "step": 14060
    },
    {
      "epoch": 4.114035087719298,
      "grad_norm": 1.0204005241394043,
      "learning_rate": 8.246449456975774e-05,
      "loss": 0.8776,
      "step": 14070
    },
    {
      "epoch": 4.116959064327485,
      "grad_norm": 1.0038259029388428,
      "learning_rate": 8.238095238095238e-05,
      "loss": 0.8197,
      "step": 14080
    },
    {
      "epoch": 4.119883040935672,
      "grad_norm": 0.6393475532531738,
      "learning_rate": 8.229741019214704e-05,
      "loss": 0.8879,
      "step": 14090
    },
    {
      "epoch": 4.12280701754386,
      "grad_norm": 0.7417382597923279,
      "learning_rate": 8.221386800334168e-05,
      "loss": 0.8581,
      "step": 14100
    },
    {
      "epoch": 4.125730994152047,
      "grad_norm": 0.7666485905647278,
      "learning_rate": 8.213032581453635e-05,
      "loss": 0.9261,
      "step": 14110
    },
    {
      "epoch": 4.128654970760234,
      "grad_norm": 0.6297743320465088,
      "learning_rate": 8.204678362573099e-05,
      "loss": 0.8053,
      "step": 14120
    },
    {
      "epoch": 4.131578947368421,
      "grad_norm": 0.8964319229125977,
      "learning_rate": 8.196324143692566e-05,
      "loss": 0.9836,
      "step": 14130
    },
    {
      "epoch": 4.1345029239766085,
      "grad_norm": 0.6763595342636108,
      "learning_rate": 8.18796992481203e-05,
      "loss": 0.8942,
      "step": 14140
    },
    {
      "epoch": 4.137426900584796,
      "grad_norm": 0.7892849445343018,
      "learning_rate": 8.179615705931496e-05,
      "loss": 0.8443,
      "step": 14150
    },
    {
      "epoch": 4.140350877192983,
      "grad_norm": 0.9805187582969666,
      "learning_rate": 8.17126148705096e-05,
      "loss": 0.8621,
      "step": 14160
    },
    {
      "epoch": 4.143274853801169,
      "grad_norm": 0.9062614440917969,
      "learning_rate": 8.162907268170427e-05,
      "loss": 0.8677,
      "step": 14170
    },
    {
      "epoch": 4.146198830409356,
      "grad_norm": 1.0824676752090454,
      "learning_rate": 8.154553049289891e-05,
      "loss": 0.8429,
      "step": 14180
    },
    {
      "epoch": 4.149122807017544,
      "grad_norm": 0.9041227698326111,
      "learning_rate": 8.146198830409358e-05,
      "loss": 0.8598,
      "step": 14190
    },
    {
      "epoch": 4.152046783625731,
      "grad_norm": 0.9032260775566101,
      "learning_rate": 8.137844611528822e-05,
      "loss": 0.7732,
      "step": 14200
    },
    {
      "epoch": 4.154970760233918,
      "grad_norm": 0.7053897976875305,
      "learning_rate": 8.129490392648288e-05,
      "loss": 0.9253,
      "step": 14210
    },
    {
      "epoch": 4.157894736842105,
      "grad_norm": 0.7946193218231201,
      "learning_rate": 8.121136173767752e-05,
      "loss": 0.9127,
      "step": 14220
    },
    {
      "epoch": 4.1608187134502925,
      "grad_norm": 1.0319496393203735,
      "learning_rate": 8.112781954887219e-05,
      "loss": 0.8367,
      "step": 14230
    },
    {
      "epoch": 4.16374269005848,
      "grad_norm": 0.7937541007995605,
      "learning_rate": 8.104427736006683e-05,
      "loss": 0.9113,
      "step": 14240
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 1.065924882888794,
      "learning_rate": 8.09607351712615e-05,
      "loss": 0.9223,
      "step": 14250
    },
    {
      "epoch": 4.169590643274854,
      "grad_norm": 1.0620273351669312,
      "learning_rate": 8.087719298245614e-05,
      "loss": 0.8803,
      "step": 14260
    },
    {
      "epoch": 4.1725146198830405,
      "grad_norm": 0.7766329050064087,
      "learning_rate": 8.07936507936508e-05,
      "loss": 0.8869,
      "step": 14270
    },
    {
      "epoch": 4.175438596491228,
      "grad_norm": 0.8924914002418518,
      "learning_rate": 8.071010860484545e-05,
      "loss": 0.8524,
      "step": 14280
    },
    {
      "epoch": 4.178362573099415,
      "grad_norm": 0.7216430902481079,
      "learning_rate": 8.062656641604011e-05,
      "loss": 0.9877,
      "step": 14290
    },
    {
      "epoch": 4.181286549707602,
      "grad_norm": 0.8800482749938965,
      "learning_rate": 8.054302422723475e-05,
      "loss": 0.8616,
      "step": 14300
    },
    {
      "epoch": 4.184210526315789,
      "grad_norm": 0.952497124671936,
      "learning_rate": 8.045948203842942e-05,
      "loss": 0.8287,
      "step": 14310
    },
    {
      "epoch": 4.187134502923977,
      "grad_norm": 0.8236814141273499,
      "learning_rate": 8.037593984962406e-05,
      "loss": 0.8354,
      "step": 14320
    },
    {
      "epoch": 4.190058479532164,
      "grad_norm": 0.8130792379379272,
      "learning_rate": 8.029239766081871e-05,
      "loss": 0.8156,
      "step": 14330
    },
    {
      "epoch": 4.192982456140351,
      "grad_norm": 1.0849517583847046,
      "learning_rate": 8.020885547201337e-05,
      "loss": 0.84,
      "step": 14340
    },
    {
      "epoch": 4.195906432748538,
      "grad_norm": 0.9138759970664978,
      "learning_rate": 8.012531328320802e-05,
      "loss": 0.9514,
      "step": 14350
    },
    {
      "epoch": 4.1988304093567255,
      "grad_norm": 0.815055251121521,
      "learning_rate": 8.004177109440267e-05,
      "loss": 0.8268,
      "step": 14360
    },
    {
      "epoch": 4.201754385964913,
      "grad_norm": 1.1649324893951416,
      "learning_rate": 7.995822890559733e-05,
      "loss": 0.9123,
      "step": 14370
    },
    {
      "epoch": 4.204678362573099,
      "grad_norm": 0.9582940340042114,
      "learning_rate": 7.987468671679198e-05,
      "loss": 0.8143,
      "step": 14380
    },
    {
      "epoch": 4.207602339181286,
      "grad_norm": 0.8119428157806396,
      "learning_rate": 7.979114452798663e-05,
      "loss": 0.8979,
      "step": 14390
    },
    {
      "epoch": 4.2105263157894735,
      "grad_norm": 0.8149955868721008,
      "learning_rate": 7.970760233918129e-05,
      "loss": 0.8328,
      "step": 14400
    },
    {
      "epoch": 4.213450292397661,
      "grad_norm": 0.9960831999778748,
      "learning_rate": 7.962406015037594e-05,
      "loss": 0.8199,
      "step": 14410
    },
    {
      "epoch": 4.216374269005848,
      "grad_norm": 0.7665348052978516,
      "learning_rate": 7.95405179615706e-05,
      "loss": 0.8228,
      "step": 14420
    },
    {
      "epoch": 4.219298245614035,
      "grad_norm": 0.8714381456375122,
      "learning_rate": 7.945697577276525e-05,
      "loss": 0.8702,
      "step": 14430
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.8793075680732727,
      "learning_rate": 7.93734335839599e-05,
      "loss": 0.8982,
      "step": 14440
    },
    {
      "epoch": 4.2251461988304095,
      "grad_norm": 0.8732748031616211,
      "learning_rate": 7.928989139515455e-05,
      "loss": 0.8796,
      "step": 14450
    },
    {
      "epoch": 4.228070175438597,
      "grad_norm": 0.9012624621391296,
      "learning_rate": 7.920634920634921e-05,
      "loss": 0.8785,
      "step": 14460
    },
    {
      "epoch": 4.230994152046784,
      "grad_norm": 0.8391129374504089,
      "learning_rate": 7.912280701754386e-05,
      "loss": 0.9887,
      "step": 14470
    },
    {
      "epoch": 4.23391812865497,
      "grad_norm": 0.6176772713661194,
      "learning_rate": 7.903926482873852e-05,
      "loss": 0.8553,
      "step": 14480
    },
    {
      "epoch": 4.2368421052631575,
      "grad_norm": 0.8443222641944885,
      "learning_rate": 7.895572263993317e-05,
      "loss": 0.8732,
      "step": 14490
    },
    {
      "epoch": 4.239766081871345,
      "grad_norm": 0.8318837881088257,
      "learning_rate": 7.887218045112782e-05,
      "loss": 0.9431,
      "step": 14500
    },
    {
      "epoch": 4.242690058479532,
      "grad_norm": 1.0031763315200806,
      "learning_rate": 7.878863826232248e-05,
      "loss": 0.8458,
      "step": 14510
    },
    {
      "epoch": 4.245614035087719,
      "grad_norm": 0.7699165344238281,
      "learning_rate": 7.870509607351713e-05,
      "loss": 0.8039,
      "step": 14520
    },
    {
      "epoch": 4.248538011695906,
      "grad_norm": 1.1028578281402588,
      "learning_rate": 7.862155388471178e-05,
      "loss": 0.784,
      "step": 14530
    },
    {
      "epoch": 4.251461988304094,
      "grad_norm": 0.8239331841468811,
      "learning_rate": 7.853801169590644e-05,
      "loss": 0.8677,
      "step": 14540
    },
    {
      "epoch": 4.254385964912281,
      "grad_norm": 0.8326019048690796,
      "learning_rate": 7.845446950710109e-05,
      "loss": 0.8252,
      "step": 14550
    },
    {
      "epoch": 4.257309941520468,
      "grad_norm": 1.0225369930267334,
      "learning_rate": 7.837092731829574e-05,
      "loss": 0.7937,
      "step": 14560
    },
    {
      "epoch": 4.260233918128655,
      "grad_norm": 0.7001851797103882,
      "learning_rate": 7.82873851294904e-05,
      "loss": 0.8593,
      "step": 14570
    },
    {
      "epoch": 4.2631578947368425,
      "grad_norm": 1.0169458389282227,
      "learning_rate": 7.820384294068505e-05,
      "loss": 0.9092,
      "step": 14580
    },
    {
      "epoch": 4.26608187134503,
      "grad_norm": 0.7984572052955627,
      "learning_rate": 7.81203007518797e-05,
      "loss": 0.9533,
      "step": 14590
    },
    {
      "epoch": 4.269005847953216,
      "grad_norm": 0.8402361273765564,
      "learning_rate": 7.803675856307436e-05,
      "loss": 0.8411,
      "step": 14600
    },
    {
      "epoch": 4.271929824561403,
      "grad_norm": 0.9520418047904968,
      "learning_rate": 7.795321637426901e-05,
      "loss": 0.8766,
      "step": 14610
    },
    {
      "epoch": 4.2748538011695905,
      "grad_norm": 0.7739049792289734,
      "learning_rate": 7.786967418546366e-05,
      "loss": 0.8408,
      "step": 14620
    },
    {
      "epoch": 4.277777777777778,
      "grad_norm": 0.7617870569229126,
      "learning_rate": 7.778613199665832e-05,
      "loss": 0.8249,
      "step": 14630
    },
    {
      "epoch": 4.280701754385965,
      "grad_norm": 0.8777617812156677,
      "learning_rate": 7.770258980785297e-05,
      "loss": 0.8762,
      "step": 14640
    },
    {
      "epoch": 4.283625730994152,
      "grad_norm": 1.1840540170669556,
      "learning_rate": 7.761904761904762e-05,
      "loss": 0.7837,
      "step": 14650
    },
    {
      "epoch": 4.286549707602339,
      "grad_norm": 0.8372240662574768,
      "learning_rate": 7.753550543024228e-05,
      "loss": 0.8393,
      "step": 14660
    },
    {
      "epoch": 4.2894736842105265,
      "grad_norm": 1.0120117664337158,
      "learning_rate": 7.745196324143693e-05,
      "loss": 0.8156,
      "step": 14670
    },
    {
      "epoch": 4.292397660818714,
      "grad_norm": 0.9243233799934387,
      "learning_rate": 7.736842105263159e-05,
      "loss": 0.8312,
      "step": 14680
    },
    {
      "epoch": 4.295321637426901,
      "grad_norm": 1.0184413194656372,
      "learning_rate": 7.728487886382624e-05,
      "loss": 0.798,
      "step": 14690
    },
    {
      "epoch": 4.298245614035087,
      "grad_norm": 0.7399880290031433,
      "learning_rate": 7.720133667502089e-05,
      "loss": 0.8899,
      "step": 14700
    },
    {
      "epoch": 4.3011695906432745,
      "grad_norm": 0.9603103995323181,
      "learning_rate": 7.711779448621555e-05,
      "loss": 0.8985,
      "step": 14710
    },
    {
      "epoch": 4.304093567251462,
      "grad_norm": 0.9072576761245728,
      "learning_rate": 7.703425229741019e-05,
      "loss": 0.9979,
      "step": 14720
    },
    {
      "epoch": 4.307017543859649,
      "grad_norm": 0.8412086367607117,
      "learning_rate": 7.695071010860485e-05,
      "loss": 0.8661,
      "step": 14730
    },
    {
      "epoch": 4.309941520467836,
      "grad_norm": 0.8352314233779907,
      "learning_rate": 7.686716791979949e-05,
      "loss": 0.8701,
      "step": 14740
    },
    {
      "epoch": 4.312865497076023,
      "grad_norm": 0.795760452747345,
      "learning_rate": 7.678362573099416e-05,
      "loss": 0.8721,
      "step": 14750
    },
    {
      "epoch": 4.315789473684211,
      "grad_norm": 0.8071960210800171,
      "learning_rate": 7.67000835421888e-05,
      "loss": 0.9147,
      "step": 14760
    },
    {
      "epoch": 4.318713450292398,
      "grad_norm": 0.9957531094551086,
      "learning_rate": 7.661654135338347e-05,
      "loss": 0.9076,
      "step": 14770
    },
    {
      "epoch": 4.321637426900585,
      "grad_norm": 0.8724799752235413,
      "learning_rate": 7.65329991645781e-05,
      "loss": 0.8977,
      "step": 14780
    },
    {
      "epoch": 4.324561403508772,
      "grad_norm": 0.9851358532905579,
      "learning_rate": 7.644945697577277e-05,
      "loss": 0.9077,
      "step": 14790
    },
    {
      "epoch": 4.3274853801169595,
      "grad_norm": 0.9519302248954773,
      "learning_rate": 7.636591478696741e-05,
      "loss": 0.8149,
      "step": 14800
    },
    {
      "epoch": 4.330409356725146,
      "grad_norm": 0.7604851722717285,
      "learning_rate": 7.628237259816208e-05,
      "loss": 0.8089,
      "step": 14810
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.9688961505889893,
      "learning_rate": 7.619883040935672e-05,
      "loss": 0.8364,
      "step": 14820
    },
    {
      "epoch": 4.33625730994152,
      "grad_norm": 0.7204034328460693,
      "learning_rate": 7.611528822055139e-05,
      "loss": 0.8855,
      "step": 14830
    },
    {
      "epoch": 4.3391812865497075,
      "grad_norm": 1.1581405401229858,
      "learning_rate": 7.603174603174603e-05,
      "loss": 0.8705,
      "step": 14840
    },
    {
      "epoch": 4.342105263157895,
      "grad_norm": 0.847998857498169,
      "learning_rate": 7.59482038429407e-05,
      "loss": 0.8921,
      "step": 14850
    },
    {
      "epoch": 4.345029239766082,
      "grad_norm": 1.1501548290252686,
      "learning_rate": 7.586466165413533e-05,
      "loss": 0.8694,
      "step": 14860
    },
    {
      "epoch": 4.347953216374269,
      "grad_norm": 0.933445155620575,
      "learning_rate": 7.578111946533e-05,
      "loss": 0.8944,
      "step": 14870
    },
    {
      "epoch": 4.350877192982456,
      "grad_norm": 0.8828547596931458,
      "learning_rate": 7.569757727652464e-05,
      "loss": 0.8353,
      "step": 14880
    },
    {
      "epoch": 4.353801169590644,
      "grad_norm": 0.9116472005844116,
      "learning_rate": 7.561403508771931e-05,
      "loss": 0.9304,
      "step": 14890
    },
    {
      "epoch": 4.356725146198831,
      "grad_norm": 0.8227487802505493,
      "learning_rate": 7.553049289891395e-05,
      "loss": 0.7989,
      "step": 14900
    },
    {
      "epoch": 4.359649122807017,
      "grad_norm": 1.1202739477157593,
      "learning_rate": 7.544695071010862e-05,
      "loss": 0.9111,
      "step": 14910
    },
    {
      "epoch": 4.362573099415204,
      "grad_norm": 0.8969612121582031,
      "learning_rate": 7.536340852130326e-05,
      "loss": 0.8169,
      "step": 14920
    },
    {
      "epoch": 4.3654970760233915,
      "grad_norm": 1.009762167930603,
      "learning_rate": 7.527986633249792e-05,
      "loss": 0.8896,
      "step": 14930
    },
    {
      "epoch": 4.368421052631579,
      "grad_norm": 0.7857012152671814,
      "learning_rate": 7.519632414369256e-05,
      "loss": 0.9162,
      "step": 14940
    },
    {
      "epoch": 4.371345029239766,
      "grad_norm": 0.714864194393158,
      "learning_rate": 7.511278195488723e-05,
      "loss": 0.8606,
      "step": 14950
    },
    {
      "epoch": 4.374269005847953,
      "grad_norm": 1.036833643913269,
      "learning_rate": 7.502923976608187e-05,
      "loss": 0.9114,
      "step": 14960
    },
    {
      "epoch": 4.37719298245614,
      "grad_norm": 1.0659221410751343,
      "learning_rate": 7.494569757727654e-05,
      "loss": 0.8757,
      "step": 14970
    },
    {
      "epoch": 4.380116959064328,
      "grad_norm": 0.8789182305335999,
      "learning_rate": 7.486215538847118e-05,
      "loss": 0.911,
      "step": 14980
    },
    {
      "epoch": 4.383040935672515,
      "grad_norm": 0.8770009279251099,
      "learning_rate": 7.477861319966584e-05,
      "loss": 0.8686,
      "step": 14990
    },
    {
      "epoch": 4.385964912280702,
      "grad_norm": 1.0611200332641602,
      "learning_rate": 7.469507101086048e-05,
      "loss": 0.8505,
      "step": 15000
    },
    {
      "epoch": 4.388888888888889,
      "grad_norm": 0.7151868939399719,
      "learning_rate": 7.461152882205514e-05,
      "loss": 0.9447,
      "step": 15010
    },
    {
      "epoch": 4.391812865497076,
      "grad_norm": 0.8963977694511414,
      "learning_rate": 7.452798663324979e-05,
      "loss": 0.9192,
      "step": 15020
    },
    {
      "epoch": 4.394736842105263,
      "grad_norm": 0.8535467982292175,
      "learning_rate": 7.444444444444444e-05,
      "loss": 0.8689,
      "step": 15030
    },
    {
      "epoch": 4.39766081871345,
      "grad_norm": 0.7381218671798706,
      "learning_rate": 7.43609022556391e-05,
      "loss": 0.8388,
      "step": 15040
    },
    {
      "epoch": 4.400584795321637,
      "grad_norm": 0.8581511974334717,
      "learning_rate": 7.427736006683375e-05,
      "loss": 0.8494,
      "step": 15050
    },
    {
      "epoch": 4.4035087719298245,
      "grad_norm": 1.3773201704025269,
      "learning_rate": 7.41938178780284e-05,
      "loss": 0.8908,
      "step": 15060
    },
    {
      "epoch": 4.406432748538012,
      "grad_norm": 0.808168888092041,
      "learning_rate": 7.411027568922306e-05,
      "loss": 0.912,
      "step": 15070
    },
    {
      "epoch": 4.409356725146199,
      "grad_norm": 0.7770195603370667,
      "learning_rate": 7.402673350041771e-05,
      "loss": 0.7705,
      "step": 15080
    },
    {
      "epoch": 4.412280701754386,
      "grad_norm": 0.8666848540306091,
      "learning_rate": 7.394319131161236e-05,
      "loss": 0.846,
      "step": 15090
    },
    {
      "epoch": 4.415204678362573,
      "grad_norm": 0.8431124091148376,
      "learning_rate": 7.385964912280702e-05,
      "loss": 0.9295,
      "step": 15100
    },
    {
      "epoch": 4.418128654970761,
      "grad_norm": 0.8006488084793091,
      "learning_rate": 7.377610693400167e-05,
      "loss": 0.8503,
      "step": 15110
    },
    {
      "epoch": 4.421052631578947,
      "grad_norm": 0.7659682631492615,
      "learning_rate": 7.369256474519633e-05,
      "loss": 0.808,
      "step": 15120
    },
    {
      "epoch": 4.423976608187134,
      "grad_norm": 0.7084401845932007,
      "learning_rate": 7.360902255639098e-05,
      "loss": 0.7613,
      "step": 15130
    },
    {
      "epoch": 4.426900584795321,
      "grad_norm": 0.7939043045043945,
      "learning_rate": 7.352548036758563e-05,
      "loss": 0.9302,
      "step": 15140
    },
    {
      "epoch": 4.4298245614035086,
      "grad_norm": 0.7732800245285034,
      "learning_rate": 7.344193817878029e-05,
      "loss": 0.8028,
      "step": 15150
    },
    {
      "epoch": 4.432748538011696,
      "grad_norm": 1.5190924406051636,
      "learning_rate": 7.335839598997494e-05,
      "loss": 0.9104,
      "step": 15160
    },
    {
      "epoch": 4.435672514619883,
      "grad_norm": 0.6730823516845703,
      "learning_rate": 7.327485380116959e-05,
      "loss": 0.8637,
      "step": 15170
    },
    {
      "epoch": 4.43859649122807,
      "grad_norm": 0.7794085144996643,
      "learning_rate": 7.319131161236425e-05,
      "loss": 0.8411,
      "step": 15180
    },
    {
      "epoch": 4.441520467836257,
      "grad_norm": 0.8262421488761902,
      "learning_rate": 7.31077694235589e-05,
      "loss": 0.835,
      "step": 15190
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.9099212884902954,
      "learning_rate": 7.302422723475355e-05,
      "loss": 0.8876,
      "step": 15200
    },
    {
      "epoch": 4.447368421052632,
      "grad_norm": 1.0179100036621094,
      "learning_rate": 7.294068504594821e-05,
      "loss": 0.8597,
      "step": 15210
    },
    {
      "epoch": 4.450292397660819,
      "grad_norm": 1.06798255443573,
      "learning_rate": 7.285714285714286e-05,
      "loss": 0.8449,
      "step": 15220
    },
    {
      "epoch": 4.453216374269006,
      "grad_norm": 1.0563668012619019,
      "learning_rate": 7.277360066833751e-05,
      "loss": 0.8498,
      "step": 15230
    },
    {
      "epoch": 4.456140350877193,
      "grad_norm": 0.8719524145126343,
      "learning_rate": 7.269005847953217e-05,
      "loss": 0.8734,
      "step": 15240
    },
    {
      "epoch": 4.45906432748538,
      "grad_norm": 1.059132695198059,
      "learning_rate": 7.260651629072682e-05,
      "loss": 0.7482,
      "step": 15250
    },
    {
      "epoch": 4.461988304093567,
      "grad_norm": 1.8524171113967896,
      "learning_rate": 7.252297410192147e-05,
      "loss": 0.9268,
      "step": 15260
    },
    {
      "epoch": 4.464912280701754,
      "grad_norm": 0.7422858476638794,
      "learning_rate": 7.243943191311613e-05,
      "loss": 0.811,
      "step": 15270
    },
    {
      "epoch": 4.4678362573099415,
      "grad_norm": 1.0681568384170532,
      "learning_rate": 7.235588972431078e-05,
      "loss": 0.9261,
      "step": 15280
    },
    {
      "epoch": 4.470760233918129,
      "grad_norm": 0.9697324633598328,
      "learning_rate": 7.227234753550543e-05,
      "loss": 0.8529,
      "step": 15290
    },
    {
      "epoch": 4.473684210526316,
      "grad_norm": 0.7053205966949463,
      "learning_rate": 7.218880534670009e-05,
      "loss": 0.8797,
      "step": 15300
    },
    {
      "epoch": 4.476608187134503,
      "grad_norm": 0.845679521560669,
      "learning_rate": 7.210526315789474e-05,
      "loss": 0.8968,
      "step": 15310
    },
    {
      "epoch": 4.47953216374269,
      "grad_norm": 0.7154535055160522,
      "learning_rate": 7.20217209690894e-05,
      "loss": 0.885,
      "step": 15320
    },
    {
      "epoch": 4.482456140350878,
      "grad_norm": 0.8231260180473328,
      "learning_rate": 7.193817878028405e-05,
      "loss": 0.7501,
      "step": 15330
    },
    {
      "epoch": 4.485380116959064,
      "grad_norm": 0.9851712584495544,
      "learning_rate": 7.18546365914787e-05,
      "loss": 0.8997,
      "step": 15340
    },
    {
      "epoch": 4.488304093567251,
      "grad_norm": 0.8075909614562988,
      "learning_rate": 7.177109440267336e-05,
      "loss": 0.7867,
      "step": 15350
    },
    {
      "epoch": 4.491228070175438,
      "grad_norm": 0.8479645252227783,
      "learning_rate": 7.168755221386801e-05,
      "loss": 0.8246,
      "step": 15360
    },
    {
      "epoch": 4.494152046783626,
      "grad_norm": 0.7521189451217651,
      "learning_rate": 7.160401002506266e-05,
      "loss": 0.9169,
      "step": 15370
    },
    {
      "epoch": 4.497076023391813,
      "grad_norm": 0.7111114263534546,
      "learning_rate": 7.152046783625732e-05,
      "loss": 0.8844,
      "step": 15380
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.7537261843681335,
      "learning_rate": 7.143692564745197e-05,
      "loss": 0.7986,
      "step": 15390
    },
    {
      "epoch": 4.502923976608187,
      "grad_norm": 0.8786681294441223,
      "learning_rate": 7.135338345864661e-05,
      "loss": 0.8857,
      "step": 15400
    },
    {
      "epoch": 4.505847953216374,
      "grad_norm": 1.4213266372680664,
      "learning_rate": 7.126984126984128e-05,
      "loss": 0.9305,
      "step": 15410
    },
    {
      "epoch": 4.508771929824562,
      "grad_norm": 1.1005892753601074,
      "learning_rate": 7.118629908103592e-05,
      "loss": 1.0201,
      "step": 15420
    },
    {
      "epoch": 4.511695906432749,
      "grad_norm": 0.8246450424194336,
      "learning_rate": 7.110275689223058e-05,
      "loss": 0.8389,
      "step": 15430
    },
    {
      "epoch": 4.514619883040936,
      "grad_norm": 0.9097051024436951,
      "learning_rate": 7.101921470342522e-05,
      "loss": 0.8583,
      "step": 15440
    },
    {
      "epoch": 4.517543859649123,
      "grad_norm": 1.1271203756332397,
      "learning_rate": 7.093567251461989e-05,
      "loss": 0.8477,
      "step": 15450
    },
    {
      "epoch": 4.52046783625731,
      "grad_norm": 0.7963667511940002,
      "learning_rate": 7.085213032581453e-05,
      "loss": 0.842,
      "step": 15460
    },
    {
      "epoch": 4.523391812865497,
      "grad_norm": 0.7237189412117004,
      "learning_rate": 7.07685881370092e-05,
      "loss": 0.8169,
      "step": 15470
    },
    {
      "epoch": 4.526315789473684,
      "grad_norm": 1.1570173501968384,
      "learning_rate": 7.068504594820384e-05,
      "loss": 0.9276,
      "step": 15480
    },
    {
      "epoch": 4.529239766081871,
      "grad_norm": 0.9892213344573975,
      "learning_rate": 7.06015037593985e-05,
      "loss": 0.813,
      "step": 15490
    },
    {
      "epoch": 4.5321637426900585,
      "grad_norm": 0.7124013304710388,
      "learning_rate": 7.051796157059314e-05,
      "loss": 0.8498,
      "step": 15500
    },
    {
      "epoch": 4.535087719298246,
      "grad_norm": 0.7884992957115173,
      "learning_rate": 7.043441938178781e-05,
      "loss": 0.8387,
      "step": 15510
    },
    {
      "epoch": 4.538011695906433,
      "grad_norm": 1.0773910284042358,
      "learning_rate": 7.035087719298245e-05,
      "loss": 0.8689,
      "step": 15520
    },
    {
      "epoch": 4.54093567251462,
      "grad_norm": 0.858232319355011,
      "learning_rate": 7.026733500417712e-05,
      "loss": 0.9435,
      "step": 15530
    },
    {
      "epoch": 4.543859649122807,
      "grad_norm": 0.7362008094787598,
      "learning_rate": 7.018379281537176e-05,
      "loss": 0.8831,
      "step": 15540
    },
    {
      "epoch": 4.546783625730994,
      "grad_norm": 0.8389219045639038,
      "learning_rate": 7.010025062656643e-05,
      "loss": 0.8945,
      "step": 15550
    },
    {
      "epoch": 4.549707602339181,
      "grad_norm": 0.7229856252670288,
      "learning_rate": 7.001670843776107e-05,
      "loss": 0.8793,
      "step": 15560
    },
    {
      "epoch": 4.552631578947368,
      "grad_norm": 0.7106977701187134,
      "learning_rate": 6.993316624895573e-05,
      "loss": 1.0193,
      "step": 15570
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.799998939037323,
      "learning_rate": 6.984962406015037e-05,
      "loss": 0.9115,
      "step": 15580
    },
    {
      "epoch": 4.558479532163743,
      "grad_norm": 0.9433621764183044,
      "learning_rate": 6.976608187134504e-05,
      "loss": 0.8481,
      "step": 15590
    },
    {
      "epoch": 4.56140350877193,
      "grad_norm": 1.15542733669281,
      "learning_rate": 6.968253968253968e-05,
      "loss": 0.9153,
      "step": 15600
    },
    {
      "epoch": 4.564327485380117,
      "grad_norm": 0.7442049384117126,
      "learning_rate": 6.959899749373435e-05,
      "loss": 0.9347,
      "step": 15610
    },
    {
      "epoch": 4.567251461988304,
      "grad_norm": 0.732308566570282,
      "learning_rate": 6.951545530492899e-05,
      "loss": 0.9273,
      "step": 15620
    },
    {
      "epoch": 4.5701754385964914,
      "grad_norm": 0.7590935826301575,
      "learning_rate": 6.943191311612365e-05,
      "loss": 0.8698,
      "step": 15630
    },
    {
      "epoch": 4.573099415204679,
      "grad_norm": 1.0074026584625244,
      "learning_rate": 6.93483709273183e-05,
      "loss": 0.8966,
      "step": 15640
    },
    {
      "epoch": 4.576023391812866,
      "grad_norm": 1.1263688802719116,
      "learning_rate": 6.926482873851296e-05,
      "loss": 0.8677,
      "step": 15650
    },
    {
      "epoch": 4.578947368421053,
      "grad_norm": 0.7561830878257751,
      "learning_rate": 6.91812865497076e-05,
      "loss": 0.848,
      "step": 15660
    },
    {
      "epoch": 4.581871345029239,
      "grad_norm": 0.9572465419769287,
      "learning_rate": 6.909774436090227e-05,
      "loss": 0.8757,
      "step": 15670
    },
    {
      "epoch": 4.584795321637427,
      "grad_norm": 0.783647358417511,
      "learning_rate": 6.901420217209691e-05,
      "loss": 0.8633,
      "step": 15680
    },
    {
      "epoch": 4.587719298245614,
      "grad_norm": 0.8218799829483032,
      "learning_rate": 6.893065998329156e-05,
      "loss": 0.8304,
      "step": 15690
    },
    {
      "epoch": 4.590643274853801,
      "grad_norm": 0.967135488986969,
      "learning_rate": 6.884711779448621e-05,
      "loss": 0.9398,
      "step": 15700
    },
    {
      "epoch": 4.593567251461988,
      "grad_norm": 0.9171820282936096,
      "learning_rate": 6.876357560568087e-05,
      "loss": 0.8421,
      "step": 15710
    },
    {
      "epoch": 4.5964912280701755,
      "grad_norm": 1.0866214036941528,
      "learning_rate": 6.868003341687552e-05,
      "loss": 0.9068,
      "step": 15720
    },
    {
      "epoch": 4.599415204678363,
      "grad_norm": 0.6250298023223877,
      "learning_rate": 6.859649122807018e-05,
      "loss": 0.912,
      "step": 15730
    },
    {
      "epoch": 4.60233918128655,
      "grad_norm": 0.8046842217445374,
      "learning_rate": 6.851294903926483e-05,
      "loss": 0.8165,
      "step": 15740
    },
    {
      "epoch": 4.605263157894737,
      "grad_norm": 0.8508862853050232,
      "learning_rate": 6.842940685045948e-05,
      "loss": 0.9084,
      "step": 15750
    },
    {
      "epoch": 4.6081871345029235,
      "grad_norm": 0.8410670161247253,
      "learning_rate": 6.834586466165414e-05,
      "loss": 0.9032,
      "step": 15760
    },
    {
      "epoch": 4.611111111111111,
      "grad_norm": 0.8171751499176025,
      "learning_rate": 6.826232247284879e-05,
      "loss": 0.8424,
      "step": 15770
    },
    {
      "epoch": 4.614035087719298,
      "grad_norm": 0.947558581829071,
      "learning_rate": 6.817878028404344e-05,
      "loss": 0.8217,
      "step": 15780
    },
    {
      "epoch": 4.616959064327485,
      "grad_norm": 0.9099254608154297,
      "learning_rate": 6.80952380952381e-05,
      "loss": 0.8204,
      "step": 15790
    },
    {
      "epoch": 4.619883040935672,
      "grad_norm": 0.8858453035354614,
      "learning_rate": 6.801169590643275e-05,
      "loss": 0.9333,
      "step": 15800
    },
    {
      "epoch": 4.62280701754386,
      "grad_norm": 0.7905833721160889,
      "learning_rate": 6.79281537176274e-05,
      "loss": 0.8213,
      "step": 15810
    },
    {
      "epoch": 4.625730994152047,
      "grad_norm": 0.7602816820144653,
      "learning_rate": 6.784461152882206e-05,
      "loss": 0.8594,
      "step": 15820
    },
    {
      "epoch": 4.628654970760234,
      "grad_norm": 0.7998543381690979,
      "learning_rate": 6.776106934001671e-05,
      "loss": 0.8946,
      "step": 15830
    },
    {
      "epoch": 4.631578947368421,
      "grad_norm": 0.7965224385261536,
      "learning_rate": 6.767752715121136e-05,
      "loss": 0.9,
      "step": 15840
    },
    {
      "epoch": 4.6345029239766085,
      "grad_norm": 0.856669545173645,
      "learning_rate": 6.759398496240602e-05,
      "loss": 0.8107,
      "step": 15850
    },
    {
      "epoch": 4.637426900584796,
      "grad_norm": 0.8630746006965637,
      "learning_rate": 6.751044277360067e-05,
      "loss": 0.9041,
      "step": 15860
    },
    {
      "epoch": 4.640350877192983,
      "grad_norm": 0.7799931168556213,
      "learning_rate": 6.742690058479532e-05,
      "loss": 0.8812,
      "step": 15870
    },
    {
      "epoch": 4.643274853801169,
      "grad_norm": 0.7149782180786133,
      "learning_rate": 6.734335839598998e-05,
      "loss": 0.8164,
      "step": 15880
    },
    {
      "epoch": 4.646198830409356,
      "grad_norm": 0.7939034700393677,
      "learning_rate": 6.725981620718463e-05,
      "loss": 0.8952,
      "step": 15890
    },
    {
      "epoch": 4.649122807017544,
      "grad_norm": 0.7481039762496948,
      "learning_rate": 6.717627401837928e-05,
      "loss": 0.819,
      "step": 15900
    },
    {
      "epoch": 4.652046783625731,
      "grad_norm": 1.2667953968048096,
      "learning_rate": 6.709273182957394e-05,
      "loss": 0.8921,
      "step": 15910
    },
    {
      "epoch": 4.654970760233918,
      "grad_norm": 0.9839855432510376,
      "learning_rate": 6.700918964076859e-05,
      "loss": 0.8153,
      "step": 15920
    },
    {
      "epoch": 4.657894736842105,
      "grad_norm": 1.2343034744262695,
      "learning_rate": 6.692564745196325e-05,
      "loss": 0.8662,
      "step": 15930
    },
    {
      "epoch": 4.6608187134502925,
      "grad_norm": 1.059377908706665,
      "learning_rate": 6.68421052631579e-05,
      "loss": 0.9139,
      "step": 15940
    },
    {
      "epoch": 4.66374269005848,
      "grad_norm": 0.7788244485855103,
      "learning_rate": 6.675856307435255e-05,
      "loss": 0.8318,
      "step": 15950
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 1.1623882055282593,
      "learning_rate": 6.66750208855472e-05,
      "loss": 0.9031,
      "step": 15960
    },
    {
      "epoch": 4.669590643274854,
      "grad_norm": 0.8921371698379517,
      "learning_rate": 6.659147869674186e-05,
      "loss": 0.8683,
      "step": 15970
    },
    {
      "epoch": 4.6725146198830405,
      "grad_norm": 0.8196752071380615,
      "learning_rate": 6.650793650793651e-05,
      "loss": 0.8934,
      "step": 15980
    },
    {
      "epoch": 4.675438596491228,
      "grad_norm": 0.8343624472618103,
      "learning_rate": 6.642439431913117e-05,
      "loss": 0.9384,
      "step": 15990
    },
    {
      "epoch": 4.678362573099415,
      "grad_norm": 0.9692774415016174,
      "learning_rate": 6.634085213032582e-05,
      "loss": 0.802,
      "step": 16000
    },
    {
      "epoch": 4.681286549707602,
      "grad_norm": 0.7432109713554382,
      "learning_rate": 6.625730994152047e-05,
      "loss": 0.8969,
      "step": 16010
    },
    {
      "epoch": 4.684210526315789,
      "grad_norm": 1.093484878540039,
      "learning_rate": 6.617376775271513e-05,
      "loss": 0.9018,
      "step": 16020
    },
    {
      "epoch": 4.687134502923977,
      "grad_norm": 1.0027499198913574,
      "learning_rate": 6.609022556390978e-05,
      "loss": 0.8885,
      "step": 16030
    },
    {
      "epoch": 4.690058479532164,
      "grad_norm": 0.9452217817306519,
      "learning_rate": 6.600668337510443e-05,
      "loss": 0.8654,
      "step": 16040
    },
    {
      "epoch": 4.692982456140351,
      "grad_norm": 0.9452949166297913,
      "learning_rate": 6.592314118629909e-05,
      "loss": 0.9797,
      "step": 16050
    },
    {
      "epoch": 4.695906432748538,
      "grad_norm": 1.0704002380371094,
      "learning_rate": 6.583959899749374e-05,
      "loss": 0.8142,
      "step": 16060
    },
    {
      "epoch": 4.6988304093567255,
      "grad_norm": 1.0801178216934204,
      "learning_rate": 6.57560568086884e-05,
      "loss": 0.8412,
      "step": 16070
    },
    {
      "epoch": 4.701754385964913,
      "grad_norm": 0.7644042372703552,
      "learning_rate": 6.567251461988303e-05,
      "loss": 0.8382,
      "step": 16080
    },
    {
      "epoch": 4.7046783625731,
      "grad_norm": 1.02506685256958,
      "learning_rate": 6.55889724310777e-05,
      "loss": 0.8279,
      "step": 16090
    },
    {
      "epoch": 4.707602339181286,
      "grad_norm": 1.0312858819961548,
      "learning_rate": 6.550543024227234e-05,
      "loss": 0.9145,
      "step": 16100
    },
    {
      "epoch": 4.7105263157894735,
      "grad_norm": 0.969837486743927,
      "learning_rate": 6.542188805346701e-05,
      "loss": 0.8831,
      "step": 16110
    },
    {
      "epoch": 4.713450292397661,
      "grad_norm": 0.8678383827209473,
      "learning_rate": 6.533834586466165e-05,
      "loss": 0.9135,
      "step": 16120
    },
    {
      "epoch": 4.716374269005848,
      "grad_norm": 0.7725731134414673,
      "learning_rate": 6.525480367585632e-05,
      "loss": 0.8983,
      "step": 16130
    },
    {
      "epoch": 4.719298245614035,
      "grad_norm": 0.9163436889648438,
      "learning_rate": 6.517126148705096e-05,
      "loss": 0.9165,
      "step": 16140
    },
    {
      "epoch": 4.722222222222222,
      "grad_norm": 0.905708372592926,
      "learning_rate": 6.508771929824562e-05,
      "loss": 0.8743,
      "step": 16150
    },
    {
      "epoch": 4.7251461988304095,
      "grad_norm": 0.7965009212493896,
      "learning_rate": 6.500417710944026e-05,
      "loss": 0.8501,
      "step": 16160
    },
    {
      "epoch": 4.728070175438597,
      "grad_norm": 0.9041869044303894,
      "learning_rate": 6.492063492063493e-05,
      "loss": 0.9024,
      "step": 16170
    },
    {
      "epoch": 4.730994152046784,
      "grad_norm": 0.6849552392959595,
      "learning_rate": 6.483709273182957e-05,
      "loss": 0.835,
      "step": 16180
    },
    {
      "epoch": 4.73391812865497,
      "grad_norm": 0.8387993574142456,
      "learning_rate": 6.475355054302424e-05,
      "loss": 0.7644,
      "step": 16190
    },
    {
      "epoch": 4.7368421052631575,
      "grad_norm": 0.9991590976715088,
      "learning_rate": 6.467000835421888e-05,
      "loss": 0.8469,
      "step": 16200
    },
    {
      "epoch": 4.739766081871345,
      "grad_norm": 0.7325652241706848,
      "learning_rate": 6.458646616541354e-05,
      "loss": 0.8435,
      "step": 16210
    },
    {
      "epoch": 4.742690058479532,
      "grad_norm": 0.8463453650474548,
      "learning_rate": 6.450292397660818e-05,
      "loss": 0.9001,
      "step": 16220
    },
    {
      "epoch": 4.745614035087719,
      "grad_norm": 0.7589909434318542,
      "learning_rate": 6.441938178780285e-05,
      "loss": 0.8922,
      "step": 16230
    },
    {
      "epoch": 4.748538011695906,
      "grad_norm": 1.1386665105819702,
      "learning_rate": 6.433583959899749e-05,
      "loss": 0.9577,
      "step": 16240
    },
    {
      "epoch": 4.751461988304094,
      "grad_norm": 0.8324999213218689,
      "learning_rate": 6.425229741019216e-05,
      "loss": 0.8149,
      "step": 16250
    },
    {
      "epoch": 4.754385964912281,
      "grad_norm": 0.6891366839408875,
      "learning_rate": 6.41687552213868e-05,
      "loss": 0.8784,
      "step": 16260
    },
    {
      "epoch": 4.757309941520468,
      "grad_norm": 0.846798837184906,
      "learning_rate": 6.408521303258146e-05,
      "loss": 0.8963,
      "step": 16270
    },
    {
      "epoch": 4.760233918128655,
      "grad_norm": 0.7721854448318481,
      "learning_rate": 6.40016708437761e-05,
      "loss": 0.7961,
      "step": 16280
    },
    {
      "epoch": 4.7631578947368425,
      "grad_norm": 0.8806120753288269,
      "learning_rate": 6.391812865497077e-05,
      "loss": 0.9169,
      "step": 16290
    },
    {
      "epoch": 4.76608187134503,
      "grad_norm": 0.8505751490592957,
      "learning_rate": 6.383458646616541e-05,
      "loss": 0.9487,
      "step": 16300
    },
    {
      "epoch": 4.769005847953216,
      "grad_norm": 0.8803578615188599,
      "learning_rate": 6.375104427736008e-05,
      "loss": 0.8363,
      "step": 16310
    },
    {
      "epoch": 4.771929824561403,
      "grad_norm": 1.0522156953811646,
      "learning_rate": 6.366750208855472e-05,
      "loss": 0.8613,
      "step": 16320
    },
    {
      "epoch": 4.7748538011695905,
      "grad_norm": 0.7774186730384827,
      "learning_rate": 6.358395989974939e-05,
      "loss": 0.855,
      "step": 16330
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 1.0289534330368042,
      "learning_rate": 6.350041771094403e-05,
      "loss": 0.9064,
      "step": 16340
    },
    {
      "epoch": 4.780701754385965,
      "grad_norm": 0.8764007091522217,
      "learning_rate": 6.341687552213869e-05,
      "loss": 0.8956,
      "step": 16350
    },
    {
      "epoch": 4.783625730994152,
      "grad_norm": 0.832737922668457,
      "learning_rate": 6.333333333333333e-05,
      "loss": 0.7954,
      "step": 16360
    },
    {
      "epoch": 4.786549707602339,
      "grad_norm": 0.9845125675201416,
      "learning_rate": 6.324979114452799e-05,
      "loss": 0.8978,
      "step": 16370
    },
    {
      "epoch": 4.7894736842105265,
      "grad_norm": 0.8679424524307251,
      "learning_rate": 6.316624895572264e-05,
      "loss": 0.8525,
      "step": 16380
    },
    {
      "epoch": 4.792397660818714,
      "grad_norm": 0.973105251789093,
      "learning_rate": 6.308270676691729e-05,
      "loss": 0.8234,
      "step": 16390
    },
    {
      "epoch": 4.7953216374269,
      "grad_norm": 0.907598614692688,
      "learning_rate": 6.299916457811195e-05,
      "loss": 0.8581,
      "step": 16400
    },
    {
      "epoch": 4.798245614035087,
      "grad_norm": 0.7073090672492981,
      "learning_rate": 6.29156223893066e-05,
      "loss": 0.8882,
      "step": 16410
    },
    {
      "epoch": 4.8011695906432745,
      "grad_norm": 0.8468961119651794,
      "learning_rate": 6.283208020050125e-05,
      "loss": 0.9465,
      "step": 16420
    },
    {
      "epoch": 4.804093567251462,
      "grad_norm": 0.8305013179779053,
      "learning_rate": 6.274853801169591e-05,
      "loss": 0.9655,
      "step": 16430
    },
    {
      "epoch": 4.807017543859649,
      "grad_norm": 0.8169237375259399,
      "learning_rate": 6.266499582289056e-05,
      "loss": 0.8403,
      "step": 16440
    },
    {
      "epoch": 4.809941520467836,
      "grad_norm": 0.923429548740387,
      "learning_rate": 6.258145363408521e-05,
      "loss": 0.9181,
      "step": 16450
    },
    {
      "epoch": 4.812865497076023,
      "grad_norm": 0.837090790271759,
      "learning_rate": 6.249791144527987e-05,
      "loss": 0.8766,
      "step": 16460
    },
    {
      "epoch": 4.815789473684211,
      "grad_norm": 0.8905428647994995,
      "learning_rate": 6.241436925647452e-05,
      "loss": 0.8686,
      "step": 16470
    },
    {
      "epoch": 4.818713450292398,
      "grad_norm": 1.1517205238342285,
      "learning_rate": 6.233082706766917e-05,
      "loss": 0.959,
      "step": 16480
    },
    {
      "epoch": 4.821637426900585,
      "grad_norm": 0.8421387076377869,
      "learning_rate": 6.224728487886383e-05,
      "loss": 0.8383,
      "step": 16490
    },
    {
      "epoch": 4.824561403508772,
      "grad_norm": 1.0177499055862427,
      "learning_rate": 6.216374269005848e-05,
      "loss": 0.8398,
      "step": 16500
    },
    {
      "epoch": 4.8274853801169595,
      "grad_norm": 1.0242849588394165,
      "learning_rate": 6.208020050125313e-05,
      "loss": 0.8749,
      "step": 16510
    },
    {
      "epoch": 4.830409356725146,
      "grad_norm": 0.8925773501396179,
      "learning_rate": 6.199665831244779e-05,
      "loss": 0.8371,
      "step": 16520
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 0.834069013595581,
      "learning_rate": 6.191311612364244e-05,
      "loss": 0.8869,
      "step": 16530
    },
    {
      "epoch": 4.83625730994152,
      "grad_norm": 0.9153419733047485,
      "learning_rate": 6.18295739348371e-05,
      "loss": 0.857,
      "step": 16540
    },
    {
      "epoch": 4.8391812865497075,
      "grad_norm": 0.837776243686676,
      "learning_rate": 6.174603174603175e-05,
      "loss": 0.8766,
      "step": 16550
    },
    {
      "epoch": 4.842105263157895,
      "grad_norm": 0.7913985252380371,
      "learning_rate": 6.16624895572264e-05,
      "loss": 0.8266,
      "step": 16560
    },
    {
      "epoch": 4.845029239766082,
      "grad_norm": 0.8458620309829712,
      "learning_rate": 6.157894736842106e-05,
      "loss": 0.8689,
      "step": 16570
    },
    {
      "epoch": 4.847953216374269,
      "grad_norm": 0.7261262536048889,
      "learning_rate": 6.149540517961571e-05,
      "loss": 0.8349,
      "step": 16580
    },
    {
      "epoch": 4.850877192982456,
      "grad_norm": 0.742978036403656,
      "learning_rate": 6.141186299081036e-05,
      "loss": 0.8339,
      "step": 16590
    },
    {
      "epoch": 4.853801169590644,
      "grad_norm": 1.008177638053894,
      "learning_rate": 6.132832080200502e-05,
      "loss": 0.8814,
      "step": 16600
    },
    {
      "epoch": 4.856725146198831,
      "grad_norm": 0.9598297476768494,
      "learning_rate": 6.124477861319967e-05,
      "loss": 0.8971,
      "step": 16610
    },
    {
      "epoch": 4.859649122807017,
      "grad_norm": 0.7421573996543884,
      "learning_rate": 6.116123642439432e-05,
      "loss": 0.8358,
      "step": 16620
    },
    {
      "epoch": 4.862573099415204,
      "grad_norm": 1.0619145631790161,
      "learning_rate": 6.107769423558898e-05,
      "loss": 0.901,
      "step": 16630
    },
    {
      "epoch": 4.8654970760233915,
      "grad_norm": 0.9426355957984924,
      "learning_rate": 6.0994152046783624e-05,
      "loss": 0.8603,
      "step": 16640
    },
    {
      "epoch": 4.868421052631579,
      "grad_norm": 0.9702262282371521,
      "learning_rate": 6.0910609857978284e-05,
      "loss": 0.9077,
      "step": 16650
    },
    {
      "epoch": 4.871345029239766,
      "grad_norm": 1.1878961324691772,
      "learning_rate": 6.082706766917293e-05,
      "loss": 0.8401,
      "step": 16660
    },
    {
      "epoch": 4.874269005847953,
      "grad_norm": 0.9066376090049744,
      "learning_rate": 6.074352548036759e-05,
      "loss": 0.8855,
      "step": 16670
    },
    {
      "epoch": 4.87719298245614,
      "grad_norm": 0.6483401656150818,
      "learning_rate": 6.065998329156224e-05,
      "loss": 0.8664,
      "step": 16680
    },
    {
      "epoch": 4.880116959064328,
      "grad_norm": 0.7522289156913757,
      "learning_rate": 6.05764411027569e-05,
      "loss": 0.8926,
      "step": 16690
    },
    {
      "epoch": 4.883040935672515,
      "grad_norm": 0.8826004266738892,
      "learning_rate": 6.0492898913951545e-05,
      "loss": 0.8235,
      "step": 16700
    },
    {
      "epoch": 4.885964912280702,
      "grad_norm": 0.7545949816703796,
      "learning_rate": 6.0409356725146205e-05,
      "loss": 0.799,
      "step": 16710
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.7456924915313721,
      "learning_rate": 6.032581453634085e-05,
      "loss": 0.8677,
      "step": 16720
    },
    {
      "epoch": 4.8918128654970765,
      "grad_norm": 0.6297284364700317,
      "learning_rate": 6.024227234753551e-05,
      "loss": 0.796,
      "step": 16730
    },
    {
      "epoch": 4.894736842105263,
      "grad_norm": 0.7120101451873779,
      "learning_rate": 6.015873015873016e-05,
      "loss": 0.7817,
      "step": 16740
    },
    {
      "epoch": 4.89766081871345,
      "grad_norm": 0.9119361042976379,
      "learning_rate": 6.007518796992482e-05,
      "loss": 0.8918,
      "step": 16750
    },
    {
      "epoch": 4.900584795321637,
      "grad_norm": 0.8225650787353516,
      "learning_rate": 5.9991645781119466e-05,
      "loss": 0.9291,
      "step": 16760
    },
    {
      "epoch": 4.9035087719298245,
      "grad_norm": 0.9636386036872864,
      "learning_rate": 5.9908103592314126e-05,
      "loss": 0.7941,
      "step": 16770
    },
    {
      "epoch": 4.906432748538012,
      "grad_norm": 0.702053964138031,
      "learning_rate": 5.982456140350877e-05,
      "loss": 0.8378,
      "step": 16780
    },
    {
      "epoch": 4.909356725146199,
      "grad_norm": 0.8396416902542114,
      "learning_rate": 5.974101921470343e-05,
      "loss": 0.937,
      "step": 16790
    },
    {
      "epoch": 4.912280701754386,
      "grad_norm": 0.7804358005523682,
      "learning_rate": 5.965747702589808e-05,
      "loss": 0.8178,
      "step": 16800
    },
    {
      "epoch": 4.915204678362573,
      "grad_norm": 1.1684727668762207,
      "learning_rate": 5.957393483709274e-05,
      "loss": 0.8968,
      "step": 16810
    },
    {
      "epoch": 4.918128654970761,
      "grad_norm": 1.055420994758606,
      "learning_rate": 5.9490392648287387e-05,
      "loss": 0.9095,
      "step": 16820
    },
    {
      "epoch": 4.921052631578947,
      "grad_norm": 0.8004031181335449,
      "learning_rate": 5.940685045948205e-05,
      "loss": 0.8486,
      "step": 16830
    },
    {
      "epoch": 4.923976608187134,
      "grad_norm": 0.7033897638320923,
      "learning_rate": 5.9323308270676694e-05,
      "loss": 0.8578,
      "step": 16840
    },
    {
      "epoch": 4.926900584795321,
      "grad_norm": 0.7584814429283142,
      "learning_rate": 5.9239766081871354e-05,
      "loss": 0.9128,
      "step": 16850
    },
    {
      "epoch": 4.9298245614035086,
      "grad_norm": 1.0405282974243164,
      "learning_rate": 5.9156223893066e-05,
      "loss": 0.9094,
      "step": 16860
    },
    {
      "epoch": 4.932748538011696,
      "grad_norm": 0.9724539518356323,
      "learning_rate": 5.9072681704260654e-05,
      "loss": 0.8269,
      "step": 16870
    },
    {
      "epoch": 4.935672514619883,
      "grad_norm": 0.8475661277770996,
      "learning_rate": 5.898913951545531e-05,
      "loss": 0.8172,
      "step": 16880
    },
    {
      "epoch": 4.93859649122807,
      "grad_norm": 1.0773956775665283,
      "learning_rate": 5.890559732664996e-05,
      "loss": 0.8736,
      "step": 16890
    },
    {
      "epoch": 4.941520467836257,
      "grad_norm": 0.8660002946853638,
      "learning_rate": 5.8822055137844615e-05,
      "loss": 0.9021,
      "step": 16900
    },
    {
      "epoch": 4.944444444444445,
      "grad_norm": 0.9275490641593933,
      "learning_rate": 5.873851294903927e-05,
      "loss": 0.8625,
      "step": 16910
    },
    {
      "epoch": 4.947368421052632,
      "grad_norm": 0.8590260744094849,
      "learning_rate": 5.865497076023392e-05,
      "loss": 0.8545,
      "step": 16920
    },
    {
      "epoch": 4.950292397660819,
      "grad_norm": 0.7795705199241638,
      "learning_rate": 5.8571428571428575e-05,
      "loss": 0.8496,
      "step": 16930
    },
    {
      "epoch": 4.953216374269006,
      "grad_norm": 1.022971272468567,
      "learning_rate": 5.848788638262322e-05,
      "loss": 0.9174,
      "step": 16940
    },
    {
      "epoch": 4.956140350877193,
      "grad_norm": 0.8683142066001892,
      "learning_rate": 5.840434419381788e-05,
      "loss": 0.9331,
      "step": 16950
    },
    {
      "epoch": 4.95906432748538,
      "grad_norm": 1.0482912063598633,
      "learning_rate": 5.832080200501253e-05,
      "loss": 0.9928,
      "step": 16960
    },
    {
      "epoch": 4.961988304093567,
      "grad_norm": 0.8858009576797485,
      "learning_rate": 5.823725981620719e-05,
      "loss": 0.9773,
      "step": 16970
    },
    {
      "epoch": 4.964912280701754,
      "grad_norm": 1.046964168548584,
      "learning_rate": 5.8153717627401836e-05,
      "loss": 0.8997,
      "step": 16980
    },
    {
      "epoch": 4.9678362573099415,
      "grad_norm": 0.8079310059547424,
      "learning_rate": 5.8070175438596496e-05,
      "loss": 0.8854,
      "step": 16990
    },
    {
      "epoch": 4.970760233918129,
      "grad_norm": 0.8484880924224854,
      "learning_rate": 5.798663324979114e-05,
      "loss": 0.8772,
      "step": 17000
    },
    {
      "epoch": 4.973684210526316,
      "grad_norm": 0.8837684392929077,
      "learning_rate": 5.79030910609858e-05,
      "loss": 0.8797,
      "step": 17010
    },
    {
      "epoch": 4.976608187134503,
      "grad_norm": 0.7334867715835571,
      "learning_rate": 5.781954887218045e-05,
      "loss": 0.858,
      "step": 17020
    },
    {
      "epoch": 4.97953216374269,
      "grad_norm": 0.9505625367164612,
      "learning_rate": 5.773600668337511e-05,
      "loss": 0.8744,
      "step": 17030
    },
    {
      "epoch": 4.982456140350877,
      "grad_norm": 0.7638054490089417,
      "learning_rate": 5.765246449456976e-05,
      "loss": 0.9183,
      "step": 17040
    },
    {
      "epoch": 4.985380116959064,
      "grad_norm": 0.6801219582557678,
      "learning_rate": 5.756892230576442e-05,
      "loss": 0.8296,
      "step": 17050
    },
    {
      "epoch": 4.988304093567251,
      "grad_norm": 0.7139809131622314,
      "learning_rate": 5.7485380116959064e-05,
      "loss": 0.8473,
      "step": 17060
    },
    {
      "epoch": 4.991228070175438,
      "grad_norm": 0.830201268196106,
      "learning_rate": 5.7401837928153724e-05,
      "loss": 0.8575,
      "step": 17070
    },
    {
      "epoch": 4.994152046783626,
      "grad_norm": 1.1650892496109009,
      "learning_rate": 5.731829573934837e-05,
      "loss": 0.8232,
      "step": 17080
    },
    {
      "epoch": 4.997076023391813,
      "grad_norm": 0.6951181888580322,
      "learning_rate": 5.723475355054303e-05,
      "loss": 0.8302,
      "step": 17090
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.3747084140777588,
      "learning_rate": 5.715121136173768e-05,
      "loss": 0.8595,
      "step": 17100
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7967675924301147,
      "eval_runtime": 76.2545,
      "eval_samples_per_second": 11.658,
      "eval_steps_per_second": 1.469,
      "step": 17100
    },
    {
      "epoch": 5.002923976608187,
      "grad_norm": 0.9785047769546509,
      "learning_rate": 5.706766917293234e-05,
      "loss": 0.8731,
      "step": 17110
    },
    {
      "epoch": 5.005847953216374,
      "grad_norm": 0.7275197505950928,
      "learning_rate": 5.6984126984126985e-05,
      "loss": 0.8355,
      "step": 17120
    },
    {
      "epoch": 5.008771929824562,
      "grad_norm": 0.7973552346229553,
      "learning_rate": 5.6900584795321645e-05,
      "loss": 0.8145,
      "step": 17130
    },
    {
      "epoch": 5.011695906432749,
      "grad_norm": 1.083949327468872,
      "learning_rate": 5.681704260651629e-05,
      "loss": 0.812,
      "step": 17140
    },
    {
      "epoch": 5.014619883040936,
      "grad_norm": 1.0916881561279297,
      "learning_rate": 5.673350041771095e-05,
      "loss": 0.8576,
      "step": 17150
    },
    {
      "epoch": 5.017543859649122,
      "grad_norm": 1.0851116180419922,
      "learning_rate": 5.66499582289056e-05,
      "loss": 0.7936,
      "step": 17160
    },
    {
      "epoch": 5.02046783625731,
      "grad_norm": 0.8302013874053955,
      "learning_rate": 5.656641604010026e-05,
      "loss": 0.8862,
      "step": 17170
    },
    {
      "epoch": 5.023391812865497,
      "grad_norm": 1.0417790412902832,
      "learning_rate": 5.6482873851294906e-05,
      "loss": 0.7939,
      "step": 17180
    },
    {
      "epoch": 5.026315789473684,
      "grad_norm": 0.6821114420890808,
      "learning_rate": 5.6399331662489566e-05,
      "loss": 0.8712,
      "step": 17190
    },
    {
      "epoch": 5.029239766081871,
      "grad_norm": 0.8355531692504883,
      "learning_rate": 5.631578947368421e-05,
      "loss": 0.7708,
      "step": 17200
    },
    {
      "epoch": 5.0321637426900585,
      "grad_norm": 0.8396860361099243,
      "learning_rate": 5.6232247284878866e-05,
      "loss": 0.7612,
      "step": 17210
    },
    {
      "epoch": 5.035087719298246,
      "grad_norm": 0.7019633054733276,
      "learning_rate": 5.614870509607352e-05,
      "loss": 0.7583,
      "step": 17220
    },
    {
      "epoch": 5.038011695906433,
      "grad_norm": 0.951296865940094,
      "learning_rate": 5.606516290726817e-05,
      "loss": 0.8243,
      "step": 17230
    },
    {
      "epoch": 5.04093567251462,
      "grad_norm": 0.8403428792953491,
      "learning_rate": 5.598162071846283e-05,
      "loss": 0.8245,
      "step": 17240
    },
    {
      "epoch": 5.043859649122807,
      "grad_norm": 0.7983550429344177,
      "learning_rate": 5.589807852965748e-05,
      "loss": 0.7614,
      "step": 17250
    },
    {
      "epoch": 5.046783625730994,
      "grad_norm": 0.958479642868042,
      "learning_rate": 5.5814536340852134e-05,
      "loss": 0.844,
      "step": 17260
    },
    {
      "epoch": 5.049707602339181,
      "grad_norm": 0.8904034495353699,
      "learning_rate": 5.573099415204679e-05,
      "loss": 0.8478,
      "step": 17270
    },
    {
      "epoch": 5.052631578947368,
      "grad_norm": 0.9148610234260559,
      "learning_rate": 5.5647451963241434e-05,
      "loss": 0.852,
      "step": 17280
    },
    {
      "epoch": 5.055555555555555,
      "grad_norm": 0.8791390657424927,
      "learning_rate": 5.5563909774436094e-05,
      "loss": 0.8469,
      "step": 17290
    },
    {
      "epoch": 5.058479532163743,
      "grad_norm": 0.8927731513977051,
      "learning_rate": 5.548036758563074e-05,
      "loss": 0.9062,
      "step": 17300
    },
    {
      "epoch": 5.06140350877193,
      "grad_norm": 0.9148722290992737,
      "learning_rate": 5.53968253968254e-05,
      "loss": 0.8712,
      "step": 17310
    },
    {
      "epoch": 5.064327485380117,
      "grad_norm": 1.0006697177886963,
      "learning_rate": 5.531328320802005e-05,
      "loss": 0.9165,
      "step": 17320
    },
    {
      "epoch": 5.067251461988304,
      "grad_norm": 0.7025697231292725,
      "learning_rate": 5.522974101921471e-05,
      "loss": 0.8006,
      "step": 17330
    },
    {
      "epoch": 5.0701754385964914,
      "grad_norm": 0.99513840675354,
      "learning_rate": 5.5146198830409355e-05,
      "loss": 0.8576,
      "step": 17340
    },
    {
      "epoch": 5.073099415204679,
      "grad_norm": 0.8557503819465637,
      "learning_rate": 5.5062656641604015e-05,
      "loss": 0.8933,
      "step": 17350
    },
    {
      "epoch": 5.076023391812866,
      "grad_norm": 1.0263317823410034,
      "learning_rate": 5.497911445279866e-05,
      "loss": 0.7896,
      "step": 17360
    },
    {
      "epoch": 5.078947368421052,
      "grad_norm": 0.9161487221717834,
      "learning_rate": 5.489557226399332e-05,
      "loss": 0.8209,
      "step": 17370
    },
    {
      "epoch": 5.081871345029239,
      "grad_norm": 0.9900037050247192,
      "learning_rate": 5.481203007518797e-05,
      "loss": 0.9111,
      "step": 17380
    },
    {
      "epoch": 5.084795321637427,
      "grad_norm": 0.8272072672843933,
      "learning_rate": 5.472848788638263e-05,
      "loss": 0.7529,
      "step": 17390
    },
    {
      "epoch": 5.087719298245614,
      "grad_norm": 0.7892258167266846,
      "learning_rate": 5.4644945697577276e-05,
      "loss": 0.8382,
      "step": 17400
    },
    {
      "epoch": 5.090643274853801,
      "grad_norm": 0.9802411198616028,
      "learning_rate": 5.4561403508771936e-05,
      "loss": 0.8306,
      "step": 17410
    },
    {
      "epoch": 5.093567251461988,
      "grad_norm": 0.8644387722015381,
      "learning_rate": 5.447786131996658e-05,
      "loss": 0.8556,
      "step": 17420
    },
    {
      "epoch": 5.0964912280701755,
      "grad_norm": 1.155194878578186,
      "learning_rate": 5.439431913116124e-05,
      "loss": 0.8166,
      "step": 17430
    },
    {
      "epoch": 5.099415204678363,
      "grad_norm": 0.9028366208076477,
      "learning_rate": 5.431077694235589e-05,
      "loss": 0.7994,
      "step": 17440
    },
    {
      "epoch": 5.10233918128655,
      "grad_norm": 0.9354451894760132,
      "learning_rate": 5.422723475355055e-05,
      "loss": 0.8459,
      "step": 17450
    },
    {
      "epoch": 5.105263157894737,
      "grad_norm": 0.9715179204940796,
      "learning_rate": 5.41436925647452e-05,
      "loss": 0.874,
      "step": 17460
    },
    {
      "epoch": 5.108187134502924,
      "grad_norm": 0.6651346683502197,
      "learning_rate": 5.406015037593986e-05,
      "loss": 0.7903,
      "step": 17470
    },
    {
      "epoch": 5.111111111111111,
      "grad_norm": 0.9608508944511414,
      "learning_rate": 5.3976608187134504e-05,
      "loss": 0.7595,
      "step": 17480
    },
    {
      "epoch": 5.114035087719298,
      "grad_norm": 0.8576231002807617,
      "learning_rate": 5.3893065998329164e-05,
      "loss": 0.7923,
      "step": 17490
    },
    {
      "epoch": 5.116959064327485,
      "grad_norm": 0.889371931552887,
      "learning_rate": 5.380952380952381e-05,
      "loss": 0.8429,
      "step": 17500
    },
    {
      "epoch": 5.119883040935672,
      "grad_norm": 0.841384768486023,
      "learning_rate": 5.372598162071847e-05,
      "loss": 0.8015,
      "step": 17510
    },
    {
      "epoch": 5.12280701754386,
      "grad_norm": 0.7987626194953918,
      "learning_rate": 5.364243943191312e-05,
      "loss": 0.8365,
      "step": 17520
    },
    {
      "epoch": 5.125730994152047,
      "grad_norm": 0.8350029587745667,
      "learning_rate": 5.355889724310778e-05,
      "loss": 0.7867,
      "step": 17530
    },
    {
      "epoch": 5.128654970760234,
      "grad_norm": 0.794456422328949,
      "learning_rate": 5.3475355054302425e-05,
      "loss": 0.7942,
      "step": 17540
    },
    {
      "epoch": 5.131578947368421,
      "grad_norm": 0.9335373640060425,
      "learning_rate": 5.339181286549708e-05,
      "loss": 0.8003,
      "step": 17550
    },
    {
      "epoch": 5.1345029239766085,
      "grad_norm": 1.1542627811431885,
      "learning_rate": 5.330827067669173e-05,
      "loss": 0.7759,
      "step": 17560
    },
    {
      "epoch": 5.137426900584796,
      "grad_norm": 1.096360445022583,
      "learning_rate": 5.3224728487886385e-05,
      "loss": 0.8698,
      "step": 17570
    },
    {
      "epoch": 5.140350877192983,
      "grad_norm": 1.050573468208313,
      "learning_rate": 5.314118629908104e-05,
      "loss": 0.8803,
      "step": 17580
    },
    {
      "epoch": 5.143274853801169,
      "grad_norm": 0.8618994355201721,
      "learning_rate": 5.305764411027569e-05,
      "loss": 0.8275,
      "step": 17590
    },
    {
      "epoch": 5.146198830409356,
      "grad_norm": 0.8900375366210938,
      "learning_rate": 5.2974101921470346e-05,
      "loss": 0.7906,
      "step": 17600
    },
    {
      "epoch": 5.149122807017544,
      "grad_norm": 0.8138661980628967,
      "learning_rate": 5.2890559732665e-05,
      "loss": 0.7743,
      "step": 17610
    },
    {
      "epoch": 5.152046783625731,
      "grad_norm": 1.2135205268859863,
      "learning_rate": 5.2807017543859646e-05,
      "loss": 0.8003,
      "step": 17620
    },
    {
      "epoch": 5.154970760233918,
      "grad_norm": 0.870267391204834,
      "learning_rate": 5.2723475355054306e-05,
      "loss": 0.7714,
      "step": 17630
    },
    {
      "epoch": 5.157894736842105,
      "grad_norm": 0.8757282495498657,
      "learning_rate": 5.263993316624895e-05,
      "loss": 0.7981,
      "step": 17640
    },
    {
      "epoch": 5.1608187134502925,
      "grad_norm": 0.8145255446434021,
      "learning_rate": 5.2556390977443613e-05,
      "loss": 0.7789,
      "step": 17650
    },
    {
      "epoch": 5.16374269005848,
      "grad_norm": 0.9697967171669006,
      "learning_rate": 5.247284878863826e-05,
      "loss": 0.8201,
      "step": 17660
    },
    {
      "epoch": 5.166666666666667,
      "grad_norm": 1.1736781597137451,
      "learning_rate": 5.238930659983292e-05,
      "loss": 0.8198,
      "step": 17670
    },
    {
      "epoch": 5.169590643274854,
      "grad_norm": 0.9806634187698364,
      "learning_rate": 5.230576441102757e-05,
      "loss": 0.7747,
      "step": 17680
    },
    {
      "epoch": 5.1725146198830405,
      "grad_norm": 0.7894709706306458,
      "learning_rate": 5.222222222222223e-05,
      "loss": 0.8751,
      "step": 17690
    },
    {
      "epoch": 5.175438596491228,
      "grad_norm": 0.9235730171203613,
      "learning_rate": 5.2138680033416874e-05,
      "loss": 0.8227,
      "step": 17700
    },
    {
      "epoch": 5.178362573099415,
      "grad_norm": 1.132676124572754,
      "learning_rate": 5.2055137844611534e-05,
      "loss": 0.8135,
      "step": 17710
    },
    {
      "epoch": 5.181286549707602,
      "grad_norm": 1.0348209142684937,
      "learning_rate": 5.197159565580618e-05,
      "loss": 0.887,
      "step": 17720
    },
    {
      "epoch": 5.184210526315789,
      "grad_norm": 0.7211926579475403,
      "learning_rate": 5.188805346700084e-05,
      "loss": 0.7751,
      "step": 17730
    },
    {
      "epoch": 5.187134502923977,
      "grad_norm": 1.0694003105163574,
      "learning_rate": 5.180451127819549e-05,
      "loss": 0.8783,
      "step": 17740
    },
    {
      "epoch": 5.190058479532164,
      "grad_norm": 1.0271044969558716,
      "learning_rate": 5.172096908939015e-05,
      "loss": 0.8601,
      "step": 17750
    },
    {
      "epoch": 5.192982456140351,
      "grad_norm": 1.150795578956604,
      "learning_rate": 5.1637426900584795e-05,
      "loss": 0.7886,
      "step": 17760
    },
    {
      "epoch": 5.195906432748538,
      "grad_norm": 1.0007057189941406,
      "learning_rate": 5.1553884711779455e-05,
      "loss": 0.9591,
      "step": 17770
    },
    {
      "epoch": 5.1988304093567255,
      "grad_norm": 1.0458641052246094,
      "learning_rate": 5.14703425229741e-05,
      "loss": 0.7755,
      "step": 17780
    },
    {
      "epoch": 5.201754385964913,
      "grad_norm": 0.869705080986023,
      "learning_rate": 5.138680033416876e-05,
      "loss": 0.8198,
      "step": 17790
    },
    {
      "epoch": 5.204678362573099,
      "grad_norm": 0.9309068322181702,
      "learning_rate": 5.130325814536341e-05,
      "loss": 0.9062,
      "step": 17800
    },
    {
      "epoch": 5.207602339181286,
      "grad_norm": 0.9709367752075195,
      "learning_rate": 5.121971595655807e-05,
      "loss": 1.0468,
      "step": 17810
    },
    {
      "epoch": 5.2105263157894735,
      "grad_norm": 1.190006136894226,
      "learning_rate": 5.1136173767752716e-05,
      "loss": 0.8178,
      "step": 17820
    },
    {
      "epoch": 5.213450292397661,
      "grad_norm": 0.8093962669372559,
      "learning_rate": 5.1052631578947376e-05,
      "loss": 0.8211,
      "step": 17830
    },
    {
      "epoch": 5.216374269005848,
      "grad_norm": 1.2215490341186523,
      "learning_rate": 5.096908939014202e-05,
      "loss": 0.8816,
      "step": 17840
    },
    {
      "epoch": 5.219298245614035,
      "grad_norm": 0.9139073491096497,
      "learning_rate": 5.0885547201336683e-05,
      "loss": 0.8487,
      "step": 17850
    },
    {
      "epoch": 5.222222222222222,
      "grad_norm": 0.9710210561752319,
      "learning_rate": 5.080200501253133e-05,
      "loss": 0.8185,
      "step": 17860
    },
    {
      "epoch": 5.2251461988304095,
      "grad_norm": 1.185869574546814,
      "learning_rate": 5.071846282372599e-05,
      "loss": 0.8039,
      "step": 17870
    },
    {
      "epoch": 5.228070175438597,
      "grad_norm": 0.933140754699707,
      "learning_rate": 5.063492063492064e-05,
      "loss": 0.8904,
      "step": 17880
    },
    {
      "epoch": 5.230994152046784,
      "grad_norm": 0.9278199076652527,
      "learning_rate": 5.055137844611529e-05,
      "loss": 0.8565,
      "step": 17890
    },
    {
      "epoch": 5.23391812865497,
      "grad_norm": 0.8544648289680481,
      "learning_rate": 5.0467836257309944e-05,
      "loss": 0.7672,
      "step": 17900
    },
    {
      "epoch": 5.2368421052631575,
      "grad_norm": 1.170577049255371,
      "learning_rate": 5.03842940685046e-05,
      "loss": 0.8091,
      "step": 17910
    },
    {
      "epoch": 5.239766081871345,
      "grad_norm": 1.0188241004943848,
      "learning_rate": 5.030075187969925e-05,
      "loss": 0.7858,
      "step": 17920
    },
    {
      "epoch": 5.242690058479532,
      "grad_norm": 0.9108693599700928,
      "learning_rate": 5.0217209690893905e-05,
      "loss": 0.8901,
      "step": 17930
    },
    {
      "epoch": 5.245614035087719,
      "grad_norm": 1.2142221927642822,
      "learning_rate": 5.013366750208856e-05,
      "loss": 0.8649,
      "step": 17940
    },
    {
      "epoch": 5.248538011695906,
      "grad_norm": 0.9702712893486023,
      "learning_rate": 5.005012531328321e-05,
      "loss": 0.8681,
      "step": 17950
    },
    {
      "epoch": 5.251461988304094,
      "grad_norm": 1.0326908826828003,
      "learning_rate": 4.996658312447786e-05,
      "loss": 0.7674,
      "step": 17960
    },
    {
      "epoch": 5.254385964912281,
      "grad_norm": 0.6828755140304565,
      "learning_rate": 4.988304093567251e-05,
      "loss": 0.7195,
      "step": 17970
    },
    {
      "epoch": 5.257309941520468,
      "grad_norm": 0.8921038508415222,
      "learning_rate": 4.9799498746867165e-05,
      "loss": 0.6806,
      "step": 17980
    },
    {
      "epoch": 5.260233918128655,
      "grad_norm": 1.0700362920761108,
      "learning_rate": 4.971595655806182e-05,
      "loss": 1.041,
      "step": 17990
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 0.8091422319412231,
      "learning_rate": 4.963241436925647e-05,
      "loss": 0.8002,
      "step": 18000
    },
    {
      "epoch": 5.26608187134503,
      "grad_norm": 0.8945265412330627,
      "learning_rate": 4.9548872180451126e-05,
      "loss": 0.7992,
      "step": 18010
    },
    {
      "epoch": 5.269005847953216,
      "grad_norm": 0.8931663036346436,
      "learning_rate": 4.946532999164578e-05,
      "loss": 0.8334,
      "step": 18020
    },
    {
      "epoch": 5.271929824561403,
      "grad_norm": 0.7928512692451477,
      "learning_rate": 4.938178780284043e-05,
      "loss": 0.8198,
      "step": 18030
    },
    {
      "epoch": 5.2748538011695905,
      "grad_norm": 0.8395307064056396,
      "learning_rate": 4.9298245614035086e-05,
      "loss": 0.7836,
      "step": 18040
    },
    {
      "epoch": 5.277777777777778,
      "grad_norm": 0.8178361654281616,
      "learning_rate": 4.921470342522974e-05,
      "loss": 0.872,
      "step": 18050
    },
    {
      "epoch": 5.280701754385965,
      "grad_norm": 0.9848296046257019,
      "learning_rate": 4.913116123642439e-05,
      "loss": 0.7664,
      "step": 18060
    },
    {
      "epoch": 5.283625730994152,
      "grad_norm": 0.8129538893699646,
      "learning_rate": 4.904761904761905e-05,
      "loss": 0.8155,
      "step": 18070
    },
    {
      "epoch": 5.286549707602339,
      "grad_norm": 0.9906991720199585,
      "learning_rate": 4.89640768588137e-05,
      "loss": 0.8412,
      "step": 18080
    },
    {
      "epoch": 5.2894736842105265,
      "grad_norm": 0.8890724182128906,
      "learning_rate": 4.8880534670008354e-05,
      "loss": 0.7469,
      "step": 18090
    },
    {
      "epoch": 5.292397660818714,
      "grad_norm": 0.669029951095581,
      "learning_rate": 4.879699248120301e-05,
      "loss": 0.7383,
      "step": 18100
    },
    {
      "epoch": 5.295321637426901,
      "grad_norm": 1.0437631607055664,
      "learning_rate": 4.871345029239766e-05,
      "loss": 0.8309,
      "step": 18110
    },
    {
      "epoch": 5.298245614035087,
      "grad_norm": 1.10707688331604,
      "learning_rate": 4.8629908103592314e-05,
      "loss": 0.8718,
      "step": 18120
    },
    {
      "epoch": 5.3011695906432745,
      "grad_norm": 1.1505934000015259,
      "learning_rate": 4.854636591478697e-05,
      "loss": 0.8337,
      "step": 18130
    },
    {
      "epoch": 5.304093567251462,
      "grad_norm": 0.8044766187667847,
      "learning_rate": 4.846282372598162e-05,
      "loss": 0.8435,
      "step": 18140
    },
    {
      "epoch": 5.307017543859649,
      "grad_norm": 1.2490946054458618,
      "learning_rate": 4.8379281537176275e-05,
      "loss": 0.8926,
      "step": 18150
    },
    {
      "epoch": 5.309941520467836,
      "grad_norm": 0.7369981408119202,
      "learning_rate": 4.829573934837093e-05,
      "loss": 0.8345,
      "step": 18160
    },
    {
      "epoch": 5.312865497076023,
      "grad_norm": 1.0823880434036255,
      "learning_rate": 4.821219715956558e-05,
      "loss": 0.8608,
      "step": 18170
    },
    {
      "epoch": 5.315789473684211,
      "grad_norm": 1.1033883094787598,
      "learning_rate": 4.8128654970760235e-05,
      "loss": 0.8238,
      "step": 18180
    },
    {
      "epoch": 5.318713450292398,
      "grad_norm": 1.0488946437835693,
      "learning_rate": 4.804511278195489e-05,
      "loss": 0.8516,
      "step": 18190
    },
    {
      "epoch": 5.321637426900585,
      "grad_norm": 1.0674561262130737,
      "learning_rate": 4.796157059314954e-05,
      "loss": 0.9251,
      "step": 18200
    },
    {
      "epoch": 5.324561403508772,
      "grad_norm": 1.8983118534088135,
      "learning_rate": 4.7878028404344196e-05,
      "loss": 0.9106,
      "step": 18210
    },
    {
      "epoch": 5.3274853801169595,
      "grad_norm": 0.7706894874572754,
      "learning_rate": 4.779448621553885e-05,
      "loss": 0.7629,
      "step": 18220
    },
    {
      "epoch": 5.330409356725146,
      "grad_norm": 0.7322795391082764,
      "learning_rate": 4.77109440267335e-05,
      "loss": 0.7292,
      "step": 18230
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.940199077129364,
      "learning_rate": 4.7627401837928156e-05,
      "loss": 0.7901,
      "step": 18240
    },
    {
      "epoch": 5.33625730994152,
      "grad_norm": 1.1204153299331665,
      "learning_rate": 4.754385964912281e-05,
      "loss": 0.8109,
      "step": 18250
    },
    {
      "epoch": 5.3391812865497075,
      "grad_norm": 0.7853578329086304,
      "learning_rate": 4.746031746031746e-05,
      "loss": 0.786,
      "step": 18260
    },
    {
      "epoch": 5.342105263157895,
      "grad_norm": 0.945173978805542,
      "learning_rate": 4.737677527151212e-05,
      "loss": 0.8835,
      "step": 18270
    },
    {
      "epoch": 5.345029239766082,
      "grad_norm": 0.7308193445205688,
      "learning_rate": 4.729323308270677e-05,
      "loss": 0.8506,
      "step": 18280
    },
    {
      "epoch": 5.347953216374269,
      "grad_norm": 0.8061455488204956,
      "learning_rate": 4.720969089390142e-05,
      "loss": 0.9407,
      "step": 18290
    },
    {
      "epoch": 5.350877192982456,
      "grad_norm": 1.079721450805664,
      "learning_rate": 4.712614870509607e-05,
      "loss": 0.8405,
      "step": 18300
    },
    {
      "epoch": 5.353801169590644,
      "grad_norm": 0.9297695755958557,
      "learning_rate": 4.7042606516290724e-05,
      "loss": 0.7861,
      "step": 18310
    },
    {
      "epoch": 5.356725146198831,
      "grad_norm": 0.9611260294914246,
      "learning_rate": 4.695906432748538e-05,
      "loss": 0.8889,
      "step": 18320
    },
    {
      "epoch": 5.359649122807017,
      "grad_norm": 0.8541953563690186,
      "learning_rate": 4.687552213868003e-05,
      "loss": 0.8531,
      "step": 18330
    },
    {
      "epoch": 5.362573099415204,
      "grad_norm": 0.7689360976219177,
      "learning_rate": 4.6791979949874685e-05,
      "loss": 0.7918,
      "step": 18340
    },
    {
      "epoch": 5.3654970760233915,
      "grad_norm": 0.9660715460777283,
      "learning_rate": 4.670843776106934e-05,
      "loss": 0.8285,
      "step": 18350
    },
    {
      "epoch": 5.368421052631579,
      "grad_norm": 0.8717630505561829,
      "learning_rate": 4.662489557226399e-05,
      "loss": 0.7975,
      "step": 18360
    },
    {
      "epoch": 5.371345029239766,
      "grad_norm": 0.8324823975563049,
      "learning_rate": 4.6541353383458645e-05,
      "loss": 0.8259,
      "step": 18370
    },
    {
      "epoch": 5.374269005847953,
      "grad_norm": 0.7800757884979248,
      "learning_rate": 4.64578111946533e-05,
      "loss": 0.7836,
      "step": 18380
    },
    {
      "epoch": 5.37719298245614,
      "grad_norm": 1.1785640716552734,
      "learning_rate": 4.637426900584795e-05,
      "loss": 0.9763,
      "step": 18390
    },
    {
      "epoch": 5.380116959064328,
      "grad_norm": 1.0720834732055664,
      "learning_rate": 4.6290726817042606e-05,
      "loss": 0.8136,
      "step": 18400
    },
    {
      "epoch": 5.383040935672515,
      "grad_norm": 0.7988366484642029,
      "learning_rate": 4.620718462823726e-05,
      "loss": 0.8816,
      "step": 18410
    },
    {
      "epoch": 5.385964912280702,
      "grad_norm": 0.7739617228507996,
      "learning_rate": 4.612364243943191e-05,
      "loss": 0.717,
      "step": 18420
    },
    {
      "epoch": 5.388888888888889,
      "grad_norm": 0.9850290417671204,
      "learning_rate": 4.6040100250626566e-05,
      "loss": 0.7744,
      "step": 18430
    },
    {
      "epoch": 5.391812865497076,
      "grad_norm": 1.0149171352386475,
      "learning_rate": 4.595655806182122e-05,
      "loss": 0.8518,
      "step": 18440
    },
    {
      "epoch": 5.394736842105263,
      "grad_norm": 1.0092874765396118,
      "learning_rate": 4.587301587301587e-05,
      "loss": 0.8279,
      "step": 18450
    },
    {
      "epoch": 5.39766081871345,
      "grad_norm": 0.7420012950897217,
      "learning_rate": 4.5789473684210527e-05,
      "loss": 0.687,
      "step": 18460
    },
    {
      "epoch": 5.400584795321637,
      "grad_norm": 0.7624884843826294,
      "learning_rate": 4.570593149540518e-05,
      "loss": 0.8346,
      "step": 18470
    },
    {
      "epoch": 5.4035087719298245,
      "grad_norm": 0.8098334670066833,
      "learning_rate": 4.5622389306599834e-05,
      "loss": 0.818,
      "step": 18480
    },
    {
      "epoch": 5.406432748538012,
      "grad_norm": 1.0509318113327026,
      "learning_rate": 4.553884711779449e-05,
      "loss": 0.8312,
      "step": 18490
    },
    {
      "epoch": 5.409356725146199,
      "grad_norm": 1.1039998531341553,
      "learning_rate": 4.545530492898914e-05,
      "loss": 0.8182,
      "step": 18500
    },
    {
      "epoch": 5.412280701754386,
      "grad_norm": 0.8865686655044556,
      "learning_rate": 4.5371762740183794e-05,
      "loss": 0.8631,
      "step": 18510
    },
    {
      "epoch": 5.415204678362573,
      "grad_norm": 0.8291560411453247,
      "learning_rate": 4.528822055137845e-05,
      "loss": 0.8034,
      "step": 18520
    },
    {
      "epoch": 5.418128654970761,
      "grad_norm": 0.9620875120162964,
      "learning_rate": 4.52046783625731e-05,
      "loss": 0.8061,
      "step": 18530
    },
    {
      "epoch": 5.421052631578947,
      "grad_norm": 1.1515276432037354,
      "learning_rate": 4.5121136173767755e-05,
      "loss": 0.785,
      "step": 18540
    },
    {
      "epoch": 5.423976608187134,
      "grad_norm": 0.7819655537605286,
      "learning_rate": 4.503759398496241e-05,
      "loss": 0.7976,
      "step": 18550
    },
    {
      "epoch": 5.426900584795321,
      "grad_norm": 1.054721713066101,
      "learning_rate": 4.495405179615706e-05,
      "loss": 0.9377,
      "step": 18560
    },
    {
      "epoch": 5.4298245614035086,
      "grad_norm": 0.744443953037262,
      "learning_rate": 4.4870509607351715e-05,
      "loss": 0.7051,
      "step": 18570
    },
    {
      "epoch": 5.432748538011696,
      "grad_norm": 0.9708077907562256,
      "learning_rate": 4.478696741854637e-05,
      "loss": 0.8356,
      "step": 18580
    },
    {
      "epoch": 5.435672514619883,
      "grad_norm": 0.7958056330680847,
      "learning_rate": 4.470342522974102e-05,
      "loss": 0.8018,
      "step": 18590
    },
    {
      "epoch": 5.43859649122807,
      "grad_norm": 1.2681689262390137,
      "learning_rate": 4.4619883040935676e-05,
      "loss": 0.8783,
      "step": 18600
    },
    {
      "epoch": 5.441520467836257,
      "grad_norm": 1.1290637254714966,
      "learning_rate": 4.453634085213033e-05,
      "loss": 0.8681,
      "step": 18610
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 1.100702166557312,
      "learning_rate": 4.445279866332498e-05,
      "loss": 0.8438,
      "step": 18620
    },
    {
      "epoch": 5.447368421052632,
      "grad_norm": 0.9046348929405212,
      "learning_rate": 4.436925647451963e-05,
      "loss": 0.7774,
      "step": 18630
    },
    {
      "epoch": 5.450292397660819,
      "grad_norm": 0.9164964556694031,
      "learning_rate": 4.428571428571428e-05,
      "loss": 0.8529,
      "step": 18640
    },
    {
      "epoch": 5.453216374269006,
      "grad_norm": 0.8482800722122192,
      "learning_rate": 4.4202172096908936e-05,
      "loss": 0.8161,
      "step": 18650
    },
    {
      "epoch": 5.456140350877193,
      "grad_norm": 1.2318588495254517,
      "learning_rate": 4.411862990810359e-05,
      "loss": 0.7935,
      "step": 18660
    },
    {
      "epoch": 5.45906432748538,
      "grad_norm": 1.1389732360839844,
      "learning_rate": 4.403508771929824e-05,
      "loss": 0.7261,
      "step": 18670
    },
    {
      "epoch": 5.461988304093567,
      "grad_norm": 0.784091591835022,
      "learning_rate": 4.39515455304929e-05,
      "loss": 0.7949,
      "step": 18680
    },
    {
      "epoch": 5.464912280701754,
      "grad_norm": 0.8373777270317078,
      "learning_rate": 4.386800334168755e-05,
      "loss": 0.8816,
      "step": 18690
    },
    {
      "epoch": 5.4678362573099415,
      "grad_norm": 0.8918607234954834,
      "learning_rate": 4.3784461152882204e-05,
      "loss": 0.8182,
      "step": 18700
    },
    {
      "epoch": 5.470760233918129,
      "grad_norm": 0.9033222198486328,
      "learning_rate": 4.370091896407686e-05,
      "loss": 0.8331,
      "step": 18710
    },
    {
      "epoch": 5.473684210526316,
      "grad_norm": 0.8572049140930176,
      "learning_rate": 4.361737677527151e-05,
      "loss": 0.8651,
      "step": 18720
    },
    {
      "epoch": 5.476608187134503,
      "grad_norm": 0.961549699306488,
      "learning_rate": 4.3533834586466164e-05,
      "loss": 0.8551,
      "step": 18730
    },
    {
      "epoch": 5.47953216374269,
      "grad_norm": 0.7690948843955994,
      "learning_rate": 4.345029239766082e-05,
      "loss": 0.7839,
      "step": 18740
    },
    {
      "epoch": 5.482456140350878,
      "grad_norm": 0.7204827070236206,
      "learning_rate": 4.336675020885547e-05,
      "loss": 0.9339,
      "step": 18750
    },
    {
      "epoch": 5.485380116959064,
      "grad_norm": 1.033738374710083,
      "learning_rate": 4.3283208020050125e-05,
      "loss": 0.854,
      "step": 18760
    },
    {
      "epoch": 5.488304093567251,
      "grad_norm": 0.9890649914741516,
      "learning_rate": 4.319966583124478e-05,
      "loss": 0.8558,
      "step": 18770
    },
    {
      "epoch": 5.491228070175438,
      "grad_norm": 0.9389200806617737,
      "learning_rate": 4.311612364243943e-05,
      "loss": 0.8584,
      "step": 18780
    },
    {
      "epoch": 5.494152046783626,
      "grad_norm": 0.6946414113044739,
      "learning_rate": 4.3032581453634085e-05,
      "loss": 0.8095,
      "step": 18790
    },
    {
      "epoch": 5.497076023391813,
      "grad_norm": 0.8412871360778809,
      "learning_rate": 4.294903926482874e-05,
      "loss": 0.7496,
      "step": 18800
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.7814881801605225,
      "learning_rate": 4.286549707602339e-05,
      "loss": 0.7846,
      "step": 18810
    },
    {
      "epoch": 5.502923976608187,
      "grad_norm": 0.7890815138816833,
      "learning_rate": 4.2781954887218046e-05,
      "loss": 0.8846,
      "step": 18820
    },
    {
      "epoch": 5.505847953216374,
      "grad_norm": 1.0523943901062012,
      "learning_rate": 4.26984126984127e-05,
      "loss": 0.8827,
      "step": 18830
    },
    {
      "epoch": 5.508771929824562,
      "grad_norm": 0.8197511434555054,
      "learning_rate": 4.261487050960735e-05,
      "loss": 0.8361,
      "step": 18840
    },
    {
      "epoch": 5.511695906432749,
      "grad_norm": 0.9003140330314636,
      "learning_rate": 4.2531328320802006e-05,
      "loss": 0.7568,
      "step": 18850
    },
    {
      "epoch": 5.514619883040936,
      "grad_norm": 0.8635956645011902,
      "learning_rate": 4.244778613199666e-05,
      "loss": 0.8543,
      "step": 18860
    },
    {
      "epoch": 5.517543859649123,
      "grad_norm": 0.8031554818153381,
      "learning_rate": 4.236424394319131e-05,
      "loss": 0.75,
      "step": 18870
    },
    {
      "epoch": 5.52046783625731,
      "grad_norm": 0.8887941241264343,
      "learning_rate": 4.228070175438597e-05,
      "loss": 0.8315,
      "step": 18880
    },
    {
      "epoch": 5.523391812865497,
      "grad_norm": 0.775651216506958,
      "learning_rate": 4.219715956558062e-05,
      "loss": 0.8346,
      "step": 18890
    },
    {
      "epoch": 5.526315789473684,
      "grad_norm": 0.9862751364707947,
      "learning_rate": 4.2113617376775274e-05,
      "loss": 0.9212,
      "step": 18900
    },
    {
      "epoch": 5.529239766081871,
      "grad_norm": 0.7931422591209412,
      "learning_rate": 4.203007518796993e-05,
      "loss": 0.7461,
      "step": 18910
    },
    {
      "epoch": 5.5321637426900585,
      "grad_norm": 0.9560166001319885,
      "learning_rate": 4.194653299916458e-05,
      "loss": 0.8916,
      "step": 18920
    },
    {
      "epoch": 5.535087719298246,
      "grad_norm": 0.8377840518951416,
      "learning_rate": 4.1862990810359234e-05,
      "loss": 0.8385,
      "step": 18930
    },
    {
      "epoch": 5.538011695906433,
      "grad_norm": 0.7991670966148376,
      "learning_rate": 4.177944862155389e-05,
      "loss": 0.7987,
      "step": 18940
    },
    {
      "epoch": 5.54093567251462,
      "grad_norm": 0.8714091777801514,
      "learning_rate": 4.169590643274854e-05,
      "loss": 0.8067,
      "step": 18950
    },
    {
      "epoch": 5.543859649122807,
      "grad_norm": 0.8719922304153442,
      "learning_rate": 4.1612364243943195e-05,
      "loss": 0.8,
      "step": 18960
    },
    {
      "epoch": 5.546783625730994,
      "grad_norm": 0.9423210024833679,
      "learning_rate": 4.152882205513784e-05,
      "loss": 0.9548,
      "step": 18970
    },
    {
      "epoch": 5.549707602339181,
      "grad_norm": 0.9675174355506897,
      "learning_rate": 4.1445279866332495e-05,
      "loss": 0.8533,
      "step": 18980
    },
    {
      "epoch": 5.552631578947368,
      "grad_norm": 0.9169345498085022,
      "learning_rate": 4.136173767752715e-05,
      "loss": 0.817,
      "step": 18990
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 0.751363217830658,
      "learning_rate": 4.12781954887218e-05,
      "loss": 0.8437,
      "step": 19000
    },
    {
      "epoch": 5.558479532163743,
      "grad_norm": 0.863926351070404,
      "learning_rate": 4.1194653299916456e-05,
      "loss": 0.793,
      "step": 19010
    },
    {
      "epoch": 5.56140350877193,
      "grad_norm": 0.855593204498291,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.8869,
      "step": 19020
    },
    {
      "epoch": 5.564327485380117,
      "grad_norm": 1.0386067628860474,
      "learning_rate": 4.102756892230576e-05,
      "loss": 0.8067,
      "step": 19030
    },
    {
      "epoch": 5.567251461988304,
      "grad_norm": 0.849815845489502,
      "learning_rate": 4.0944026733500416e-05,
      "loss": 0.8042,
      "step": 19040
    },
    {
      "epoch": 5.5701754385964914,
      "grad_norm": 0.9879305958747864,
      "learning_rate": 4.086048454469507e-05,
      "loss": 0.8114,
      "step": 19050
    },
    {
      "epoch": 5.573099415204679,
      "grad_norm": 1.0013878345489502,
      "learning_rate": 4.077694235588972e-05,
      "loss": 0.8551,
      "step": 19060
    },
    {
      "epoch": 5.576023391812866,
      "grad_norm": 0.8983682990074158,
      "learning_rate": 4.0693400167084377e-05,
      "loss": 0.943,
      "step": 19070
    },
    {
      "epoch": 5.578947368421053,
      "grad_norm": 1.0934950113296509,
      "learning_rate": 4.060985797827903e-05,
      "loss": 0.8155,
      "step": 19080
    },
    {
      "epoch": 5.581871345029239,
      "grad_norm": 0.8794332146644592,
      "learning_rate": 4.0526315789473684e-05,
      "loss": 0.8305,
      "step": 19090
    },
    {
      "epoch": 5.584795321637427,
      "grad_norm": 0.6750245094299316,
      "learning_rate": 4.044277360066834e-05,
      "loss": 0.7774,
      "step": 19100
    },
    {
      "epoch": 5.587719298245614,
      "grad_norm": 0.8733628988265991,
      "learning_rate": 4.035923141186299e-05,
      "loss": 0.8317,
      "step": 19110
    },
    {
      "epoch": 5.590643274853801,
      "grad_norm": 0.9299160838127136,
      "learning_rate": 4.0275689223057644e-05,
      "loss": 0.8075,
      "step": 19120
    },
    {
      "epoch": 5.593567251461988,
      "grad_norm": 0.9192930459976196,
      "learning_rate": 4.01921470342523e-05,
      "loss": 0.8216,
      "step": 19130
    },
    {
      "epoch": 5.5964912280701755,
      "grad_norm": 1.2247085571289062,
      "learning_rate": 4.010860484544695e-05,
      "loss": 0.8795,
      "step": 19140
    },
    {
      "epoch": 5.599415204678363,
      "grad_norm": 0.8251049518585205,
      "learning_rate": 4.0025062656641605e-05,
      "loss": 0.8021,
      "step": 19150
    },
    {
      "epoch": 5.60233918128655,
      "grad_norm": 1.1075048446655273,
      "learning_rate": 3.994152046783626e-05,
      "loss": 0.8651,
      "step": 19160
    },
    {
      "epoch": 5.605263157894737,
      "grad_norm": 0.9735004901885986,
      "learning_rate": 3.985797827903091e-05,
      "loss": 0.7609,
      "step": 19170
    },
    {
      "epoch": 5.6081871345029235,
      "grad_norm": 0.7939799427986145,
      "learning_rate": 3.9774436090225565e-05,
      "loss": 0.9259,
      "step": 19180
    },
    {
      "epoch": 5.611111111111111,
      "grad_norm": 0.8239023089408875,
      "learning_rate": 3.969089390142022e-05,
      "loss": 0.8302,
      "step": 19190
    },
    {
      "epoch": 5.614035087719298,
      "grad_norm": 0.8683722615242004,
      "learning_rate": 3.960735171261487e-05,
      "loss": 0.7991,
      "step": 19200
    },
    {
      "epoch": 5.616959064327485,
      "grad_norm": 0.8714495897293091,
      "learning_rate": 3.9523809523809526e-05,
      "loss": 0.8117,
      "step": 19210
    },
    {
      "epoch": 5.619883040935672,
      "grad_norm": 0.8096063733100891,
      "learning_rate": 3.944026733500418e-05,
      "loss": 0.8774,
      "step": 19220
    },
    {
      "epoch": 5.62280701754386,
      "grad_norm": 1.0524574518203735,
      "learning_rate": 3.935672514619883e-05,
      "loss": 0.8289,
      "step": 19230
    },
    {
      "epoch": 5.625730994152047,
      "grad_norm": 1.0314912796020508,
      "learning_rate": 3.9273182957393486e-05,
      "loss": 0.8678,
      "step": 19240
    },
    {
      "epoch": 5.628654970760234,
      "grad_norm": 0.9464252591133118,
      "learning_rate": 3.918964076858814e-05,
      "loss": 0.8516,
      "step": 19250
    },
    {
      "epoch": 5.631578947368421,
      "grad_norm": 0.9104366302490234,
      "learning_rate": 3.910609857978279e-05,
      "loss": 0.7639,
      "step": 19260
    },
    {
      "epoch": 5.6345029239766085,
      "grad_norm": 1.167206048965454,
      "learning_rate": 3.9022556390977447e-05,
      "loss": 0.8319,
      "step": 19270
    },
    {
      "epoch": 5.637426900584796,
      "grad_norm": 0.7246946692466736,
      "learning_rate": 3.89390142021721e-05,
      "loss": 0.7951,
      "step": 19280
    },
    {
      "epoch": 5.640350877192983,
      "grad_norm": 0.8309154510498047,
      "learning_rate": 3.8855472013366754e-05,
      "loss": 0.8649,
      "step": 19290
    },
    {
      "epoch": 5.643274853801169,
      "grad_norm": 0.803803563117981,
      "learning_rate": 3.877192982456141e-05,
      "loss": 0.795,
      "step": 19300
    },
    {
      "epoch": 5.646198830409356,
      "grad_norm": 0.9021098613739014,
      "learning_rate": 3.8688387635756054e-05,
      "loss": 0.8321,
      "step": 19310
    },
    {
      "epoch": 5.649122807017544,
      "grad_norm": 0.9901075959205627,
      "learning_rate": 3.860484544695071e-05,
      "loss": 0.8019,
      "step": 19320
    },
    {
      "epoch": 5.652046783625731,
      "grad_norm": 1.1074281930923462,
      "learning_rate": 3.852130325814536e-05,
      "loss": 0.8122,
      "step": 19330
    },
    {
      "epoch": 5.654970760233918,
      "grad_norm": 0.7591851353645325,
      "learning_rate": 3.8437761069340014e-05,
      "loss": 0.8486,
      "step": 19340
    },
    {
      "epoch": 5.657894736842105,
      "grad_norm": 0.9474319815635681,
      "learning_rate": 3.835421888053467e-05,
      "loss": 0.7949,
      "step": 19350
    },
    {
      "epoch": 5.6608187134502925,
      "grad_norm": 0.8688178062438965,
      "learning_rate": 3.827067669172932e-05,
      "loss": 0.801,
      "step": 19360
    },
    {
      "epoch": 5.66374269005848,
      "grad_norm": 0.7988731861114502,
      "learning_rate": 3.8187134502923975e-05,
      "loss": 0.9041,
      "step": 19370
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.9234721064567566,
      "learning_rate": 3.810359231411863e-05,
      "loss": 0.7835,
      "step": 19380
    },
    {
      "epoch": 5.669590643274854,
      "grad_norm": 0.7535423040390015,
      "learning_rate": 3.802005012531328e-05,
      "loss": 0.7925,
      "step": 19390
    },
    {
      "epoch": 5.6725146198830405,
      "grad_norm": 1.1486365795135498,
      "learning_rate": 3.7936507936507935e-05,
      "loss": 0.8476,
      "step": 19400
    },
    {
      "epoch": 5.675438596491228,
      "grad_norm": 0.7752389311790466,
      "learning_rate": 3.785296574770259e-05,
      "loss": 0.7974,
      "step": 19410
    },
    {
      "epoch": 5.678362573099415,
      "grad_norm": 0.9282712340354919,
      "learning_rate": 3.776942355889724e-05,
      "loss": 0.8,
      "step": 19420
    },
    {
      "epoch": 5.681286549707602,
      "grad_norm": 0.9921309947967529,
      "learning_rate": 3.7685881370091896e-05,
      "loss": 0.831,
      "step": 19430
    },
    {
      "epoch": 5.684210526315789,
      "grad_norm": 1.1429260969161987,
      "learning_rate": 3.760233918128655e-05,
      "loss": 0.7751,
      "step": 19440
    },
    {
      "epoch": 5.687134502923977,
      "grad_norm": 0.9101859331130981,
      "learning_rate": 3.75187969924812e-05,
      "loss": 0.8,
      "step": 19450
    },
    {
      "epoch": 5.690058479532164,
      "grad_norm": 0.887750506401062,
      "learning_rate": 3.7435254803675856e-05,
      "loss": 0.7423,
      "step": 19460
    },
    {
      "epoch": 5.692982456140351,
      "grad_norm": 0.8892670273780823,
      "learning_rate": 3.735171261487051e-05,
      "loss": 0.7485,
      "step": 19470
    },
    {
      "epoch": 5.695906432748538,
      "grad_norm": 0.9422518014907837,
      "learning_rate": 3.726817042606516e-05,
      "loss": 0.8086,
      "step": 19480
    },
    {
      "epoch": 5.6988304093567255,
      "grad_norm": 0.9208603501319885,
      "learning_rate": 3.718462823725982e-05,
      "loss": 0.8259,
      "step": 19490
    },
    {
      "epoch": 5.701754385964913,
      "grad_norm": 0.7589127421379089,
      "learning_rate": 3.710108604845447e-05,
      "loss": 0.9102,
      "step": 19500
    },
    {
      "epoch": 5.7046783625731,
      "grad_norm": 0.8304083347320557,
      "learning_rate": 3.7017543859649124e-05,
      "loss": 0.843,
      "step": 19510
    },
    {
      "epoch": 5.707602339181286,
      "grad_norm": 0.8171948194503784,
      "learning_rate": 3.693400167084378e-05,
      "loss": 0.9303,
      "step": 19520
    },
    {
      "epoch": 5.7105263157894735,
      "grad_norm": 1.044559121131897,
      "learning_rate": 3.685045948203843e-05,
      "loss": 0.8861,
      "step": 19530
    },
    {
      "epoch": 5.713450292397661,
      "grad_norm": 0.9781683683395386,
      "learning_rate": 3.6766917293233084e-05,
      "loss": 0.8555,
      "step": 19540
    },
    {
      "epoch": 5.716374269005848,
      "grad_norm": 1.0169904232025146,
      "learning_rate": 3.668337510442774e-05,
      "loss": 0.7183,
      "step": 19550
    },
    {
      "epoch": 5.719298245614035,
      "grad_norm": 0.7793774008750916,
      "learning_rate": 3.659983291562239e-05,
      "loss": 0.7089,
      "step": 19560
    },
    {
      "epoch": 5.722222222222222,
      "grad_norm": 1.731019377708435,
      "learning_rate": 3.6516290726817045e-05,
      "loss": 0.857,
      "step": 19570
    },
    {
      "epoch": 5.7251461988304095,
      "grad_norm": 1.0035301446914673,
      "learning_rate": 3.64327485380117e-05,
      "loss": 0.8312,
      "step": 19580
    },
    {
      "epoch": 5.728070175438597,
      "grad_norm": 0.8496510982513428,
      "learning_rate": 3.634920634920635e-05,
      "loss": 0.84,
      "step": 19590
    },
    {
      "epoch": 5.730994152046784,
      "grad_norm": 1.0303759574890137,
      "learning_rate": 3.6265664160401005e-05,
      "loss": 0.8345,
      "step": 19600
    },
    {
      "epoch": 5.73391812865497,
      "grad_norm": 0.9249581098556519,
      "learning_rate": 3.618212197159566e-05,
      "loss": 0.8442,
      "step": 19610
    },
    {
      "epoch": 5.7368421052631575,
      "grad_norm": 0.9952461123466492,
      "learning_rate": 3.609857978279031e-05,
      "loss": 0.8336,
      "step": 19620
    },
    {
      "epoch": 5.739766081871345,
      "grad_norm": 0.8456581830978394,
      "learning_rate": 3.6015037593984966e-05,
      "loss": 0.8731,
      "step": 19630
    },
    {
      "epoch": 5.742690058479532,
      "grad_norm": 0.9225148558616638,
      "learning_rate": 3.593149540517962e-05,
      "loss": 0.8291,
      "step": 19640
    },
    {
      "epoch": 5.745614035087719,
      "grad_norm": 0.77442467212677,
      "learning_rate": 3.5847953216374266e-05,
      "loss": 0.8315,
      "step": 19650
    },
    {
      "epoch": 5.748538011695906,
      "grad_norm": 0.828454852104187,
      "learning_rate": 3.576441102756892e-05,
      "loss": 0.9218,
      "step": 19660
    },
    {
      "epoch": 5.751461988304094,
      "grad_norm": 0.9485740065574646,
      "learning_rate": 3.568086883876357e-05,
      "loss": 0.8708,
      "step": 19670
    },
    {
      "epoch": 5.754385964912281,
      "grad_norm": 0.9103066921234131,
      "learning_rate": 3.5597326649958226e-05,
      "loss": 0.7526,
      "step": 19680
    },
    {
      "epoch": 5.757309941520468,
      "grad_norm": 0.913029670715332,
      "learning_rate": 3.551378446115288e-05,
      "loss": 0.7877,
      "step": 19690
    },
    {
      "epoch": 5.760233918128655,
      "grad_norm": 0.8956108689308167,
      "learning_rate": 3.5430242272347533e-05,
      "loss": 0.8291,
      "step": 19700
    },
    {
      "epoch": 5.7631578947368425,
      "grad_norm": 1.1432772874832153,
      "learning_rate": 3.534670008354219e-05,
      "loss": 0.8029,
      "step": 19710
    },
    {
      "epoch": 5.76608187134503,
      "grad_norm": 0.8981364369392395,
      "learning_rate": 3.526315789473684e-05,
      "loss": 0.7293,
      "step": 19720
    },
    {
      "epoch": 5.769005847953216,
      "grad_norm": 0.7840620875358582,
      "learning_rate": 3.5179615705931494e-05,
      "loss": 0.8217,
      "step": 19730
    },
    {
      "epoch": 5.771929824561403,
      "grad_norm": 0.7241488695144653,
      "learning_rate": 3.509607351712615e-05,
      "loss": 0.8306,
      "step": 19740
    },
    {
      "epoch": 5.7748538011695905,
      "grad_norm": 0.9387281537055969,
      "learning_rate": 3.50125313283208e-05,
      "loss": 0.9875,
      "step": 19750
    },
    {
      "epoch": 5.777777777777778,
      "grad_norm": 1.1296310424804688,
      "learning_rate": 3.4928989139515454e-05,
      "loss": 0.9334,
      "step": 19760
    },
    {
      "epoch": 5.780701754385965,
      "grad_norm": 0.8449294567108154,
      "learning_rate": 3.484544695071011e-05,
      "loss": 0.8868,
      "step": 19770
    },
    {
      "epoch": 5.783625730994152,
      "grad_norm": 0.9299430847167969,
      "learning_rate": 3.476190476190476e-05,
      "loss": 0.8633,
      "step": 19780
    },
    {
      "epoch": 5.786549707602339,
      "grad_norm": 0.9162601828575134,
      "learning_rate": 3.4678362573099415e-05,
      "loss": 0.8577,
      "step": 19790
    },
    {
      "epoch": 5.7894736842105265,
      "grad_norm": 1.1225595474243164,
      "learning_rate": 3.459482038429407e-05,
      "loss": 0.8113,
      "step": 19800
    },
    {
      "epoch": 5.792397660818714,
      "grad_norm": 0.8385511636734009,
      "learning_rate": 3.451127819548872e-05,
      "loss": 0.7627,
      "step": 19810
    },
    {
      "epoch": 5.7953216374269,
      "grad_norm": 0.9704752564430237,
      "learning_rate": 3.4427736006683375e-05,
      "loss": 0.8286,
      "step": 19820
    },
    {
      "epoch": 5.798245614035087,
      "grad_norm": 0.9787039160728455,
      "learning_rate": 3.434419381787803e-05,
      "loss": 0.752,
      "step": 19830
    },
    {
      "epoch": 5.8011695906432745,
      "grad_norm": 1.1519988775253296,
      "learning_rate": 3.426065162907268e-05,
      "loss": 0.8719,
      "step": 19840
    },
    {
      "epoch": 5.804093567251462,
      "grad_norm": 0.8302141427993774,
      "learning_rate": 3.4177109440267336e-05,
      "loss": 0.8331,
      "step": 19850
    },
    {
      "epoch": 5.807017543859649,
      "grad_norm": 0.890887975692749,
      "learning_rate": 3.409356725146199e-05,
      "loss": 0.8137,
      "step": 19860
    },
    {
      "epoch": 5.809941520467836,
      "grad_norm": 0.9330620169639587,
      "learning_rate": 3.401002506265664e-05,
      "loss": 0.8624,
      "step": 19870
    },
    {
      "epoch": 5.812865497076023,
      "grad_norm": 0.8091893792152405,
      "learning_rate": 3.3926482873851296e-05,
      "loss": 0.8738,
      "step": 19880
    },
    {
      "epoch": 5.815789473684211,
      "grad_norm": 0.8530290722846985,
      "learning_rate": 3.384294068504595e-05,
      "loss": 0.7514,
      "step": 19890
    },
    {
      "epoch": 5.818713450292398,
      "grad_norm": 1.0886226892471313,
      "learning_rate": 3.3759398496240603e-05,
      "loss": 0.8016,
      "step": 19900
    },
    {
      "epoch": 5.821637426900585,
      "grad_norm": 0.86577308177948,
      "learning_rate": 3.367585630743526e-05,
      "loss": 0.8091,
      "step": 19910
    },
    {
      "epoch": 5.824561403508772,
      "grad_norm": 0.9420570135116577,
      "learning_rate": 3.359231411862991e-05,
      "loss": 0.8129,
      "step": 19920
    },
    {
      "epoch": 5.8274853801169595,
      "grad_norm": 0.7947217226028442,
      "learning_rate": 3.3508771929824564e-05,
      "loss": 0.8175,
      "step": 19930
    },
    {
      "epoch": 5.830409356725146,
      "grad_norm": 0.8582928776741028,
      "learning_rate": 3.342522974101922e-05,
      "loss": 0.7636,
      "step": 19940
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 0.9759869575500488,
      "learning_rate": 3.334168755221387e-05,
      "loss": 0.8369,
      "step": 19950
    },
    {
      "epoch": 5.83625730994152,
      "grad_norm": 0.9925355315208435,
      "learning_rate": 3.3258145363408524e-05,
      "loss": 0.7846,
      "step": 19960
    },
    {
      "epoch": 5.8391812865497075,
      "grad_norm": 0.9220842719078064,
      "learning_rate": 3.317460317460318e-05,
      "loss": 0.7775,
      "step": 19970
    },
    {
      "epoch": 5.842105263157895,
      "grad_norm": 0.9613915085792542,
      "learning_rate": 3.309106098579783e-05,
      "loss": 0.7734,
      "step": 19980
    },
    {
      "epoch": 5.845029239766082,
      "grad_norm": 0.925105631351471,
      "learning_rate": 3.300751879699248e-05,
      "loss": 0.7746,
      "step": 19990
    },
    {
      "epoch": 5.847953216374269,
      "grad_norm": 0.8684672713279724,
      "learning_rate": 3.292397660818713e-05,
      "loss": 0.7191,
      "step": 20000
    },
    {
      "epoch": 5.850877192982456,
      "grad_norm": 0.8733486533164978,
      "learning_rate": 3.2840434419381785e-05,
      "loss": 0.8826,
      "step": 20010
    },
    {
      "epoch": 5.853801169590644,
      "grad_norm": 0.7785593867301941,
      "learning_rate": 3.275689223057644e-05,
      "loss": 0.852,
      "step": 20020
    },
    {
      "epoch": 5.856725146198831,
      "grad_norm": 0.7117680311203003,
      "learning_rate": 3.267335004177109e-05,
      "loss": 0.7614,
      "step": 20030
    },
    {
      "epoch": 5.859649122807017,
      "grad_norm": 1.0846136808395386,
      "learning_rate": 3.2589807852965746e-05,
      "loss": 0.8218,
      "step": 20040
    },
    {
      "epoch": 5.862573099415204,
      "grad_norm": 0.9730960130691528,
      "learning_rate": 3.25062656641604e-05,
      "loss": 0.9034,
      "step": 20050
    },
    {
      "epoch": 5.8654970760233915,
      "grad_norm": 0.6351088285446167,
      "learning_rate": 3.242272347535505e-05,
      "loss": 0.7802,
      "step": 20060
    },
    {
      "epoch": 5.868421052631579,
      "grad_norm": 0.8714959025382996,
      "learning_rate": 3.2339181286549706e-05,
      "loss": 0.7809,
      "step": 20070
    },
    {
      "epoch": 5.871345029239766,
      "grad_norm": 0.97869473695755,
      "learning_rate": 3.225563909774436e-05,
      "loss": 0.87,
      "step": 20080
    },
    {
      "epoch": 5.874269005847953,
      "grad_norm": 1.0965981483459473,
      "learning_rate": 3.217209690893901e-05,
      "loss": 0.852,
      "step": 20090
    },
    {
      "epoch": 5.87719298245614,
      "grad_norm": 0.7982155680656433,
      "learning_rate": 3.208855472013367e-05,
      "loss": 0.8691,
      "step": 20100
    },
    {
      "epoch": 5.880116959064328,
      "grad_norm": 0.8002012372016907,
      "learning_rate": 3.200501253132832e-05,
      "loss": 0.8503,
      "step": 20110
    },
    {
      "epoch": 5.883040935672515,
      "grad_norm": 0.9411835670471191,
      "learning_rate": 3.1921470342522974e-05,
      "loss": 0.7546,
      "step": 20120
    },
    {
      "epoch": 5.885964912280702,
      "grad_norm": 0.8520601391792297,
      "learning_rate": 3.183792815371763e-05,
      "loss": 0.8621,
      "step": 20130
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 0.7613560557365417,
      "learning_rate": 3.175438596491228e-05,
      "loss": 0.8039,
      "step": 20140
    },
    {
      "epoch": 5.8918128654970765,
      "grad_norm": 0.9155701994895935,
      "learning_rate": 3.1670843776106934e-05,
      "loss": 0.8118,
      "step": 20150
    },
    {
      "epoch": 5.894736842105263,
      "grad_norm": 1.124966025352478,
      "learning_rate": 3.158730158730159e-05,
      "loss": 0.8009,
      "step": 20160
    },
    {
      "epoch": 5.89766081871345,
      "grad_norm": 0.8720261454582214,
      "learning_rate": 3.150375939849624e-05,
      "loss": 0.7838,
      "step": 20170
    },
    {
      "epoch": 5.900584795321637,
      "grad_norm": 1.0952140092849731,
      "learning_rate": 3.1420217209690895e-05,
      "loss": 0.8173,
      "step": 20180
    },
    {
      "epoch": 5.9035087719298245,
      "grad_norm": 0.829732358455658,
      "learning_rate": 3.133667502088555e-05,
      "loss": 0.734,
      "step": 20190
    },
    {
      "epoch": 5.906432748538012,
      "grad_norm": 0.8692874312400818,
      "learning_rate": 3.12531328320802e-05,
      "loss": 0.8492,
      "step": 20200
    },
    {
      "epoch": 5.909356725146199,
      "grad_norm": 0.9283249974250793,
      "learning_rate": 3.1169590643274855e-05,
      "loss": 0.8726,
      "step": 20210
    },
    {
      "epoch": 5.912280701754386,
      "grad_norm": 0.8039892315864563,
      "learning_rate": 3.108604845446951e-05,
      "loss": 0.7686,
      "step": 20220
    },
    {
      "epoch": 5.915204678362573,
      "grad_norm": 0.9208670258522034,
      "learning_rate": 3.100250626566416e-05,
      "loss": 0.7815,
      "step": 20230
    },
    {
      "epoch": 5.918128654970761,
      "grad_norm": 0.9371284246444702,
      "learning_rate": 3.0918964076858816e-05,
      "loss": 0.8429,
      "step": 20240
    },
    {
      "epoch": 5.921052631578947,
      "grad_norm": 1.269730567932129,
      "learning_rate": 3.083542188805347e-05,
      "loss": 0.8483,
      "step": 20250
    },
    {
      "epoch": 5.923976608187134,
      "grad_norm": 1.0699710845947266,
      "learning_rate": 3.075187969924812e-05,
      "loss": 0.8752,
      "step": 20260
    },
    {
      "epoch": 5.926900584795321,
      "grad_norm": 1.004862666130066,
      "learning_rate": 3.0668337510442776e-05,
      "loss": 0.8339,
      "step": 20270
    },
    {
      "epoch": 5.9298245614035086,
      "grad_norm": 0.8179751038551331,
      "learning_rate": 3.058479532163743e-05,
      "loss": 0.8067,
      "step": 20280
    },
    {
      "epoch": 5.932748538011696,
      "grad_norm": 0.8596381545066833,
      "learning_rate": 3.050125313283208e-05,
      "loss": 0.7435,
      "step": 20290
    },
    {
      "epoch": 5.935672514619883,
      "grad_norm": 0.8504213094711304,
      "learning_rate": 3.0417710944026733e-05,
      "loss": 0.8185,
      "step": 20300
    },
    {
      "epoch": 5.93859649122807,
      "grad_norm": 0.8940963745117188,
      "learning_rate": 3.0334168755221387e-05,
      "loss": 0.7336,
      "step": 20310
    },
    {
      "epoch": 5.941520467836257,
      "grad_norm": 0.8742912411689758,
      "learning_rate": 3.025062656641604e-05,
      "loss": 0.7854,
      "step": 20320
    },
    {
      "epoch": 5.944444444444445,
      "grad_norm": 0.7512471079826355,
      "learning_rate": 3.0167084377610694e-05,
      "loss": 0.857,
      "step": 20330
    },
    {
      "epoch": 5.947368421052632,
      "grad_norm": 0.9290715456008911,
      "learning_rate": 3.0083542188805347e-05,
      "loss": 0.7975,
      "step": 20340
    },
    {
      "epoch": 5.950292397660819,
      "grad_norm": 1.461103081703186,
      "learning_rate": 3e-05,
      "loss": 0.924,
      "step": 20350
    },
    {
      "epoch": 5.953216374269006,
      "grad_norm": 1.0348734855651855,
      "learning_rate": 2.9916457811194654e-05,
      "loss": 0.8232,
      "step": 20360
    },
    {
      "epoch": 5.956140350877193,
      "grad_norm": 0.8712729215621948,
      "learning_rate": 2.9832915622389308e-05,
      "loss": 0.8449,
      "step": 20370
    },
    {
      "epoch": 5.95906432748538,
      "grad_norm": 0.9031181335449219,
      "learning_rate": 2.974937343358396e-05,
      "loss": 0.9127,
      "step": 20380
    },
    {
      "epoch": 5.961988304093567,
      "grad_norm": 1.108471393585205,
      "learning_rate": 2.9665831244778615e-05,
      "loss": 0.8623,
      "step": 20390
    },
    {
      "epoch": 5.964912280701754,
      "grad_norm": 0.9660110473632812,
      "learning_rate": 2.9582289055973268e-05,
      "loss": 0.9221,
      "step": 20400
    },
    {
      "epoch": 5.9678362573099415,
      "grad_norm": 0.9449328184127808,
      "learning_rate": 2.9498746867167922e-05,
      "loss": 0.8582,
      "step": 20410
    },
    {
      "epoch": 5.970760233918129,
      "grad_norm": 0.745402455329895,
      "learning_rate": 2.9415204678362572e-05,
      "loss": 0.7318,
      "step": 20420
    },
    {
      "epoch": 5.973684210526316,
      "grad_norm": 0.9363768100738525,
      "learning_rate": 2.9331662489557225e-05,
      "loss": 0.9498,
      "step": 20430
    },
    {
      "epoch": 5.976608187134503,
      "grad_norm": 0.7549898624420166,
      "learning_rate": 2.924812030075188e-05,
      "loss": 0.8627,
      "step": 20440
    },
    {
      "epoch": 5.97953216374269,
      "grad_norm": 0.9310477375984192,
      "learning_rate": 2.9164578111946532e-05,
      "loss": 0.8101,
      "step": 20450
    },
    {
      "epoch": 5.982456140350877,
      "grad_norm": 1.1086691617965698,
      "learning_rate": 2.9081035923141186e-05,
      "loss": 0.8385,
      "step": 20460
    },
    {
      "epoch": 5.985380116959064,
      "grad_norm": 0.7598568797111511,
      "learning_rate": 2.899749373433584e-05,
      "loss": 0.8446,
      "step": 20470
    },
    {
      "epoch": 5.988304093567251,
      "grad_norm": 0.863086462020874,
      "learning_rate": 2.8913951545530493e-05,
      "loss": 0.8868,
      "step": 20480
    },
    {
      "epoch": 5.991228070175438,
      "grad_norm": 0.988186240196228,
      "learning_rate": 2.8830409356725146e-05,
      "loss": 0.8957,
      "step": 20490
    },
    {
      "epoch": 5.994152046783626,
      "grad_norm": 0.864406406879425,
      "learning_rate": 2.87468671679198e-05,
      "loss": 0.8276,
      "step": 20500
    },
    {
      "epoch": 5.997076023391813,
      "grad_norm": 1.010983943939209,
      "learning_rate": 2.8663324979114453e-05,
      "loss": 0.7699,
      "step": 20510
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.9085206985473633,
      "learning_rate": 2.8579782790309107e-05,
      "loss": 0.8667,
      "step": 20520
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.770371913909912,
      "eval_runtime": 76.7249,
      "eval_samples_per_second": 11.587,
      "eval_steps_per_second": 1.46,
      "step": 20520
    },
    {
      "epoch": 6.002923976608187,
      "grad_norm": 0.7493784427642822,
      "learning_rate": 2.849624060150376e-05,
      "loss": 0.8003,
      "step": 20530
    },
    {
      "epoch": 6.005847953216374,
      "grad_norm": 0.9757897257804871,
      "learning_rate": 2.8412698412698414e-05,
      "loss": 0.7505,
      "step": 20540
    },
    {
      "epoch": 6.008771929824562,
      "grad_norm": 0.9738652110099792,
      "learning_rate": 2.8329156223893067e-05,
      "loss": 0.7922,
      "step": 20550
    },
    {
      "epoch": 6.011695906432749,
      "grad_norm": 0.9079806804656982,
      "learning_rate": 2.824561403508772e-05,
      "loss": 0.7219,
      "step": 20560
    },
    {
      "epoch": 6.014619883040936,
      "grad_norm": 1.0792807340621948,
      "learning_rate": 2.8162071846282374e-05,
      "loss": 0.7757,
      "step": 20570
    },
    {
      "epoch": 6.017543859649122,
      "grad_norm": 1.2500978708267212,
      "learning_rate": 2.8078529657477028e-05,
      "loss": 0.8012,
      "step": 20580
    },
    {
      "epoch": 6.02046783625731,
      "grad_norm": 1.473921298980713,
      "learning_rate": 2.7994987468671678e-05,
      "loss": 0.8606,
      "step": 20590
    },
    {
      "epoch": 6.023391812865497,
      "grad_norm": 0.9827733635902405,
      "learning_rate": 2.791144527986633e-05,
      "loss": 0.7726,
      "step": 20600
    },
    {
      "epoch": 6.026315789473684,
      "grad_norm": 1.0316295623779297,
      "learning_rate": 2.7827903091060985e-05,
      "loss": 0.8188,
      "step": 20610
    },
    {
      "epoch": 6.029239766081871,
      "grad_norm": 0.9787739515304565,
      "learning_rate": 2.774436090225564e-05,
      "loss": 0.7308,
      "step": 20620
    },
    {
      "epoch": 6.0321637426900585,
      "grad_norm": 0.7712202668190002,
      "learning_rate": 2.7660818713450292e-05,
      "loss": 0.7501,
      "step": 20630
    },
    {
      "epoch": 6.035087719298246,
      "grad_norm": 0.8451964259147644,
      "learning_rate": 2.7577276524644945e-05,
      "loss": 0.803,
      "step": 20640
    },
    {
      "epoch": 6.038011695906433,
      "grad_norm": 0.8075204491615295,
      "learning_rate": 2.74937343358396e-05,
      "loss": 0.7221,
      "step": 20650
    },
    {
      "epoch": 6.04093567251462,
      "grad_norm": 0.8665851950645447,
      "learning_rate": 2.7410192147034252e-05,
      "loss": 0.7631,
      "step": 20660
    },
    {
      "epoch": 6.043859649122807,
      "grad_norm": 1.2835800647735596,
      "learning_rate": 2.7326649958228906e-05,
      "loss": 0.7446,
      "step": 20670
    },
    {
      "epoch": 6.046783625730994,
      "grad_norm": 0.8883496522903442,
      "learning_rate": 2.724310776942356e-05,
      "loss": 0.7757,
      "step": 20680
    },
    {
      "epoch": 6.049707602339181,
      "grad_norm": 0.7636245489120483,
      "learning_rate": 2.7159565580618213e-05,
      "loss": 0.7927,
      "step": 20690
    },
    {
      "epoch": 6.052631578947368,
      "grad_norm": 0.8195797801017761,
      "learning_rate": 2.7076023391812866e-05,
      "loss": 0.8157,
      "step": 20700
    },
    {
      "epoch": 6.055555555555555,
      "grad_norm": 0.756473183631897,
      "learning_rate": 2.699248120300752e-05,
      "loss": 0.7285,
      "step": 20710
    },
    {
      "epoch": 6.058479532163743,
      "grad_norm": 1.0931034088134766,
      "learning_rate": 2.6908939014202173e-05,
      "loss": 0.7386,
      "step": 20720
    },
    {
      "epoch": 6.06140350877193,
      "grad_norm": 0.9703212976455688,
      "learning_rate": 2.6825396825396827e-05,
      "loss": 0.7862,
      "step": 20730
    },
    {
      "epoch": 6.064327485380117,
      "grad_norm": 0.8138741850852966,
      "learning_rate": 2.674185463659148e-05,
      "loss": 0.7717,
      "step": 20740
    },
    {
      "epoch": 6.067251461988304,
      "grad_norm": 1.169615626335144,
      "learning_rate": 2.6658312447786134e-05,
      "loss": 0.8269,
      "step": 20750
    },
    {
      "epoch": 6.0701754385964914,
      "grad_norm": 1.1220775842666626,
      "learning_rate": 2.6574770258980784e-05,
      "loss": 0.7787,
      "step": 20760
    },
    {
      "epoch": 6.073099415204679,
      "grad_norm": 1.1949068307876587,
      "learning_rate": 2.6491228070175438e-05,
      "loss": 0.817,
      "step": 20770
    },
    {
      "epoch": 6.076023391812866,
      "grad_norm": 0.9269863367080688,
      "learning_rate": 2.640768588137009e-05,
      "loss": 0.8901,
      "step": 20780
    },
    {
      "epoch": 6.078947368421052,
      "grad_norm": 0.9739307165145874,
      "learning_rate": 2.6324143692564745e-05,
      "loss": 0.8227,
      "step": 20790
    },
    {
      "epoch": 6.081871345029239,
      "grad_norm": 1.0878729820251465,
      "learning_rate": 2.6240601503759398e-05,
      "loss": 0.8342,
      "step": 20800
    },
    {
      "epoch": 6.084795321637427,
      "grad_norm": 1.5962103605270386,
      "learning_rate": 2.615705931495405e-05,
      "loss": 0.9055,
      "step": 20810
    },
    {
      "epoch": 6.087719298245614,
      "grad_norm": 1.0106618404388428,
      "learning_rate": 2.6073517126148705e-05,
      "loss": 0.7871,
      "step": 20820
    },
    {
      "epoch": 6.090643274853801,
      "grad_norm": 1.0621639490127563,
      "learning_rate": 2.598997493734336e-05,
      "loss": 0.8857,
      "step": 20830
    },
    {
      "epoch": 6.093567251461988,
      "grad_norm": 0.8923892974853516,
      "learning_rate": 2.5906432748538012e-05,
      "loss": 0.7428,
      "step": 20840
    },
    {
      "epoch": 6.0964912280701755,
      "grad_norm": 0.8436835408210754,
      "learning_rate": 2.5822890559732666e-05,
      "loss": 0.8191,
      "step": 20850
    },
    {
      "epoch": 6.099415204678363,
      "grad_norm": 0.8987312912940979,
      "learning_rate": 2.573934837092732e-05,
      "loss": 0.8436,
      "step": 20860
    },
    {
      "epoch": 6.10233918128655,
      "grad_norm": 0.9903823137283325,
      "learning_rate": 2.5655806182121973e-05,
      "loss": 0.7991,
      "step": 20870
    },
    {
      "epoch": 6.105263157894737,
      "grad_norm": 0.8451842665672302,
      "learning_rate": 2.5572263993316626e-05,
      "loss": 0.7911,
      "step": 20880
    },
    {
      "epoch": 6.108187134502924,
      "grad_norm": 1.1632858514785767,
      "learning_rate": 2.548872180451128e-05,
      "loss": 0.7591,
      "step": 20890
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 0.8964642882347107,
      "learning_rate": 2.5405179615705933e-05,
      "loss": 0.847,
      "step": 20900
    },
    {
      "epoch": 6.114035087719298,
      "grad_norm": 0.9065833687782288,
      "learning_rate": 2.5321637426900587e-05,
      "loss": 0.8372,
      "step": 20910
    },
    {
      "epoch": 6.116959064327485,
      "grad_norm": 1.073601245880127,
      "learning_rate": 2.523809523809524e-05,
      "loss": 0.7743,
      "step": 20920
    },
    {
      "epoch": 6.119883040935672,
      "grad_norm": 1.0541869401931763,
      "learning_rate": 2.515455304928989e-05,
      "loss": 0.8062,
      "step": 20930
    },
    {
      "epoch": 6.12280701754386,
      "grad_norm": 0.8170167803764343,
      "learning_rate": 2.5071010860484544e-05,
      "loss": 0.7773,
      "step": 20940
    },
    {
      "epoch": 6.125730994152047,
      "grad_norm": 1.0459911823272705,
      "learning_rate": 2.4987468671679197e-05,
      "loss": 0.7506,
      "step": 20950
    },
    {
      "epoch": 6.128654970760234,
      "grad_norm": 0.8132696151733398,
      "learning_rate": 2.490392648287385e-05,
      "loss": 0.7409,
      "step": 20960
    },
    {
      "epoch": 6.131578947368421,
      "grad_norm": 1.0645179748535156,
      "learning_rate": 2.4820384294068504e-05,
      "loss": 0.8231,
      "step": 20970
    },
    {
      "epoch": 6.1345029239766085,
      "grad_norm": 1.1652185916900635,
      "learning_rate": 2.4736842105263158e-05,
      "loss": 0.6981,
      "step": 20980
    },
    {
      "epoch": 6.137426900584796,
      "grad_norm": 1.1801693439483643,
      "learning_rate": 2.465329991645781e-05,
      "loss": 0.7445,
      "step": 20990
    },
    {
      "epoch": 6.140350877192983,
      "grad_norm": 0.940390944480896,
      "learning_rate": 2.4569757727652465e-05,
      "loss": 0.7683,
      "step": 21000
    },
    {
      "epoch": 6.143274853801169,
      "grad_norm": 1.0547195672988892,
      "learning_rate": 2.4486215538847118e-05,
      "loss": 0.791,
      "step": 21010
    },
    {
      "epoch": 6.146198830409356,
      "grad_norm": 1.1594852209091187,
      "learning_rate": 2.440267335004177e-05,
      "loss": 0.7947,
      "step": 21020
    },
    {
      "epoch": 6.149122807017544,
      "grad_norm": 1.0545897483825684,
      "learning_rate": 2.4319131161236425e-05,
      "loss": 0.8094,
      "step": 21030
    },
    {
      "epoch": 6.152046783625731,
      "grad_norm": 1.0104292631149292,
      "learning_rate": 2.423558897243108e-05,
      "loss": 0.7944,
      "step": 21040
    },
    {
      "epoch": 6.154970760233918,
      "grad_norm": 0.7253930568695068,
      "learning_rate": 2.4152046783625732e-05,
      "loss": 0.6732,
      "step": 21050
    },
    {
      "epoch": 6.157894736842105,
      "grad_norm": 0.856158435344696,
      "learning_rate": 2.4068504594820386e-05,
      "loss": 0.7739,
      "step": 21060
    },
    {
      "epoch": 6.1608187134502925,
      "grad_norm": 1.1701830625534058,
      "learning_rate": 2.398496240601504e-05,
      "loss": 0.857,
      "step": 21070
    },
    {
      "epoch": 6.16374269005848,
      "grad_norm": 1.0004396438598633,
      "learning_rate": 2.3901420217209693e-05,
      "loss": 0.82,
      "step": 21080
    },
    {
      "epoch": 6.166666666666667,
      "grad_norm": 1.041050910949707,
      "learning_rate": 2.3817878028404346e-05,
      "loss": 0.701,
      "step": 21090
    },
    {
      "epoch": 6.169590643274854,
      "grad_norm": 0.8682876825332642,
      "learning_rate": 2.3734335839598996e-05,
      "loss": 0.8152,
      "step": 21100
    },
    {
      "epoch": 6.1725146198830405,
      "grad_norm": 1.5896129608154297,
      "learning_rate": 2.365079365079365e-05,
      "loss": 0.7862,
      "step": 21110
    },
    {
      "epoch": 6.175438596491228,
      "grad_norm": 1.466393232345581,
      "learning_rate": 2.3567251461988303e-05,
      "loss": 0.93,
      "step": 21120
    },
    {
      "epoch": 6.178362573099415,
      "grad_norm": 0.7872610688209534,
      "learning_rate": 2.3483709273182957e-05,
      "loss": 0.7523,
      "step": 21130
    },
    {
      "epoch": 6.181286549707602,
      "grad_norm": 0.8667925000190735,
      "learning_rate": 2.340016708437761e-05,
      "loss": 0.7583,
      "step": 21140
    },
    {
      "epoch": 6.184210526315789,
      "grad_norm": 0.9376769065856934,
      "learning_rate": 2.3316624895572264e-05,
      "loss": 0.7755,
      "step": 21150
    },
    {
      "epoch": 6.187134502923977,
      "grad_norm": 0.8670893907546997,
      "learning_rate": 2.3233082706766917e-05,
      "loss": 0.8245,
      "step": 21160
    },
    {
      "epoch": 6.190058479532164,
      "grad_norm": 0.8770755529403687,
      "learning_rate": 2.314954051796157e-05,
      "loss": 0.7825,
      "step": 21170
    },
    {
      "epoch": 6.192982456140351,
      "grad_norm": 1.1377427577972412,
      "learning_rate": 2.3065998329156224e-05,
      "loss": 0.8024,
      "step": 21180
    },
    {
      "epoch": 6.195906432748538,
      "grad_norm": 0.8436418175697327,
      "learning_rate": 2.2982456140350878e-05,
      "loss": 0.8556,
      "step": 21190
    },
    {
      "epoch": 6.1988304093567255,
      "grad_norm": 0.8767042756080627,
      "learning_rate": 2.289891395154553e-05,
      "loss": 0.7216,
      "step": 21200
    },
    {
      "epoch": 6.201754385964913,
      "grad_norm": 0.7967628836631775,
      "learning_rate": 2.2815371762740185e-05,
      "loss": 0.7834,
      "step": 21210
    },
    {
      "epoch": 6.204678362573099,
      "grad_norm": 0.8001621961593628,
      "learning_rate": 2.2731829573934838e-05,
      "loss": 0.8599,
      "step": 21220
    },
    {
      "epoch": 6.207602339181286,
      "grad_norm": 1.0533419847488403,
      "learning_rate": 2.2648287385129492e-05,
      "loss": 0.7846,
      "step": 21230
    },
    {
      "epoch": 6.2105263157894735,
      "grad_norm": 0.8905472755432129,
      "learning_rate": 2.2564745196324145e-05,
      "loss": 0.8508,
      "step": 21240
    },
    {
      "epoch": 6.213450292397661,
      "grad_norm": 0.8203181028366089,
      "learning_rate": 2.24812030075188e-05,
      "loss": 0.8484,
      "step": 21250
    },
    {
      "epoch": 6.216374269005848,
      "grad_norm": 0.8044881820678711,
      "learning_rate": 2.2397660818713452e-05,
      "loss": 0.7083,
      "step": 21260
    },
    {
      "epoch": 6.219298245614035,
      "grad_norm": 0.8773710131645203,
      "learning_rate": 2.2314118629908102e-05,
      "loss": 0.8139,
      "step": 21270
    },
    {
      "epoch": 6.222222222222222,
      "grad_norm": 0.8977236151695251,
      "learning_rate": 2.2230576441102756e-05,
      "loss": 0.7488,
      "step": 21280
    },
    {
      "epoch": 6.2251461988304095,
      "grad_norm": 0.8540544509887695,
      "learning_rate": 2.214703425229741e-05,
      "loss": 0.7701,
      "step": 21290
    },
    {
      "epoch": 6.228070175438597,
      "grad_norm": 1.0137596130371094,
      "learning_rate": 2.2063492063492063e-05,
      "loss": 0.7947,
      "step": 21300
    },
    {
      "epoch": 6.230994152046784,
      "grad_norm": 1.336719036102295,
      "learning_rate": 2.1979949874686716e-05,
      "loss": 0.8368,
      "step": 21310
    },
    {
      "epoch": 6.23391812865497,
      "grad_norm": 0.919578492641449,
      "learning_rate": 2.189640768588137e-05,
      "loss": 0.7852,
      "step": 21320
    },
    {
      "epoch": 6.2368421052631575,
      "grad_norm": 0.9549248218536377,
      "learning_rate": 2.1812865497076023e-05,
      "loss": 0.813,
      "step": 21330
    },
    {
      "epoch": 6.239766081871345,
      "grad_norm": 0.8603499531745911,
      "learning_rate": 2.1729323308270677e-05,
      "loss": 0.7384,
      "step": 21340
    },
    {
      "epoch": 6.242690058479532,
      "grad_norm": 1.0338770151138306,
      "learning_rate": 2.164578111946533e-05,
      "loss": 0.8127,
      "step": 21350
    },
    {
      "epoch": 6.245614035087719,
      "grad_norm": 0.8552778363227844,
      "learning_rate": 2.1562238930659984e-05,
      "loss": 0.7732,
      "step": 21360
    },
    {
      "epoch": 6.248538011695906,
      "grad_norm": 0.9738237857818604,
      "learning_rate": 2.1478696741854637e-05,
      "loss": 0.8502,
      "step": 21370
    },
    {
      "epoch": 6.251461988304094,
      "grad_norm": 0.7883892059326172,
      "learning_rate": 2.139515455304929e-05,
      "loss": 0.8238,
      "step": 21380
    },
    {
      "epoch": 6.254385964912281,
      "grad_norm": 0.7702419757843018,
      "learning_rate": 2.1311612364243944e-05,
      "loss": 0.801,
      "step": 21390
    },
    {
      "epoch": 6.257309941520468,
      "grad_norm": 0.6842671632766724,
      "learning_rate": 2.1228070175438598e-05,
      "loss": 0.8042,
      "step": 21400
    },
    {
      "epoch": 6.260233918128655,
      "grad_norm": 0.9993213415145874,
      "learning_rate": 2.114452798663325e-05,
      "loss": 0.8342,
      "step": 21410
    },
    {
      "epoch": 6.2631578947368425,
      "grad_norm": 1.0591752529144287,
      "learning_rate": 2.1060985797827905e-05,
      "loss": 0.7601,
      "step": 21420
    },
    {
      "epoch": 6.26608187134503,
      "grad_norm": 1.2480204105377197,
      "learning_rate": 2.097744360902256e-05,
      "loss": 0.7302,
      "step": 21430
    },
    {
      "epoch": 6.269005847953216,
      "grad_norm": 1.015159010887146,
      "learning_rate": 2.089390142021721e-05,
      "loss": 0.7283,
      "step": 21440
    },
    {
      "epoch": 6.271929824561403,
      "grad_norm": 0.8431274890899658,
      "learning_rate": 2.0810359231411862e-05,
      "loss": 0.8357,
      "step": 21450
    },
    {
      "epoch": 6.2748538011695905,
      "grad_norm": 0.9942505955696106,
      "learning_rate": 2.0726817042606516e-05,
      "loss": 0.8287,
      "step": 21460
    },
    {
      "epoch": 6.277777777777778,
      "grad_norm": 0.7290269732475281,
      "learning_rate": 2.064327485380117e-05,
      "loss": 0.7454,
      "step": 21470
    },
    {
      "epoch": 6.280701754385965,
      "grad_norm": 0.8834197521209717,
      "learning_rate": 2.0559732664995823e-05,
      "loss": 0.7709,
      "step": 21480
    },
    {
      "epoch": 6.283625730994152,
      "grad_norm": 0.8364848494529724,
      "learning_rate": 2.0476190476190476e-05,
      "loss": 0.7279,
      "step": 21490
    },
    {
      "epoch": 6.286549707602339,
      "grad_norm": 0.8511419892311096,
      "learning_rate": 2.039264828738513e-05,
      "loss": 0.7631,
      "step": 21500
    },
    {
      "epoch": 6.2894736842105265,
      "grad_norm": 1.0816401243209839,
      "learning_rate": 2.0309106098579783e-05,
      "loss": 0.8239,
      "step": 21510
    },
    {
      "epoch": 6.292397660818714,
      "grad_norm": 0.875367283821106,
      "learning_rate": 2.0225563909774437e-05,
      "loss": 0.7615,
      "step": 21520
    },
    {
      "epoch": 6.295321637426901,
      "grad_norm": 0.8843970894813538,
      "learning_rate": 2.014202172096909e-05,
      "loss": 0.7731,
      "step": 21530
    },
    {
      "epoch": 6.298245614035087,
      "grad_norm": 0.796400249004364,
      "learning_rate": 2.0058479532163744e-05,
      "loss": 0.8161,
      "step": 21540
    },
    {
      "epoch": 6.3011695906432745,
      "grad_norm": 0.771580696105957,
      "learning_rate": 1.9974937343358397e-05,
      "loss": 0.7871,
      "step": 21550
    },
    {
      "epoch": 6.304093567251462,
      "grad_norm": 1.0665854215621948,
      "learning_rate": 1.989139515455305e-05,
      "loss": 0.7829,
      "step": 21560
    },
    {
      "epoch": 6.307017543859649,
      "grad_norm": 0.9536515474319458,
      "learning_rate": 1.9807852965747704e-05,
      "loss": 0.7603,
      "step": 21570
    },
    {
      "epoch": 6.309941520467836,
      "grad_norm": 0.9379246830940247,
      "learning_rate": 1.9724310776942358e-05,
      "loss": 0.8091,
      "step": 21580
    },
    {
      "epoch": 6.312865497076023,
      "grad_norm": 0.8491466045379639,
      "learning_rate": 1.964076858813701e-05,
      "loss": 0.7835,
      "step": 21590
    },
    {
      "epoch": 6.315789473684211,
      "grad_norm": 1.300621509552002,
      "learning_rate": 1.9557226399331665e-05,
      "loss": 0.7783,
      "step": 21600
    },
    {
      "epoch": 6.318713450292398,
      "grad_norm": 1.0075644254684448,
      "learning_rate": 1.9473684210526315e-05,
      "loss": 0.7762,
      "step": 21610
    },
    {
      "epoch": 6.321637426900585,
      "grad_norm": 1.0339374542236328,
      "learning_rate": 1.9390142021720968e-05,
      "loss": 0.8092,
      "step": 21620
    },
    {
      "epoch": 6.324561403508772,
      "grad_norm": 0.9111803770065308,
      "learning_rate": 1.930659983291562e-05,
      "loss": 0.9109,
      "step": 21630
    },
    {
      "epoch": 6.3274853801169595,
      "grad_norm": 0.9425721168518066,
      "learning_rate": 1.9223057644110275e-05,
      "loss": 0.7752,
      "step": 21640
    },
    {
      "epoch": 6.330409356725146,
      "grad_norm": 0.9337989091873169,
      "learning_rate": 1.913951545530493e-05,
      "loss": 0.7391,
      "step": 21650
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 1.0570135116577148,
      "learning_rate": 1.9055973266499582e-05,
      "loss": 0.7848,
      "step": 21660
    },
    {
      "epoch": 6.33625730994152,
      "grad_norm": 0.9833163022994995,
      "learning_rate": 1.8972431077694236e-05,
      "loss": 0.7515,
      "step": 21670
    },
    {
      "epoch": 6.3391812865497075,
      "grad_norm": 1.071761131286621,
      "learning_rate": 1.888888888888889e-05,
      "loss": 0.7131,
      "step": 21680
    },
    {
      "epoch": 6.342105263157895,
      "grad_norm": 0.8866185545921326,
      "learning_rate": 1.8805346700083543e-05,
      "loss": 0.7362,
      "step": 21690
    },
    {
      "epoch": 6.345029239766082,
      "grad_norm": 0.9173751473426819,
      "learning_rate": 1.8721804511278196e-05,
      "loss": 0.7462,
      "step": 21700
    },
    {
      "epoch": 6.347953216374269,
      "grad_norm": 0.6874373555183411,
      "learning_rate": 1.863826232247285e-05,
      "loss": 0.7475,
      "step": 21710
    },
    {
      "epoch": 6.350877192982456,
      "grad_norm": 0.9989421367645264,
      "learning_rate": 1.8554720133667503e-05,
      "loss": 0.7793,
      "step": 21720
    },
    {
      "epoch": 6.353801169590644,
      "grad_norm": 0.8941901922225952,
      "learning_rate": 1.8471177944862157e-05,
      "loss": 0.7981,
      "step": 21730
    },
    {
      "epoch": 6.356725146198831,
      "grad_norm": 0.8349552154541016,
      "learning_rate": 1.838763575605681e-05,
      "loss": 0.7747,
      "step": 21740
    },
    {
      "epoch": 6.359649122807017,
      "grad_norm": 1.1265491247177124,
      "learning_rate": 1.8304093567251464e-05,
      "loss": 0.8351,
      "step": 21750
    },
    {
      "epoch": 6.362573099415204,
      "grad_norm": 0.7770697474479675,
      "learning_rate": 1.8220551378446117e-05,
      "loss": 0.8005,
      "step": 21760
    },
    {
      "epoch": 6.3654970760233915,
      "grad_norm": 1.0455472469329834,
      "learning_rate": 1.813700918964077e-05,
      "loss": 0.8139,
      "step": 21770
    },
    {
      "epoch": 6.368421052631579,
      "grad_norm": 0.812205970287323,
      "learning_rate": 1.805346700083542e-05,
      "loss": 0.8377,
      "step": 21780
    },
    {
      "epoch": 6.371345029239766,
      "grad_norm": 0.7672185301780701,
      "learning_rate": 1.7969924812030074e-05,
      "loss": 0.7537,
      "step": 21790
    },
    {
      "epoch": 6.374269005847953,
      "grad_norm": 1.1221137046813965,
      "learning_rate": 1.7886382623224728e-05,
      "loss": 0.8702,
      "step": 21800
    },
    {
      "epoch": 6.37719298245614,
      "grad_norm": 0.8758943676948547,
      "learning_rate": 1.780284043441938e-05,
      "loss": 0.7501,
      "step": 21810
    },
    {
      "epoch": 6.380116959064328,
      "grad_norm": 0.9318228960037231,
      "learning_rate": 1.7719298245614035e-05,
      "loss": 0.7017,
      "step": 21820
    },
    {
      "epoch": 6.383040935672515,
      "grad_norm": 0.9367669820785522,
      "learning_rate": 1.7635756056808688e-05,
      "loss": 0.752,
      "step": 21830
    },
    {
      "epoch": 6.385964912280702,
      "grad_norm": 0.8637677431106567,
      "learning_rate": 1.7552213868003342e-05,
      "loss": 0.7409,
      "step": 21840
    },
    {
      "epoch": 6.388888888888889,
      "grad_norm": 0.9546135067939758,
      "learning_rate": 1.7468671679197995e-05,
      "loss": 0.843,
      "step": 21850
    },
    {
      "epoch": 6.391812865497076,
      "grad_norm": 0.977661669254303,
      "learning_rate": 1.738512949039265e-05,
      "loss": 0.781,
      "step": 21860
    },
    {
      "epoch": 6.394736842105263,
      "grad_norm": 1.0266774892807007,
      "learning_rate": 1.7301587301587302e-05,
      "loss": 0.7893,
      "step": 21870
    },
    {
      "epoch": 6.39766081871345,
      "grad_norm": 0.995816171169281,
      "learning_rate": 1.7218045112781956e-05,
      "loss": 0.7508,
      "step": 21880
    },
    {
      "epoch": 6.400584795321637,
      "grad_norm": 0.8171446323394775,
      "learning_rate": 1.713450292397661e-05,
      "loss": 0.8241,
      "step": 21890
    },
    {
      "epoch": 6.4035087719298245,
      "grad_norm": 0.9009144306182861,
      "learning_rate": 1.7050960735171263e-05,
      "loss": 0.7478,
      "step": 21900
    },
    {
      "epoch": 6.406432748538012,
      "grad_norm": 0.9060096144676208,
      "learning_rate": 1.6967418546365916e-05,
      "loss": 0.8958,
      "step": 21910
    },
    {
      "epoch": 6.409356725146199,
      "grad_norm": 1.0024176836013794,
      "learning_rate": 1.688387635756057e-05,
      "loss": 0.7693,
      "step": 21920
    },
    {
      "epoch": 6.412280701754386,
      "grad_norm": 0.8997098803520203,
      "learning_rate": 1.6800334168755223e-05,
      "loss": 0.759,
      "step": 21930
    },
    {
      "epoch": 6.415204678362573,
      "grad_norm": 0.8031482100486755,
      "learning_rate": 1.6716791979949877e-05,
      "loss": 0.8043,
      "step": 21940
    },
    {
      "epoch": 6.418128654970761,
      "grad_norm": 0.6531106233596802,
      "learning_rate": 1.6633249791144527e-05,
      "loss": 0.7971,
      "step": 21950
    },
    {
      "epoch": 6.421052631578947,
      "grad_norm": 0.9639943838119507,
      "learning_rate": 1.654970760233918e-05,
      "loss": 0.7189,
      "step": 21960
    },
    {
      "epoch": 6.423976608187134,
      "grad_norm": 0.8359330296516418,
      "learning_rate": 1.6466165413533834e-05,
      "loss": 0.774,
      "step": 21970
    },
    {
      "epoch": 6.426900584795321,
      "grad_norm": 1.135057806968689,
      "learning_rate": 1.6382623224728487e-05,
      "loss": 0.9222,
      "step": 21980
    },
    {
      "epoch": 6.4298245614035086,
      "grad_norm": 1.1212718486785889,
      "learning_rate": 1.629908103592314e-05,
      "loss": 0.8379,
      "step": 21990
    },
    {
      "epoch": 6.432748538011696,
      "grad_norm": 0.7413593530654907,
      "learning_rate": 1.6215538847117794e-05,
      "loss": 0.7846,
      "step": 22000
    },
    {
      "epoch": 6.435672514619883,
      "grad_norm": 0.8719362020492554,
      "learning_rate": 1.6131996658312448e-05,
      "loss": 0.8016,
      "step": 22010
    },
    {
      "epoch": 6.43859649122807,
      "grad_norm": 0.917022168636322,
      "learning_rate": 1.60484544695071e-05,
      "loss": 0.8055,
      "step": 22020
    },
    {
      "epoch": 6.441520467836257,
      "grad_norm": 0.8520413637161255,
      "learning_rate": 1.5964912280701755e-05,
      "loss": 0.8171,
      "step": 22030
    },
    {
      "epoch": 6.444444444444445,
      "grad_norm": 0.8817914128303528,
      "learning_rate": 1.588137009189641e-05,
      "loss": 0.6835,
      "step": 22040
    },
    {
      "epoch": 6.447368421052632,
      "grad_norm": 0.9753085374832153,
      "learning_rate": 1.5797827903091062e-05,
      "loss": 0.8061,
      "step": 22050
    },
    {
      "epoch": 6.450292397660819,
      "grad_norm": 0.9140809178352356,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 0.7639,
      "step": 22060
    },
    {
      "epoch": 6.453216374269006,
      "grad_norm": 0.89158034324646,
      "learning_rate": 1.563074352548037e-05,
      "loss": 0.6917,
      "step": 22070
    },
    {
      "epoch": 6.456140350877193,
      "grad_norm": 1.051928997039795,
      "learning_rate": 1.5547201336675022e-05,
      "loss": 0.737,
      "step": 22080
    },
    {
      "epoch": 6.45906432748538,
      "grad_norm": 0.9158104658126831,
      "learning_rate": 1.5463659147869676e-05,
      "loss": 0.7111,
      "step": 22090
    },
    {
      "epoch": 6.461988304093567,
      "grad_norm": 1.2286241054534912,
      "learning_rate": 1.538011695906433e-05,
      "loss": 0.7531,
      "step": 22100
    },
    {
      "epoch": 6.464912280701754,
      "grad_norm": 1.3063366413116455,
      "learning_rate": 1.5296574770258983e-05,
      "loss": 0.8349,
      "step": 22110
    },
    {
      "epoch": 6.4678362573099415,
      "grad_norm": 0.8266087174415588,
      "learning_rate": 1.5213032581453635e-05,
      "loss": 0.6851,
      "step": 22120
    },
    {
      "epoch": 6.470760233918129,
      "grad_norm": 1.104136347770691,
      "learning_rate": 1.5129490392648288e-05,
      "loss": 0.7877,
      "step": 22130
    },
    {
      "epoch": 6.473684210526316,
      "grad_norm": 0.9301328063011169,
      "learning_rate": 1.5045948203842942e-05,
      "loss": 0.7905,
      "step": 22140
    },
    {
      "epoch": 6.476608187134503,
      "grad_norm": 1.177189826965332,
      "learning_rate": 1.4962406015037595e-05,
      "loss": 0.8716,
      "step": 22150
    },
    {
      "epoch": 6.47953216374269,
      "grad_norm": 1.1002044677734375,
      "learning_rate": 1.4878863826232247e-05,
      "loss": 0.8548,
      "step": 22160
    },
    {
      "epoch": 6.482456140350878,
      "grad_norm": 0.8268432021141052,
      "learning_rate": 1.47953216374269e-05,
      "loss": 0.8578,
      "step": 22170
    },
    {
      "epoch": 6.485380116959064,
      "grad_norm": 0.8553434610366821,
      "learning_rate": 1.4711779448621554e-05,
      "loss": 0.7551,
      "step": 22180
    },
    {
      "epoch": 6.488304093567251,
      "grad_norm": 1.1820911169052124,
      "learning_rate": 1.4628237259816207e-05,
      "loss": 0.8967,
      "step": 22190
    },
    {
      "epoch": 6.491228070175438,
      "grad_norm": 1.15714430809021,
      "learning_rate": 1.4544695071010861e-05,
      "loss": 0.8913,
      "step": 22200
    },
    {
      "epoch": 6.494152046783626,
      "grad_norm": 1.2937458753585815,
      "learning_rate": 1.4461152882205514e-05,
      "loss": 0.8607,
      "step": 22210
    },
    {
      "epoch": 6.497076023391813,
      "grad_norm": 1.0380533933639526,
      "learning_rate": 1.4377610693400168e-05,
      "loss": 0.793,
      "step": 22220
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.8542847037315369,
      "learning_rate": 1.4294068504594821e-05,
      "loss": 0.7823,
      "step": 22230
    },
    {
      "epoch": 6.502923976608187,
      "grad_norm": 0.801660418510437,
      "learning_rate": 1.4210526315789475e-05,
      "loss": 0.775,
      "step": 22240
    },
    {
      "epoch": 6.505847953216374,
      "grad_norm": 0.9207666516304016,
      "learning_rate": 1.4126984126984127e-05,
      "loss": 0.7675,
      "step": 22250
    },
    {
      "epoch": 6.508771929824562,
      "grad_norm": 0.8128026723861694,
      "learning_rate": 1.404344193817878e-05,
      "loss": 0.8004,
      "step": 22260
    },
    {
      "epoch": 6.511695906432749,
      "grad_norm": 0.8219393491744995,
      "learning_rate": 1.3959899749373434e-05,
      "loss": 0.8091,
      "step": 22270
    },
    {
      "epoch": 6.514619883040936,
      "grad_norm": 1.0443588495254517,
      "learning_rate": 1.3876357560568087e-05,
      "loss": 0.7646,
      "step": 22280
    },
    {
      "epoch": 6.517543859649123,
      "grad_norm": 0.7536600232124329,
      "learning_rate": 1.379281537176274e-05,
      "loss": 0.7891,
      "step": 22290
    },
    {
      "epoch": 6.52046783625731,
      "grad_norm": 0.7190296649932861,
      "learning_rate": 1.3709273182957394e-05,
      "loss": 0.8124,
      "step": 22300
    },
    {
      "epoch": 6.523391812865497,
      "grad_norm": 0.8526133894920349,
      "learning_rate": 1.3625730994152048e-05,
      "loss": 0.6754,
      "step": 22310
    },
    {
      "epoch": 6.526315789473684,
      "grad_norm": 0.9668596386909485,
      "learning_rate": 1.3542188805346701e-05,
      "loss": 0.8012,
      "step": 22320
    },
    {
      "epoch": 6.529239766081871,
      "grad_norm": 0.6749060153961182,
      "learning_rate": 1.3458646616541353e-05,
      "loss": 0.6916,
      "step": 22330
    },
    {
      "epoch": 6.5321637426900585,
      "grad_norm": 1.062486171722412,
      "learning_rate": 1.3375104427736007e-05,
      "loss": 0.7739,
      "step": 22340
    },
    {
      "epoch": 6.535087719298246,
      "grad_norm": 0.839022696018219,
      "learning_rate": 1.329156223893066e-05,
      "loss": 0.821,
      "step": 22350
    },
    {
      "epoch": 6.538011695906433,
      "grad_norm": 0.8743129968643188,
      "learning_rate": 1.3208020050125314e-05,
      "loss": 0.7651,
      "step": 22360
    },
    {
      "epoch": 6.54093567251462,
      "grad_norm": 0.715049147605896,
      "learning_rate": 1.3124477861319967e-05,
      "loss": 0.7972,
      "step": 22370
    },
    {
      "epoch": 6.543859649122807,
      "grad_norm": 0.8219879269599915,
      "learning_rate": 1.304093567251462e-05,
      "loss": 0.7887,
      "step": 22380
    },
    {
      "epoch": 6.546783625730994,
      "grad_norm": 1.0256643295288086,
      "learning_rate": 1.2957393483709274e-05,
      "loss": 0.7858,
      "step": 22390
    },
    {
      "epoch": 6.549707602339181,
      "grad_norm": 0.8570824265480042,
      "learning_rate": 1.2873851294903928e-05,
      "loss": 0.7926,
      "step": 22400
    },
    {
      "epoch": 6.552631578947368,
      "grad_norm": 0.9582344889640808,
      "learning_rate": 1.2790309106098581e-05,
      "loss": 0.7836,
      "step": 22410
    },
    {
      "epoch": 6.555555555555555,
      "grad_norm": 1.1662325859069824,
      "learning_rate": 1.2706766917293233e-05,
      "loss": 0.7693,
      "step": 22420
    },
    {
      "epoch": 6.558479532163743,
      "grad_norm": 1.0227470397949219,
      "learning_rate": 1.2623224728487886e-05,
      "loss": 0.8336,
      "step": 22430
    },
    {
      "epoch": 6.56140350877193,
      "grad_norm": 1.0574654340744019,
      "learning_rate": 1.253968253968254e-05,
      "loss": 0.7918,
      "step": 22440
    },
    {
      "epoch": 6.564327485380117,
      "grad_norm": 1.0940659046173096,
      "learning_rate": 1.2456140350877193e-05,
      "loss": 0.7935,
      "step": 22450
    },
    {
      "epoch": 6.567251461988304,
      "grad_norm": 0.8472974300384521,
      "learning_rate": 1.2372598162071847e-05,
      "loss": 0.8158,
      "step": 22460
    },
    {
      "epoch": 6.5701754385964914,
      "grad_norm": 0.87281334400177,
      "learning_rate": 1.22890559732665e-05,
      "loss": 0.7711,
      "step": 22470
    },
    {
      "epoch": 6.573099415204679,
      "grad_norm": 0.9783638119697571,
      "learning_rate": 1.2205513784461154e-05,
      "loss": 0.7981,
      "step": 22480
    },
    {
      "epoch": 6.576023391812866,
      "grad_norm": 0.902428150177002,
      "learning_rate": 1.2121971595655807e-05,
      "loss": 0.7334,
      "step": 22490
    },
    {
      "epoch": 6.578947368421053,
      "grad_norm": 0.9304060339927673,
      "learning_rate": 1.2038429406850459e-05,
      "loss": 0.7177,
      "step": 22500
    },
    {
      "epoch": 6.581871345029239,
      "grad_norm": 1.0088117122650146,
      "learning_rate": 1.1954887218045113e-05,
      "loss": 0.8291,
      "step": 22510
    },
    {
      "epoch": 6.584795321637427,
      "grad_norm": 0.9347225427627563,
      "learning_rate": 1.1871345029239766e-05,
      "loss": 0.7948,
      "step": 22520
    },
    {
      "epoch": 6.587719298245614,
      "grad_norm": 0.988340437412262,
      "learning_rate": 1.178780284043442e-05,
      "loss": 0.7855,
      "step": 22530
    },
    {
      "epoch": 6.590643274853801,
      "grad_norm": 0.9738653302192688,
      "learning_rate": 1.1704260651629073e-05,
      "loss": 0.8276,
      "step": 22540
    },
    {
      "epoch": 6.593567251461988,
      "grad_norm": 0.7493982315063477,
      "learning_rate": 1.1620718462823727e-05,
      "loss": 0.775,
      "step": 22550
    },
    {
      "epoch": 6.5964912280701755,
      "grad_norm": 0.8535363674163818,
      "learning_rate": 1.153717627401838e-05,
      "loss": 0.8169,
      "step": 22560
    },
    {
      "epoch": 6.599415204678363,
      "grad_norm": 0.8112404346466064,
      "learning_rate": 1.1453634085213034e-05,
      "loss": 0.7478,
      "step": 22570
    },
    {
      "epoch": 6.60233918128655,
      "grad_norm": 0.841284453868866,
      "learning_rate": 1.1370091896407687e-05,
      "loss": 0.8003,
      "step": 22580
    },
    {
      "epoch": 6.605263157894737,
      "grad_norm": 0.8700429201126099,
      "learning_rate": 1.1286549707602339e-05,
      "loss": 0.7646,
      "step": 22590
    },
    {
      "epoch": 6.6081871345029235,
      "grad_norm": 0.6798155307769775,
      "learning_rate": 1.1203007518796992e-05,
      "loss": 0.7744,
      "step": 22600
    },
    {
      "epoch": 6.611111111111111,
      "grad_norm": 1.010519027709961,
      "learning_rate": 1.1119465329991646e-05,
      "loss": 0.7082,
      "step": 22610
    },
    {
      "epoch": 6.614035087719298,
      "grad_norm": 1.1174529790878296,
      "learning_rate": 1.10359231411863e-05,
      "loss": 0.7944,
      "step": 22620
    },
    {
      "epoch": 6.616959064327485,
      "grad_norm": 0.8927540183067322,
      "learning_rate": 1.0952380952380953e-05,
      "loss": 0.7913,
      "step": 22630
    },
    {
      "epoch": 6.619883040935672,
      "grad_norm": 0.9836248159408569,
      "learning_rate": 1.0868838763575606e-05,
      "loss": 0.7745,
      "step": 22640
    },
    {
      "epoch": 6.62280701754386,
      "grad_norm": 1.1049314737319946,
      "learning_rate": 1.078529657477026e-05,
      "loss": 0.8046,
      "step": 22650
    },
    {
      "epoch": 6.625730994152047,
      "grad_norm": 0.904973030090332,
      "learning_rate": 1.0701754385964913e-05,
      "loss": 0.7924,
      "step": 22660
    },
    {
      "epoch": 6.628654970760234,
      "grad_norm": 0.8648794293403625,
      "learning_rate": 1.0618212197159565e-05,
      "loss": 0.735,
      "step": 22670
    },
    {
      "epoch": 6.631578947368421,
      "grad_norm": 0.8984849452972412,
      "learning_rate": 1.0534670008354219e-05,
      "loss": 0.7238,
      "step": 22680
    },
    {
      "epoch": 6.6345029239766085,
      "grad_norm": 0.9964382648468018,
      "learning_rate": 1.0451127819548872e-05,
      "loss": 0.8211,
      "step": 22690
    },
    {
      "epoch": 6.637426900584796,
      "grad_norm": 1.7100588083267212,
      "learning_rate": 1.0367585630743526e-05,
      "loss": 0.8256,
      "step": 22700
    },
    {
      "epoch": 6.640350877192983,
      "grad_norm": 1.2247496843338013,
      "learning_rate": 1.028404344193818e-05,
      "loss": 0.7007,
      "step": 22710
    },
    {
      "epoch": 6.643274853801169,
      "grad_norm": 1.1229199171066284,
      "learning_rate": 1.0200501253132833e-05,
      "loss": 0.8036,
      "step": 22720
    },
    {
      "epoch": 6.646198830409356,
      "grad_norm": 1.0399301052093506,
      "learning_rate": 1.0116959064327486e-05,
      "loss": 0.7165,
      "step": 22730
    },
    {
      "epoch": 6.649122807017544,
      "grad_norm": 1.2164121866226196,
      "learning_rate": 1.003341687552214e-05,
      "loss": 0.8663,
      "step": 22740
    },
    {
      "epoch": 6.652046783625731,
      "grad_norm": 0.819801390171051,
      "learning_rate": 9.949874686716793e-06,
      "loss": 0.8261,
      "step": 22750
    },
    {
      "epoch": 6.654970760233918,
      "grad_norm": 1.1400607824325562,
      "learning_rate": 9.866332497911445e-06,
      "loss": 0.7806,
      "step": 22760
    },
    {
      "epoch": 6.657894736842105,
      "grad_norm": 1.0526994466781616,
      "learning_rate": 9.782790309106099e-06,
      "loss": 0.7322,
      "step": 22770
    },
    {
      "epoch": 6.6608187134502925,
      "grad_norm": 0.878353476524353,
      "learning_rate": 9.699248120300752e-06,
      "loss": 0.6925,
      "step": 22780
    },
    {
      "epoch": 6.66374269005848,
      "grad_norm": 1.1214295625686646,
      "learning_rate": 9.615705931495406e-06,
      "loss": 0.7507,
      "step": 22790
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 1.0644539594650269,
      "learning_rate": 9.532163742690059e-06,
      "loss": 0.7917,
      "step": 22800
    },
    {
      "epoch": 6.669590643274854,
      "grad_norm": 0.9000777006149292,
      "learning_rate": 9.448621553884713e-06,
      "loss": 0.7409,
      "step": 22810
    },
    {
      "epoch": 6.6725146198830405,
      "grad_norm": 0.9414494037628174,
      "learning_rate": 9.365079365079366e-06,
      "loss": 0.8609,
      "step": 22820
    },
    {
      "epoch": 6.675438596491228,
      "grad_norm": 0.8923594355583191,
      "learning_rate": 9.28153717627402e-06,
      "loss": 0.7849,
      "step": 22830
    },
    {
      "epoch": 6.678362573099415,
      "grad_norm": 0.7758263349533081,
      "learning_rate": 9.197994987468671e-06,
      "loss": 0.7987,
      "step": 22840
    },
    {
      "epoch": 6.681286549707602,
      "grad_norm": 1.2035870552062988,
      "learning_rate": 9.114452798663325e-06,
      "loss": 0.8386,
      "step": 22850
    },
    {
      "epoch": 6.684210526315789,
      "grad_norm": 1.0072486400604248,
      "learning_rate": 9.030910609857978e-06,
      "loss": 0.8173,
      "step": 22860
    },
    {
      "epoch": 6.687134502923977,
      "grad_norm": 1.2024457454681396,
      "learning_rate": 8.947368421052632e-06,
      "loss": 0.8389,
      "step": 22870
    },
    {
      "epoch": 6.690058479532164,
      "grad_norm": 1.078213095664978,
      "learning_rate": 8.863826232247285e-06,
      "loss": 0.8561,
      "step": 22880
    },
    {
      "epoch": 6.692982456140351,
      "grad_norm": 0.9020522236824036,
      "learning_rate": 8.780284043441939e-06,
      "loss": 0.8009,
      "step": 22890
    },
    {
      "epoch": 6.695906432748538,
      "grad_norm": 1.0358794927597046,
      "learning_rate": 8.696741854636592e-06,
      "loss": 0.7273,
      "step": 22900
    },
    {
      "epoch": 6.6988304093567255,
      "grad_norm": 1.0250297784805298,
      "learning_rate": 8.613199665831246e-06,
      "loss": 0.8143,
      "step": 22910
    },
    {
      "epoch": 6.701754385964913,
      "grad_norm": 1.0967718362808228,
      "learning_rate": 8.5296574770259e-06,
      "loss": 0.7422,
      "step": 22920
    },
    {
      "epoch": 6.7046783625731,
      "grad_norm": 0.9048606753349304,
      "learning_rate": 8.446115288220551e-06,
      "loss": 0.8032,
      "step": 22930
    },
    {
      "epoch": 6.707602339181286,
      "grad_norm": 0.8561185598373413,
      "learning_rate": 8.362573099415205e-06,
      "loss": 0.7852,
      "step": 22940
    },
    {
      "epoch": 6.7105263157894735,
      "grad_norm": 0.9776885509490967,
      "learning_rate": 8.279030910609858e-06,
      "loss": 0.7711,
      "step": 22950
    },
    {
      "epoch": 6.713450292397661,
      "grad_norm": 1.0187649726867676,
      "learning_rate": 8.195488721804512e-06,
      "loss": 0.7811,
      "step": 22960
    },
    {
      "epoch": 6.716374269005848,
      "grad_norm": 0.9412125945091248,
      "learning_rate": 8.111946532999165e-06,
      "loss": 0.754,
      "step": 22970
    },
    {
      "epoch": 6.719298245614035,
      "grad_norm": 0.9231497645378113,
      "learning_rate": 8.028404344193819e-06,
      "loss": 0.7428,
      "step": 22980
    },
    {
      "epoch": 6.722222222222222,
      "grad_norm": 0.9191237092018127,
      "learning_rate": 7.944862155388472e-06,
      "loss": 0.7976,
      "step": 22990
    },
    {
      "epoch": 6.7251461988304095,
      "grad_norm": 0.802189826965332,
      "learning_rate": 7.861319966583126e-06,
      "loss": 0.7979,
      "step": 23000
    },
    {
      "epoch": 6.728070175438597,
      "grad_norm": 0.9660919904708862,
      "learning_rate": 7.777777777777777e-06,
      "loss": 0.867,
      "step": 23010
    },
    {
      "epoch": 6.730994152046784,
      "grad_norm": 1.0083328485488892,
      "learning_rate": 7.694235588972431e-06,
      "loss": 0.8096,
      "step": 23020
    },
    {
      "epoch": 6.73391812865497,
      "grad_norm": 1.2040741443634033,
      "learning_rate": 7.6106934001670845e-06,
      "loss": 0.8125,
      "step": 23030
    },
    {
      "epoch": 6.7368421052631575,
      "grad_norm": 1.0336014032363892,
      "learning_rate": 7.527151211361738e-06,
      "loss": 0.8829,
      "step": 23040
    },
    {
      "epoch": 6.739766081871345,
      "grad_norm": 0.8278973698616028,
      "learning_rate": 7.4436090225563915e-06,
      "loss": 0.8271,
      "step": 23050
    },
    {
      "epoch": 6.742690058479532,
      "grad_norm": 0.9181512594223022,
      "learning_rate": 7.360066833751045e-06,
      "loss": 0.7427,
      "step": 23060
    },
    {
      "epoch": 6.745614035087719,
      "grad_norm": 0.7453210353851318,
      "learning_rate": 7.2765246449456985e-06,
      "loss": 0.8106,
      "step": 23070
    },
    {
      "epoch": 6.748538011695906,
      "grad_norm": 1.0476481914520264,
      "learning_rate": 7.192982456140351e-06,
      "loss": 0.7763,
      "step": 23080
    },
    {
      "epoch": 6.751461988304094,
      "grad_norm": 0.9732862710952759,
      "learning_rate": 7.109440267335005e-06,
      "loss": 0.7726,
      "step": 23090
    },
    {
      "epoch": 6.754385964912281,
      "grad_norm": 1.0023688077926636,
      "learning_rate": 7.025898078529658e-06,
      "loss": 0.7935,
      "step": 23100
    },
    {
      "epoch": 6.757309941520468,
      "grad_norm": 0.7687841057777405,
      "learning_rate": 6.942355889724312e-06,
      "loss": 0.7827,
      "step": 23110
    },
    {
      "epoch": 6.760233918128655,
      "grad_norm": 1.00005042552948,
      "learning_rate": 6.858813700918964e-06,
      "loss": 0.847,
      "step": 23120
    },
    {
      "epoch": 6.7631578947368425,
      "grad_norm": 0.8227249383926392,
      "learning_rate": 6.775271512113618e-06,
      "loss": 0.8344,
      "step": 23130
    },
    {
      "epoch": 6.76608187134503,
      "grad_norm": 1.2074323892593384,
      "learning_rate": 6.691729323308271e-06,
      "loss": 0.808,
      "step": 23140
    },
    {
      "epoch": 6.769005847953216,
      "grad_norm": 0.8138761520385742,
      "learning_rate": 6.608187134502925e-06,
      "loss": 0.8191,
      "step": 23150
    },
    {
      "epoch": 6.771929824561403,
      "grad_norm": 1.1985746622085571,
      "learning_rate": 6.5246449456975774e-06,
      "loss": 0.8158,
      "step": 23160
    },
    {
      "epoch": 6.7748538011695905,
      "grad_norm": 1.0717058181762695,
      "learning_rate": 6.441102756892231e-06,
      "loss": 0.7928,
      "step": 23170
    },
    {
      "epoch": 6.777777777777778,
      "grad_norm": 0.7890302538871765,
      "learning_rate": 6.3575605680868844e-06,
      "loss": 0.7228,
      "step": 23180
    },
    {
      "epoch": 6.780701754385965,
      "grad_norm": 1.0966747999191284,
      "learning_rate": 6.274018379281538e-06,
      "loss": 0.8495,
      "step": 23190
    },
    {
      "epoch": 6.783625730994152,
      "grad_norm": 1.1365879774093628,
      "learning_rate": 6.190476190476191e-06,
      "loss": 0.7626,
      "step": 23200
    },
    {
      "epoch": 6.786549707602339,
      "grad_norm": 1.0720258951187134,
      "learning_rate": 6.106934001670844e-06,
      "loss": 0.8379,
      "step": 23210
    },
    {
      "epoch": 6.7894736842105265,
      "grad_norm": 0.9635602235794067,
      "learning_rate": 6.023391812865498e-06,
      "loss": 0.8133,
      "step": 23220
    },
    {
      "epoch": 6.792397660818714,
      "grad_norm": 0.9140884280204773,
      "learning_rate": 5.939849624060151e-06,
      "loss": 0.7579,
      "step": 23230
    },
    {
      "epoch": 6.7953216374269,
      "grad_norm": 0.9914793968200684,
      "learning_rate": 5.856307435254805e-06,
      "loss": 0.7294,
      "step": 23240
    },
    {
      "epoch": 6.798245614035087,
      "grad_norm": 0.7882007360458374,
      "learning_rate": 5.772765246449457e-06,
      "loss": 0.7255,
      "step": 23250
    },
    {
      "epoch": 6.8011695906432745,
      "grad_norm": 0.8094307780265808,
      "learning_rate": 5.689223057644111e-06,
      "loss": 0.8114,
      "step": 23260
    },
    {
      "epoch": 6.804093567251462,
      "grad_norm": 0.9374693036079407,
      "learning_rate": 5.605680868838764e-06,
      "loss": 0.7402,
      "step": 23270
    },
    {
      "epoch": 6.807017543859649,
      "grad_norm": 1.2312395572662354,
      "learning_rate": 5.522138680033418e-06,
      "loss": 0.8936,
      "step": 23280
    },
    {
      "epoch": 6.809941520467836,
      "grad_norm": 0.7959995269775391,
      "learning_rate": 5.43859649122807e-06,
      "loss": 0.717,
      "step": 23290
    },
    {
      "epoch": 6.812865497076023,
      "grad_norm": 0.9462886452674866,
      "learning_rate": 5.355054302422724e-06,
      "loss": 0.8092,
      "step": 23300
    },
    {
      "epoch": 6.815789473684211,
      "grad_norm": 0.81472247838974,
      "learning_rate": 5.271512113617377e-06,
      "loss": 0.8478,
      "step": 23310
    },
    {
      "epoch": 6.818713450292398,
      "grad_norm": 0.8924699425697327,
      "learning_rate": 5.187969924812031e-06,
      "loss": 0.7305,
      "step": 23320
    },
    {
      "epoch": 6.821637426900585,
      "grad_norm": 0.7768769264221191,
      "learning_rate": 5.1044277360066835e-06,
      "loss": 0.7661,
      "step": 23330
    },
    {
      "epoch": 6.824561403508772,
      "grad_norm": 1.1010364294052124,
      "learning_rate": 5.020885547201337e-06,
      "loss": 0.7961,
      "step": 23340
    },
    {
      "epoch": 6.8274853801169595,
      "grad_norm": 0.981530487537384,
      "learning_rate": 4.9373433583959905e-06,
      "loss": 0.7892,
      "step": 23350
    },
    {
      "epoch": 6.830409356725146,
      "grad_norm": 1.0322909355163574,
      "learning_rate": 4.853801169590644e-06,
      "loss": 0.7004,
      "step": 23360
    },
    {
      "epoch": 6.833333333333333,
      "grad_norm": 0.9405953884124756,
      "learning_rate": 4.770258980785297e-06,
      "loss": 0.7706,
      "step": 23370
    },
    {
      "epoch": 6.83625730994152,
      "grad_norm": 0.9242417812347412,
      "learning_rate": 4.68671679197995e-06,
      "loss": 0.7809,
      "step": 23380
    },
    {
      "epoch": 6.8391812865497075,
      "grad_norm": 1.0386909246444702,
      "learning_rate": 4.603174603174604e-06,
      "loss": 0.7904,
      "step": 23390
    },
    {
      "epoch": 6.842105263157895,
      "grad_norm": 0.7981669306755066,
      "learning_rate": 4.519632414369257e-06,
      "loss": 0.7853,
      "step": 23400
    },
    {
      "epoch": 6.845029239766082,
      "grad_norm": 0.9732141494750977,
      "learning_rate": 4.436090225563911e-06,
      "loss": 0.8912,
      "step": 23410
    },
    {
      "epoch": 6.847953216374269,
      "grad_norm": 1.0650794506072998,
      "learning_rate": 4.352548036758563e-06,
      "loss": 0.7709,
      "step": 23420
    },
    {
      "epoch": 6.850877192982456,
      "grad_norm": 0.8969686031341553,
      "learning_rate": 4.269005847953217e-06,
      "loss": 0.7834,
      "step": 23430
    },
    {
      "epoch": 6.853801169590644,
      "grad_norm": 0.7580151557922363,
      "learning_rate": 4.18546365914787e-06,
      "loss": 0.7646,
      "step": 23440
    },
    {
      "epoch": 6.856725146198831,
      "grad_norm": 0.6725398898124695,
      "learning_rate": 4.101921470342524e-06,
      "loss": 0.7539,
      "step": 23450
    },
    {
      "epoch": 6.859649122807017,
      "grad_norm": 0.8958998322486877,
      "learning_rate": 4.0183792815371765e-06,
      "loss": 0.8087,
      "step": 23460
    },
    {
      "epoch": 6.862573099415204,
      "grad_norm": 0.7556310296058655,
      "learning_rate": 3.93483709273183e-06,
      "loss": 0.8387,
      "step": 23470
    },
    {
      "epoch": 6.8654970760233915,
      "grad_norm": 0.8616858124732971,
      "learning_rate": 3.8512949039264835e-06,
      "loss": 0.7902,
      "step": 23480
    },
    {
      "epoch": 6.868421052631579,
      "grad_norm": 0.8970156908035278,
      "learning_rate": 3.7677527151211366e-06,
      "loss": 0.766,
      "step": 23490
    },
    {
      "epoch": 6.871345029239766,
      "grad_norm": 0.8742868900299072,
      "learning_rate": 3.6842105263157892e-06,
      "loss": 0.8216,
      "step": 23500
    },
    {
      "epoch": 6.874269005847953,
      "grad_norm": 0.7734381556510925,
      "learning_rate": 3.6006683375104427e-06,
      "loss": 0.7351,
      "step": 23510
    },
    {
      "epoch": 6.87719298245614,
      "grad_norm": 0.8172942996025085,
      "learning_rate": 3.517126148705096e-06,
      "loss": 0.8269,
      "step": 23520
    },
    {
      "epoch": 6.880116959064328,
      "grad_norm": 0.8966111540794373,
      "learning_rate": 3.4335839598997493e-06,
      "loss": 0.7186,
      "step": 23530
    },
    {
      "epoch": 6.883040935672515,
      "grad_norm": 0.7977485656738281,
      "learning_rate": 3.3500417710944024e-06,
      "loss": 0.7833,
      "step": 23540
    },
    {
      "epoch": 6.885964912280702,
      "grad_norm": 1.0746971368789673,
      "learning_rate": 3.266499582289056e-06,
      "loss": 0.7379,
      "step": 23550
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 0.7515290975570679,
      "learning_rate": 3.182957393483709e-06,
      "loss": 0.8263,
      "step": 23560
    },
    {
      "epoch": 6.8918128654970765,
      "grad_norm": 0.9161444306373596,
      "learning_rate": 3.099415204678363e-06,
      "loss": 0.8091,
      "step": 23570
    },
    {
      "epoch": 6.894736842105263,
      "grad_norm": 0.8602563738822937,
      "learning_rate": 3.015873015873016e-06,
      "loss": 0.7662,
      "step": 23580
    },
    {
      "epoch": 6.89766081871345,
      "grad_norm": 0.9496611952781677,
      "learning_rate": 2.9323308270676694e-06,
      "loss": 0.7364,
      "step": 23590
    },
    {
      "epoch": 6.900584795321637,
      "grad_norm": 0.9510214328765869,
      "learning_rate": 2.8487886382623225e-06,
      "loss": 0.8499,
      "step": 23600
    },
    {
      "epoch": 6.9035087719298245,
      "grad_norm": 0.9437460899353027,
      "learning_rate": 2.765246449456976e-06,
      "loss": 0.8073,
      "step": 23610
    },
    {
      "epoch": 6.906432748538012,
      "grad_norm": 0.9748089909553528,
      "learning_rate": 2.681704260651629e-06,
      "loss": 0.7409,
      "step": 23620
    },
    {
      "epoch": 6.909356725146199,
      "grad_norm": 0.8952815532684326,
      "learning_rate": 2.5981620718462826e-06,
      "loss": 0.7775,
      "step": 23630
    },
    {
      "epoch": 6.912280701754386,
      "grad_norm": 0.8361846208572388,
      "learning_rate": 2.5146198830409357e-06,
      "loss": 0.7283,
      "step": 23640
    },
    {
      "epoch": 6.915204678362573,
      "grad_norm": 0.9072235822677612,
      "learning_rate": 2.431077694235589e-06,
      "loss": 0.8716,
      "step": 23650
    },
    {
      "epoch": 6.918128654970761,
      "grad_norm": 0.9651509523391724,
      "learning_rate": 2.3475355054302427e-06,
      "loss": 0.7431,
      "step": 23660
    },
    {
      "epoch": 6.921052631578947,
      "grad_norm": 1.138825535774231,
      "learning_rate": 2.2639933166248957e-06,
      "loss": 0.7848,
      "step": 23670
    },
    {
      "epoch": 6.923976608187134,
      "grad_norm": 0.9016859531402588,
      "learning_rate": 2.1804511278195492e-06,
      "loss": 0.8537,
      "step": 23680
    },
    {
      "epoch": 6.926900584795321,
      "grad_norm": 0.8823428750038147,
      "learning_rate": 2.0969089390142023e-06,
      "loss": 0.7815,
      "step": 23690
    },
    {
      "epoch": 6.9298245614035086,
      "grad_norm": 1.0725650787353516,
      "learning_rate": 2.013366750208856e-06,
      "loss": 0.8405,
      "step": 23700
    },
    {
      "epoch": 6.932748538011696,
      "grad_norm": 1.0721971988677979,
      "learning_rate": 1.929824561403509e-06,
      "loss": 0.767,
      "step": 23710
    },
    {
      "epoch": 6.935672514619883,
      "grad_norm": 1.0636227130889893,
      "learning_rate": 1.8462823725981622e-06,
      "loss": 0.8548,
      "step": 23720
    },
    {
      "epoch": 6.93859649122807,
      "grad_norm": 1.0549023151397705,
      "learning_rate": 1.7627401837928155e-06,
      "loss": 0.7859,
      "step": 23730
    },
    {
      "epoch": 6.941520467836257,
      "grad_norm": 1.1718884706497192,
      "learning_rate": 1.6791979949874688e-06,
      "loss": 0.7681,
      "step": 23740
    },
    {
      "epoch": 6.944444444444445,
      "grad_norm": 1.110626459121704,
      "learning_rate": 1.5956558061821223e-06,
      "loss": 0.7867,
      "step": 23750
    },
    {
      "epoch": 6.947368421052632,
      "grad_norm": 0.8030319809913635,
      "learning_rate": 1.5121136173767753e-06,
      "loss": 0.8079,
      "step": 23760
    },
    {
      "epoch": 6.950292397660819,
      "grad_norm": 1.0981099605560303,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 0.7535,
      "step": 23770
    },
    {
      "epoch": 6.953216374269006,
      "grad_norm": 1.0420663356781006,
      "learning_rate": 1.345029239766082e-06,
      "loss": 0.805,
      "step": 23780
    },
    {
      "epoch": 6.956140350877193,
      "grad_norm": 0.8550309538841248,
      "learning_rate": 1.2614870509607352e-06,
      "loss": 0.811,
      "step": 23790
    },
    {
      "epoch": 6.95906432748538,
      "grad_norm": 0.7595778107643127,
      "learning_rate": 1.1779448621553885e-06,
      "loss": 0.8408,
      "step": 23800
    },
    {
      "epoch": 6.961988304093567,
      "grad_norm": 0.7660789489746094,
      "learning_rate": 1.0944026733500418e-06,
      "loss": 0.7499,
      "step": 23810
    },
    {
      "epoch": 6.964912280701754,
      "grad_norm": 1.0423612594604492,
      "learning_rate": 1.010860484544695e-06,
      "loss": 0.8805,
      "step": 23820
    },
    {
      "epoch": 6.9678362573099415,
      "grad_norm": 0.6637542843818665,
      "learning_rate": 9.273182957393484e-07,
      "loss": 0.7624,
      "step": 23830
    },
    {
      "epoch": 6.970760233918129,
      "grad_norm": 0.9208176136016846,
      "learning_rate": 8.437761069340017e-07,
      "loss": 0.7696,
      "step": 23840
    },
    {
      "epoch": 6.973684210526316,
      "grad_norm": 0.8171494603157043,
      "learning_rate": 7.60233918128655e-07,
      "loss": 0.7108,
      "step": 23850
    },
    {
      "epoch": 6.976608187134503,
      "grad_norm": 0.7772337794303894,
      "learning_rate": 6.766917293233083e-07,
      "loss": 0.7581,
      "step": 23860
    },
    {
      "epoch": 6.97953216374269,
      "grad_norm": 0.9100651741027832,
      "learning_rate": 5.931495405179616e-07,
      "loss": 0.7646,
      "step": 23870
    },
    {
      "epoch": 6.982456140350877,
      "grad_norm": 0.8445008993148804,
      "learning_rate": 5.096073517126149e-07,
      "loss": 0.6885,
      "step": 23880
    },
    {
      "epoch": 6.985380116959064,
      "grad_norm": 1.0129066705703735,
      "learning_rate": 4.260651629072682e-07,
      "loss": 0.7103,
      "step": 23890
    },
    {
      "epoch": 6.988304093567251,
      "grad_norm": 0.9159678220748901,
      "learning_rate": 3.425229741019215e-07,
      "loss": 0.7931,
      "step": 23900
    },
    {
      "epoch": 6.991228070175438,
      "grad_norm": 0.9731971621513367,
      "learning_rate": 2.589807852965748e-07,
      "loss": 0.694,
      "step": 23910
    },
    {
      "epoch": 6.994152046783626,
      "grad_norm": 1.3638858795166016,
      "learning_rate": 1.7543859649122808e-07,
      "loss": 0.836,
      "step": 23920
    },
    {
      "epoch": 6.997076023391813,
      "grad_norm": 0.7431387305259705,
      "learning_rate": 9.189640768588137e-08,
      "loss": 0.778,
      "step": 23930
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.2590886354446411,
      "learning_rate": 8.35421888053467e-09,
      "loss": 0.8976,
      "step": 23940
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.7650781869888306,
      "eval_runtime": 76.3822,
      "eval_samples_per_second": 11.639,
      "eval_steps_per_second": 1.466,
      "step": 23940
    }
  ],
  "logging_steps": 10,
  "max_steps": 23940,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0004910966728294e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
